[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "In this web site, you will find all course materials here.\nInstructor: Dr. Kam Tin Seong, Associate Professor of Information Systems (Practice)\nDate & Time: Saturday 12:30pm - 3:45pm\nVenue: SOE/SCIS2 Seminar Room 5-2\nPiazza link.\nConsultation booking link.\nVirtual Meeting Room: Zoom\n\n\nFor the next 10 weeks, your learning journey will be very bumpy, especially come to R programming.\n\nLearning R can be difficult at first. It is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham, the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily.\n\n\n\n\n\n\nAll acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them. Emergency\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the WebEx platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the WebEx platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "index.html#words-of-encouragement",
    "href": "index.html#words-of-encouragement",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "For the next 10 weeks, your learning journey will be very bumpy, especially come to R programming.\n\nLearning R can be difficult at first. It is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham, the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily."
  },
  {
    "objectID": "index.html#important-information",
    "href": "index.html#important-information",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "All acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them. Emergency\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the WebEx platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the WebEx platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Getting started",
    "text": "Getting started\n\nLaunch the coursework project with RStudio\nCreate a new folder called In-class_Ex.\nCreate a new sub-folder inside the newly created In-class_Ex folder. Name the sub-folder In-class_Ex01.\nCreate a new Quarto document. Save the newly create qmd file in In-class_Ex01 sub-folder. Call the file In-class_Ex01."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-the-r-packages",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Loading the R packages",
    "text": "Loading the R packages\n\nThe taskThe code\n\n\nFor the purpose of this in-class exercise, the following R packages will be used:\n\ntidyverse\nsf\ntmap\nggstatsplot\n\nWrite a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\n\n\n\npacman::p_load(tidyverse, sf, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nCreate a sub-folder called data in In-class_Ex01 folder.\nIf necessary visit data.gov.sg and download Master Plan 2014 Subzone Boundary (Web) from the portal. You are required to download both the ESRI shapefile and kml file.\nWrite a code chunk to import Master Plan 2014 Subzone Boundary (Web) in shapefile and kml save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports shapefile.\n\nmpsz14_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nThis code chunk imports kml file.\n\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nWrite a code chunk to export mpsz14_shp sf data.frame into kml file save the output in data sub-folder. Name the output file MP14_SUBZONE_WEB_PL.\n\n\n\n\n\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Pre-school Location Data",
    "text": "Working with Pre-school Location Data\n\nThe taskThe code\n\n\n\nIf necessary visit data.gov.sg and download Pre-Schools Location from the portal. You are required to download both the kml and geojson files.\nWrite a code chunk to import Pre-Schools Location in kml geojson save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports kml file.\n\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nThis code chunk imports geojson file.\n\npreschool_geojson &lt;- st_read(\"data/PreSchoolsLocation.geojson\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-2019-subzone-boundary-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-2019-subzone-boundary-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan 2019 Subzone Boundary Data",
    "text": "Working with Master Plan 2019 Subzone Boundary Data\n\nThe taskTo import shapefileTo import kml\n\n\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both the kml file.\nMove MPSZ-2019 shapefile provided for In-class Exercise 1 folder on elearn to data sub-folder of In-class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No SEA) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\")\n\n\n\n\n\n\nmpsz19_kml &lt;- st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nChecking coordinate system\n\nThe taskThe code\n\n\nWrite a code chunk to check the project of the imported sf objects.\n\n\n\n\nst_crs(mpsz19_shp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nTransforming coordinate system\n\nThe taskTo import MPSZ-2019To import PreSchoolsLocation.kml\n\n\nRe-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nPoint-in-Polygon count\n\nThe taskThe code\n\n\nWrite a code chunk to count the number of pre-schools in each planning sub-zone.\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(`PreSch Count` = lengths(\n    st_intersects(mpsz19_shp, preschool)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nComputing density\n\nThe taskThe code\n\n\nWrite a single line code to perform the following tasks:\n\nDerive the area of each planning sub-zone.\nDrop the unit of measurement of the area (i.e. m^2)\nCalculate the density of pre-school at the planning sub-zone level.\n\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nThe taskThe codeThe plot\n\n\nUsing appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\nTip: Refer to ggscatterstats() of ggstatsplot package.\n\n\n\n\nmpsz$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\nmpsz$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`)) \nmpsz19_shp &lt;- as.data.frame(mpsz19_shp)\n\nggscatterstats(data = mpsz19_shp,\n               x = `PreSch Density`,\n               y = `PreSch Count`,\n               type = \"parametric\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Population Data",
    "text": "Working with Population Data\n\nThe taskThe code\n\n\n\nVisit and extract the latest Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling from Singstat homepage.\n\n\n\n\npopdata &lt;- read_csv(\"data/respopagesextod2023.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-wrangling",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe taskThe code\n\n\n\nWrite a code chunk to prepare a data.frame showing population by Planning Area and Planning subzone\n\n\n\n\npopdata2023 &lt;- popdata %&gt;% \n  group_by(PA, SZ, AG) %&gt;% \n  summarise(`POP`=sum(`Pop`)) %&gt;%  \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG,\n              values_from = POP)\n\ncolnames(popdata2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Data Processing",
    "text": "Data Processing\n\nThe taskThe code\n\n\nWrite a code chunk to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Joining popdata2023 and mpsz19_shp",
    "text": "Joining popdata2023 and mpsz19_shp\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) \n\n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\npop2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp, \n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Choropleth Map of Dependency Ratio by Planning Subzone",
    "text": "Choropleth Map of Dependency Ratio by Planning Subzone\n\nThe mapThe code"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nThe concept\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 1: Data Preparation\nThe code chunk below excludes records with NA by using the code chunk below.\n\n\nmpsz_pop2023 &lt;- mpsz_pop2023 %&gt;%\n  drop_na()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-2",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-2",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 2: The get function\nThe code chunk below defines a function to get the input data and field to be used for creating the percentile map.\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-3",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-3",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 3: A percentile mapping function\nThe code chunk below creates a function for computing and plotting the percentile map.\n\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(mpsz_pop2023) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-4",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-4",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 4: Running the functions\nThe code chunk below runs the percentile map function.\n\n\npercentmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\n\n\nThe Concept\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nggplot(data = mpsz_pop2023,\n       aes(x = \"\",\n           y = DEPENDENCY)) +\n  geom_boxplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 1: Creating the boxbreaks function\n\n\nThe code chunk on the right is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analyticsal-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analyticsal-map-box-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analyticsal Map: Box Map",
    "text": "Analyticsal Map: Box Map\nStep 2: Creating the get.var function\n\n\nThe code chunk on the right an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-2",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-2",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 3: Boxmap function\n\n\nThe code chunk on the right is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-3",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-3",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 4: Plotting Box Map\n\nboxmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-4",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-4",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nPlotting Interactive Box Map\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nboxmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "The Data",
    "text": "The Data\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Installing and launching the R packages",
    "text": "Installing and launching the R packages\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\nThe code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\nnetwork &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_St\")\n\n\nchildcare &lt;- st_read(dsn=\"data/rawdata\",\n                     layer=\"Punggol_CC\")\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\nChildcareNetwork\n\n\n\nchildcare\n\n\n\n\nnetwork\n\n\n\n\nWhen I exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Visualising the Geospatial Data",
    "text": "Visualising the Geospatial Data\nBefore we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\nplot(st_geometry(network))\nplot(childcare,add=T,col='red',pch = 19)\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare) + \n  tm_dots() + \n  tm_shape(network) +\n  tm_lines()\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Network KDE (NKDE) Analysis",
    "text": "Network KDE (NKDE) Analysis\nIn this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\nPreparing the lixels objects\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\nlixels &lt;- lixelize_lines(network, \n                         700, \n                         mindist = 350)\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\nGenerating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\nsamples &lt;- lines_center(lixels) \n\n\ntmap_mode('view')\ntm_shape(lixels) + \n  tm_lines() + \ntm_shape(samples) +\n  tm_dots(size = 0.01)\ntmap_mode('plot')\n\nThe points are located at center of the line based on the length of the line.\n\n\nPerforming NKDE\nWe are ready to computer the NKDE by using the code chunk below.\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1, nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\nVisualising NKDE\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\ntmap_mode('plot')\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Network Constrained G- and K-Function Analysis",
    "text": "Network Constrained G- and K-Function Analysis\nIn this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\nkfun_childcare &lt;- kfunctions(network, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\nkfun_childcare$plotk\n\nThe blue line is the empirical network K-function of the childcare centres in Punggol planning area. The gray envelop represents the results of the 50 simulations in the interval 2.5% - 97.5%. Because the blue line between the distance of 250m-400m are below the gray area, we can infer that the childcare centres in Punggol planning area resemble regular pattern at the distance of 250m-400m."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#bus-stop",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#bus-stop",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Bus Stop",
    "text": "Bus Stop\n\nbusstop &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nroad &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_Road\") %&gt;%\n  st_transform(crs = 3414)\n\n\nlixels &lt;- lixelize_lines(road, \n                         700, \n                         mindist = 350)\n\n\nsamples &lt;- lines_center(lixels) \n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"discontinuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"continuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndata(\"mtl_network\")\ndata(\"bike_accidents\")\n\n\ntm_shape(mtl_network) +\n  tm_lines(\"black\") +\ntm_shape(bike_accidents) +\n  tm_dots(\"red\",\n          size = 0.2)\n\n\nlixels &lt;- lixelize_lines(mtl_network,\n                         200,\n                         mindist = 50)\n\n\nsamples &lt;- lines_center(lixels)\n\n\ndensities &lt;- nkde(mtl_network, \n                  events = bike_accidents,\n                  w = rep(1,nrow(bike_accidents)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, div= \"bw\", \n                  method = \"discontinuous\", digits = 1, tol = 1,\n                  grid_shape = c(1,1), max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nsamples$density &lt;- densities * 1000\nlixels$density &lt;- densities*1000\n\n\ntmap_mode('plot')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(bike_accidents)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(mtl_network, \n                  events = bike_accidents,\n                  w = rep(1,nrow(bike_accidents)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, div= \"bw\", \n                  method = \"continuous\", digits = 1, tol = 1,\n                  grid_shape = c(1,1), max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nsamples$density &lt;- densities * 1000\nlixels$density &lt;- densities*1000\n\n\ntmap_mode('pplot')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(bike_accidents)+\n  tm_dots()\ntmap_mode('plot')\n\n\nConverting the Date field to a numeric field (counting days)\n\nbike_accidents$Time &lt;- as.POSIXct(bike_accidents$Date, format = \"%Y/%m/%d\")\nstart &lt;- as.POSIXct(\"2016/01/01\", format = \"%Y/%m/%d\")\nbike_accidents$Time &lt;- difftime(bike_accidents$Time, start, units = \"days\")\nbike_accidents$Time &lt;- as.numeric(bike_accidents$Time)\n\n\nmonths &lt;- as.character(1:12)\nmonths &lt;- ifelse(nchar(months)==1, paste0(\"0\", months), months)\nmonths_starts_labs &lt;- paste(\"2016/\",months,\"/01\", sep = \"\")\nmonths_starts_num &lt;- as.POSIXct(months_starts_labs, format = \"%Y/%m/%d\")\nmonths_starts_num &lt;- difftime(months_starts_num, start, units = \"days\")\nmonths_starts_num &lt;- as.numeric(months_starts_num)\nmonths_starts_labs &lt;- gsub(\"2016/\", \"\", months_starts_labs, fixed = TRUE)\n\n\nggplot(bike_accidents) + \n  geom_histogram(aes(x = Time), \n                 bins = 30, \n                 color = \"white\") + \n  scale_x_continuous(breaks = months_starts_num, \n                     labels = months_starts_labs)\n\n\nbike_accidents &lt;- subset(bike_accidents, bike_accidents$Time &gt;= 90)\n\n\nw &lt;- rep(1,nrow(bike_accidents))\nsamples &lt;- seq(0, max(bike_accidents$Time), 0.5)\n\ntime_kernel_values &lt;- data.frame(\n  bw_10 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 10, kernel_name = \"quartic\"),\n  bw_20 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 20, kernel_name = \"quartic\"),\n  bw_30 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 30, kernel_name = \"quartic\"),\n  bw_40 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 40, kernel_name = \"quartic\"),\n  bw_50 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 50, kernel_name = \"quartic\"),\n  bw_60 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 60, kernel_name = \"quartic\"),\n  time = samples\n)\n\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = months_starts_num, \n                     labels = months_starts_labs) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\") + \n  theme(axis.text = element_text(size = 5))\n\n\nbw1 &lt;- bw.bcv(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\nbw2 &lt;- bw.ucv(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\nbw3 &lt;- bw.SJ(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\n\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw1, kernel_name = \"quartic\"),\n  bw_ucv = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw2, kernel_name = \"quartic\"),\n  bw_SJ = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw3, kernel_name = \"quartic\"),\n  time = samples\n)\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = months_starts_num, labels = months_starts_labs) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\ntm_shape(mtl_network) + \n  tm_lines(col = \"black\") + \n  tm_shape(bike_accidents) + \n  tm_dots(col = \"red\", size = 0.1)\n\n\n# creating sample points\nlixels &lt;- lixelize_lines(mtl_network, 50)\nsample_points &lt;- lines_center(lixels)\n\n# calculating the densities\nnkde_densities &lt;- nkde(lines = mtl_network,\n                       events = bike_accidents,\n                       w = rep(1,nrow(bike_accidents)),\n                       samples = sample_points,\n                       kernel_name = \"quartic\",\n                       bw = 450,\n                       adaptive = TRUE, trim_bw = 900,\n                       method = \"discontinuous\",\n                       div = \"bw\",\n                       max_depth = 10,\n                       digits = 2, tol = 0.1, agg = 5,\n                       grid_shape = c(1,1),\n                       verbose = FALSE)\n\nsample_points$density &lt;- nkde_densities$k * 1000\n\ntm_shape(sample_points) + \n  tm_dots(col = \"density\", style = \"kmeans\", n = 8, palette = \"viridis\", size = 0.05) + \n  tm_layout(legend.outside = TRUE)\n\n\n# creating sample points\nlixels &lt;- lixelize_lines(mtl_network, 50)\nsample_points &lt;- lines_center(lixels)\n\n# calculating the densities\nnkde_densities &lt;- nkde(lines = mtl_network,\n                       events = bike_accidents,\n                       w = rep(1,nrow(bike_accidents)),\n                       samples = sample_points,\n                       kernel_name = \"quartic\",\n                       bw = 450,\n                       adaptive = TRUE, trim_bw = 900,\n                       method = \"discontinuous\",\n                       div = \"bw\",\n                       max_depth = 10,\n                       digits = 2, tol = 0.1, agg = 5,\n                       grid_shape = c(1,1),\n                       verbose = FALSE)\n\n\nsample_points$density &lt;- nkde_densities$k * 1000\n\n\ntm_shape(sample_points) + \n  tm_dots(col = \"density\", \n          style = \"kmeans\", \n          n = 8, \n          palette = \"viridis\", \n          size = 0.05) + \n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#references",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#references",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nspNetwork: Spatial Analysis on Network\nNetwork Kernel Density Estimate\nDetails about NKDE\nNetwork k Functions"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#getting-started",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#importing-the-data",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Importing the data",
    "text": "Importing the data\n\nURA Master Plan 2014 planning subzone boundary\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\n\ncondo_resale_sf &lt;- read_rds(\n  \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#correlation-analysis---ggstatsplot-methods",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#correlation-analysis---ggstatsplot-methods",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Correlation Analysis - ggstatsplot methods",
    "text": "Correlation Analysis - ggstatsplot methods\nInstead of using corrplot package, in the code chunk below, ggcorrmat() of ggstatsplot is used.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method",
    "text": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo_mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                  AGE   + PROX_CBD + PROX_CHILDCARE + \n                  PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n                  PROX_HAWKER_MARKET    + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + PROX_BUS_STOP + \n                  NO_Of_UNITS + FAMILY_FRIENDLY + \n                  FREEHOLD + LEASEHOLD_99YR, \n                data=condo_resale_sf)\nsummary(condo_mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#model-assessment-olsrr-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#model-assessment-olsrr-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Model Assessment: olsrr method",
    "text": "Model Assessment: olsrr method\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\n\nGenerating tidy linear regression report\n\nols_regress(condo_mlr)\n\n\nMulticolinearuty\n\nols_vif_tol(condo_mlr)\n\n\n\nVariable selection\n\ncondo_fw_mlr &lt;- ols_step_forward_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\n\n\nplot(condo_fw_mlr)\n\n\n\n\nVisualising model parameters\n\nggcoefstats(condo_mlr,\n            sort = \"ascending\")\n\n\n\nTest for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo_fw_mlr$model)\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\nTest for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo_fw_mlr$model)\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo_fw_mlr$model)\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#testing-for-spatial-autocorrelation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#testing-for-spatial-autocorrelation",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Testing for Spatial Autocorrelation",
    "text": "Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)\n\nNext, we will join the newly created data frame with condo_resale_sf object.\n\ncondo_resale_sf &lt;- cbind(condo_resale_sf, \n                        mlr_output$FW_MLR_RES) %&gt;%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\")\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\n\nSpatial stationary test\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nHo: The residuals are randomly distributed (also known as spatial stationary) H1: The residuals are spatially non-stationary\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building Hedonic Pricing Models using GWmodel",
    "text": "Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing by using geographically weighted regression model. Two spatial weights will be used, they are: fixed and adaptive bandwidth schemes.\n\nBuilding Fixed Bandwidth GWR Model\n\nComputing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach agreement.\n\nbw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                     PROX_CBD + PROX_CHILDCARE + \n                     PROX_ELDERLYCARE   + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale_sf, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\nGWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\ngwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                         AGE    + PROX_CBD + PROX_CHILDCARE + \n                         PROX_ELDERLYCARE   +PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +\n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                         NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale_sf, \n                       bw=bw_fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\ngwr_fixed\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\nBuilding Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\nComputing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw_adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale_sf, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale_sf, \n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe code below can be used to display the model output.\n\ngwr_adaptive\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\nVisualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\nConverting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ngwr_adaptive_output &lt;- as.data.frame(\n  gwr_adaptive$SDF) %&gt;%\n  select(-c(2:15))\n\n\ngwr_sf_adaptive &lt;- cbind(condo_resale_sf,\n                         gwr_adaptive_output)\n\nNext, glimpse() is used to display the content of condo_resale_sf.adaptive sf data frame.\n\nglimpse(gwr_sf_adaptive)\n\n\nsummary(gwr_adaptive$SDF$yhat)\n\n\n\nVisualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\ntmap_mode(\"plot\")\n\n\n\nVisualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\ntmap_mode(\"plot\")\n\n\nBy URA Plannign Region\n\ntm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(gwr_sf_adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#getting-started",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#getting-started",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Getting Started",
    "text": "Getting Started\nInstalling and Loading R packages\n\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse,\n               knitr, kableExtra)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#preparing-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#preparing-data",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Preparing Data",
    "text": "Preparing Data\n\nData importData SamplingChecking of overlapping pointSpatial jitter\n\n\n\n\nmdata &lt;- read_rds(\"data/rds/mdata.rds\")\n\n\n\n\nCalibrating predictive models are computational intensive, especially random forest method is used. For quick prototyping, a 10% sample will be selected at random from the data by using the code chunk below.\n\n\nset.seed(1234)\nHDB_sample &lt;- mdata %&gt;%\n  sample_n(1500)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nWhen using GWmodel to calibrate explanatory or predictive models, it is very important to ensure that there are no overlapping point features\n\n\n\nThe code chunk below is used to check if there are overlapping point features.\n\n\noverlapping_points &lt;- HDB_sample %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)\n\n\n\n\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5m to avoid overlapping point features.\n\n\nHDB_sample &lt;- HDB_sample %&gt;%\n  st_jitter(amount = 5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-sampling-1",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-sampling-1",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Data Sampling",
    "text": "Data Sampling\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n\nset.seed(1234)\nresale_split &lt;- initial_split(HDB_sample, \n                              prop = 6.67/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#building-a-non-spatial-multiple-linear-regression",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#building-a-non-spatial-multiple-linear-regression",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Building a non-spatial multiple linear regression",
    "text": "Building a non-spatial multiple linear regression\n\nThe reportThe code chunk\n\n\n\n\n\n\n\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nolsrr::ols_regress(price_mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-gwr",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-gwr",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling with gwr",
    "text": "Predictive Modelling with gwr\nComputing adaptive bandwidth\n\nThe codeThe output\n\n\n\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\n\n\n\n\nbw_adaptive"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-mlr",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-mlr",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling with MLR",
    "text": "Predictive Modelling with MLR\nPredicting with test data\n\nTest data bwPredicting\n\n\n\n\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\n\n\n\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-rf-method",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-rf-method",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling: RF method",
    "text": "Predictive Modelling: RF method\n\nData preparationCalibrating RF modelModel output\n\n\nFirstly, code chunk below is used to extract the coordinates of training and test data sets\n\n\ncoords &lt;- st_coordinates(HDB_sample)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\nNext, code chunk below is used to drop the geometry column of both training and test data sets.\n\n\ntrain_data_nogeom &lt;- train_data %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data_nogeom)\n\n\n\n\n\n\nrf"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-spatialml-method",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-spatialml-method",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling: SpatialML method",
    "text": "Predictive Modelling: SpatialML method\n\nDetermining bandwidthCalibrating with grf\n\n\n\nset.seed(1234)\ngwRF_bw &lt;- grf.bw(formula = resale_price ~ floor_area_sqm + \n                       storey_order + remaining_lease_mths + \n                       PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n                       PROX_MRT + PROX_PARK + PROX_MALL + \n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dataset=train_data_nogeom, \n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\n\n\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + \n                       storey_order + remaining_lease_mths + \n                       PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n                       PROX_MRT + PROX_PARK + PROX_MALL + \n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predicting-by-using-the-test-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predicting-by-using-the-test-data",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predicting by using the test data",
    "text": "Predicting by using the test data\n\nPreparing the test dataPredicting with the test dataCreating DF\n\n\n\n\ntest_data_nogeom &lt;- cbind(\n  test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\nIn the code chunk below, predict.grf() of spatialML for predicting re-sale prices in the test data set (i.e. test_data_nogeom)\n\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nNext, the code chunk below is used to convert the output from predict.grf() into a data.frame.\n\n\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\n\nThen, cbind() is used to append fields in GRF_pred_df data.frame onto test_data.\n\n\ntest_data_pred &lt;- cbind(test_data, \n                        GRF_pred_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-the-predicted-values",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-the-predicted-values",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Visualising the predicted values",
    "text": "Visualising the predicted values\n\nThe plotThe code chunk\n\n\n\n\n\n\n\nggplot(data = test_data_pred,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#content",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#content",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Content",
    "text": "Content\n\nIntroducing maps\nTypology of maps\n\nReference maps\nThematic maps\n\nProportional Symbol Map\nChoropleth Mapping\nIntroduction to tmap Methods\n\n\nThis lesson consists of two parts. First, I will share with you the concepts and design principles of choropleth maps. Next, I will introduce you to tmap, an R package specially designed for thematic mapping based on Layered Grammar of Graphics"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#what-is-a-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#what-is-a-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "What is a Map?",
    "text": "What is a Map?\nA model of real world depict by a collection of cartographic symbols or/and visual abstraction."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#typology-of-maps",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#typology-of-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Typology of Maps",
    "text": "Typology of Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#thematic-mapping-principles-and-methods",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#thematic-mapping-principles-and-methods",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Thematic Mapping: Principles and Methods",
    "text": "Thematic Mapping: Principles and Methods\n\nDisplaying\n\nQualitative data\nQuantitative data\n\nChoosing -Appropriate classification method for displaying data\n\nAppropriate number of classes\n\nTechniques in data analysis\n\nUsing the classification histogram\nNormalizing data"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-maps",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Maps",
    "text": "Qualitative Thematic Maps\nVisual Variables and Cartographic Symbols\n\n\n\nQualitative visual variables are used for nominal scale data.\nThe goal of qualitative visual variables is to show how entities differ from each other.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right for examples of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nPoint symbol map\n\n\n\nDifferent point symbols are used to represent school types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-1",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nLine symbol map\n\n\n\nA road map is an example of a thematic map. It shows the road network of an area. In this map, lines with different colour intensity and tickness are used to differentiate hierarchy of roads."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-2",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-2",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nArea map\n\n\n\nLand use map below is a good example of a discrete thematic map. In this map, different colours are use to represent different land use types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#quantitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#quantitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Quantitative Thematic Map",
    "text": "Quantitative Thematic Map\nVisual Variables and Cartographic Symbols\n\n\n\nQuantitative visual variables are used to display ordinal, interval or ratio scale data.\n\nThe goal of the quantitative visual variable is to show relative magnitude or order between entities.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right shows of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#proportional-symbol-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#proportional-symbol-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Proportional Symbol Map",
    "text": "Proportional Symbol Map\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map.\n\n\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map. For example, the proportional maps above use circle with different sizes to represent millions of people. There are two types of point features that are typically depicted with proportional symbols: features for which the data represents a geographic position directly (e.g., gallons of oil from individual oil wells), and features that are geographic areas to which data are aggregated and the data magnitudes are assigned to a representative point within the area (e.g., the geographic centroid of a state as in the examples above). In either case, the area of the symbol is scaled to represent the data magnitude, sometimes with a bit of exaggeration to adjust for a general tendency of human vision to underestimate differences in area. A variant on this direct data-to-symbol scaling groups values into categories first, then scales the symbol to represent the mean for the category, assigning a symbol to each place to represent the category range that the mean for the place falls within"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#dot-density-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#dot-density-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Dot Density Map",
    "text": "Dot Density Map\nA dot-density map is a type of thematic map that uses dots or other symbols on the map to show the values of one or more numeric data fields. Each dot on a dot-density map represents some amount of data.\n\n\nOne dot represent 100 households.\n\n\nReference: Dot distribution map at wiki and Dot Density Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#choropleth-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#choropleth-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Choropleth Map",
    "text": "Choropleth Map\nA choropleth map is a type of thematic map in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population or per-capita income."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Data classification",
    "text": "Data classification\n\n\nNot sure how many classes to use? Have a look at the distribution of your data in a histogram (see examples below): Are there obvious clusters within your data? Are there large gaps in your data range that suggest nice compact data classes? If so, pick that number of classes and place those class breaks around those clusters."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Colour scheme",
    "text": "Colour scheme\nColorBrewer"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#mapping-packages-in-r",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#mapping-packages-in-r",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Mapping packages in R",
    "text": "Mapping packages in R\n\n\nSelected popular mapping packages\nCRAN Task View: Analysis of Spatial Data\n\ntmap\nmapsf\nleaflet\nggplot2. Read Chapter 6: Maps of ‘ggplot2: Elegant Graphics for Data Analysis’ for more detail.\nggmap\nquickmapr\nmapview\n\n\nOther packages\n\nRColorBrewer\nclassInt"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#introducing-tmap",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#introducing-tmap",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Introducing tmap",
    "text": "Introducing tmap\n\n\n\ntmap is a R package specially designed for creating thematic maps using the pricinples of the Grammar of Graphics.\nIt offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and proportional symbol maps.\nIt supports two modes: tm_plot() for static maps and tm_view() for interactive maps.\nIt provides shiny integration with renderTmap(), tmapOutput(), tmapProxy() and tm_remove_layer()."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\ntm_shape()\n\nThe first element to start with is tm_shape(), which specifies the shape object."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-1",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nNext, one, or a combination of the following drawing layers should be specified:\n\n\n\nLinks to tm_polygons(), tm_symbols(), tm_lines(), tm_raster() and tm_text()"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-2",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-2",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nEach of these functions specifies the geometry, mapping, and scaling component of the LGTM.\nAn aesthetic can take a constant value, a data variable name, or a vector consisting of values or variable names.\nIf a data variable is provided, the scale is automatically configured according to the values of this variable, but can be adjusted with several arguments. For instance, the main scaling arguments for a color aesthetic are color palette, the preferred number of classes, and a style to create classes.\nAlso, for each aesthetic, except for the text labels, a legend is automatically created.\nIf a vector of variable names is provided, small multiples are created, which will be explained further below."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-3",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#tmap-elements-3",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nDerived layers\n\n\nEach aesthetic can take a constant value or a data variable name. For instance, tm_fill(col=\"blue\") colors all polygons blue, while tm_fill(col=\"var1\"), where “var1” is the name of a data variable in the shape object, creates a choropleth.\n\nThe supported derived layers are as follows:"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification-methods-of-tmap",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification-methods-of-tmap",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Data classification methods of tmap",
    "text": "Data classification methods of tmap\n\n\nMost choropleth maps employ some method of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\n\n\n\n\n\nNote\n\n\n\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\nThe choropleth map on the right shows a quantile data classification with 8 classes are used.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme-1",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Colour Scheme",
    "text": "Colour Scheme\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\n\n\n\n\nNote\n\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill().\nNotice that the word blues is used instead of blue and the alphabet b is in uppercase.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#reference",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#reference",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Reference",
    "text": "Reference\nPrinciples, Concepts and Methods of Choropleth Maps Design\nCore Reading\n\nChoropleth Maps\nThe Basics of Data Classification\n\nAdditional Readings\n\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps\nChoropleth Mapping with Exploratory Data Analysis"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#references",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#references",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "References",
    "text": "References\nAll About tmap package\n\ntmap: Thematic Maps in R\ntmap home page\nSpatial Data Visualization with tmap: A Practical Guide to Thematic Mapping in R\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Take-home_Ex03b.html",
    "href": "Take-home_Ex03b.html",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 2. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex03b.html#setting-the-scene",
    "href": "Take-home_Ex03b.html#setting-the-scene",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nHousing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.\nConventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, Geographical Weighted Models were introduced to better calibrate predictive models for housing resale prices."
  },
  {
    "objectID": "Take-home_Ex03b.html#the-task",
    "href": "Take-home_Ex03b.html#the-task",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "The Task",
    "text": "The Task\nIn this take-home exercise, you are required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023."
  },
  {
    "objectID": "Take-home_Ex03b.html#the-data",
    "href": "Take-home_Ex03b.html#the-data",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.\nBelow is a list of recommended predictors to consider. However, students are free to include other appropriate independent variables.\n\nStructural factors\n\nArea of the unit\nFloor level\nRemaining lease\nAge of the unit\nMain Upgrading Program (MUP) completed (optional)\n\nLocational factors\n\nProxomity to CBD\nProximity to eldercare\nProximity to foodcourt/hawker centres\nProximity to MRT\nProximity to park\nProximity to good primary school\nProximity to shopping mall\nProximity to supermarket\nNumbers of kindergartens within 350m\nNumbers of childcare centres within 350m\nNumbers of bus stop within 350m\nNumbers of primary school within 1km"
  },
  {
    "objectID": "Take-home_Ex03b.html#grading-criteria",
    "href": "Take-home_Ex03b.html#grading-criteria",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use conventional multiple linear regression, random forest and geographically weighted random forest methods learned in class to calibrate the predictive models including model comparison. You will be assessed on your ability:\n\nto describe the methods used correctly, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex03b.html#submission-instructions",
    "href": "Take-home_Ex03b.html#submission-instructions",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex03b.html#due-date",
    "href": "Take-home_Ex03b.html#due-date",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Due Date",
    "text": "Due Date\n10th November 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex03b.html#learning-from-senior",
    "href": "Take-home_Ex03b.html#learning-from-senior",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors."
  },
  {
    "objectID": "Take-home_Ex03b.html#q-a",
    "href": "Take-home_Ex03b.html#q-a",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex03b.html#peer-learning",
    "href": "Take-home_Ex03b.html#peer-learning",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex03b.html#reference",
    "href": "Take-home_Ex03b.html#reference",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Reference",
    "text": "Reference\n\nSpatialML\nSpatialRF\n\n\nResearch articles\nWang, Shuli et. al. (2024) “Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US”, Accident analysis and prevention, Vol.199, p.107528-107528, Article 107528. SMU Library e-journal.\nLotfata, Aynaz & Georganos, Stefanos (2023) “Spatial machine learning for predicting physical inactivity prevalence from socioecological determinants in Chicago, Illinois, USA”, Journal of geographical systems, pp.1-21\nWu, Dongyu ; Zhang, Yingheng ; Xiang, Qiaojun (2024) “Geographically weighted random forests for macro-level crash frequency prediction”, Accident analysis and prevention, Vol.194, p.107370-107370, Article 107370."
  },
  {
    "objectID": "Take-home_Ex02.html",
    "href": "Take-home_Ex02.html",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 2. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex02.html#setting-the-scene",
    "href": "Take-home_Ex02.html#setting-the-scene",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nTourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US$ in 2020.\nFigure below shows the total revenue receipt from tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue from tourism industry have been recovered gradually since September 2021.\n\nHowever, it is important to note that the tourism economy of Thailand are not evenly distributed. Figure below reveals that the tourism economy of Thailand are mainly focus on five provinces, namely Bangkok, Phuket, Chiang Mai, Sukhothai and Phetchaburi."
  },
  {
    "objectID": "Take-home_Ex02.html#objectives",
    "href": "Take-home_Ex02.html#objectives",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Objectives",
    "text": "Objectives\nAs a curious geospatial analytics green horn, you are interested to discover:\n\nif the key indicators of tourism economy of Thailand are independent from space and space and time.\n\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "Take-home_Ex02.html#the-task",
    "href": "Take-home_Ex02.html#the-task",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "The Task",
    "text": "The Task\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "Take-home_Ex02.html#the-data",
    "href": "Take-home_Ex02.html#the-data",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Domestic Tourism Statistics at Kaggle. You are required to use version 2 of the data set.\nThailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set."
  },
  {
    "objectID": "Take-home_Ex02.html#grading-criteria",
    "href": "Take-home_Ex02.html#grading-criteria",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate global and local measures of spatial autocorrelation, and emerging hot spot analysis methods in class to perform the analysis. You will be assessed on your ability:\n\nto describe the methods used correctly, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex02.html#submission-instructions",
    "href": "Take-home_Ex02.html#submission-instructions",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex02.html#due-date",
    "href": "Take-home_Ex02.html#due-date",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Due Date",
    "text": "Due Date\n13th October 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex02.html#learning-from-senior",
    "href": "Take-home_Ex02.html#learning-from-senior",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors.\n\nCHUA YAN TING: Have done well in all five grading criteria especially the geocommunication criterion.\nLIN SHUYAN Geospatial data wrangling is very comprehensively done especially identifying water points located outside Nigeria administrative boundary due to location precision issue.\nLOH SI YING Have done well in all five grading criteria especially the followings: (i) the geospatial wrangling are very comprehensively done including to exclude LGAs without water points from the analysis, (ii) managed to compute the p-values, (iii) Start each analysis by explaining the purpose of the analysis. Managed to relate the analysis results to the location context."
  },
  {
    "objectID": "Take-home_Ex02.html#learning-from-is415",
    "href": "Take-home_Ex02.html#learning-from-is415",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Learning from IS415",
    "text": "Learning from IS415\n\nKHANT MIN NAING: Very well done in all the five grading criteria especially the ability to provide a comprehensive overview of the analysis methods used and discussion on the analysis results.\nMATTHEW HO YIWEN Able to provide a clear and comprehensive discussion on the geospatial data wrangling process and to communicate the analysis results by using appropriate geovisualisation and data visualisation methods."
  },
  {
    "objectID": "Take-home_Ex02.html#q-a",
    "href": "Take-home_Ex02.html#q-a",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex02.html#peer-learning",
    "href": "Take-home_Ex02.html#peer-learning",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex02.html#reference",
    "href": "Take-home_Ex02.html#reference",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Reference",
    "text": "Reference\n\nIMPACT OF COVID-19 ON THAILAND’S TOURISM SECTOR\nCovid: Thailand tourism up but still below pre-pandemic level\nTourism in Thailand\n\n\nResearch articles\n\nUglješa Stankov et. al. (2017) “Spatial autocorrelation analysis of tourist arrivals using municipal data: A Serbian example”, Geographica Pannonica, Vol.21 (2), p.106-114. SMU e-journal.\nKhan, D. et. al. (2017) “Hot spots, cluster detection and spatial outlier analysis of teen birth rates in the U.S., 2003–2012”, Spatial and Spatio-temporal Epidemiology, Vol. 21, pp. 67–75.\nMuhammad Arif & Didit Purnomo (2017) “Measuring Spatial Cluster for Leading Industries in Surakarta with Exploratory Spatial Data Analysis (ESDA)”, Jurnal Ekonomi Pembangunan, Vol. 18 (1), pp. 64-81.\nJoshua T. Fergen * and Ryan D. Bergstrom (2021) “Social Vulnerability across the Great Lakes Basin: A County-Level Comparative and Spatial Analysis”, Sustainability, Vol. 13(13).\nV Putrenko, N Pashynska, and S Nazarenko (2018) “Data Mining of Network Events With Space-Time Cube Application”. In: R Westerholt, F-B Mocnik, and A Zipf (eds.), Proceedings of the 1st Workshop on Platial Analysis (PLATIAL’18), pp. 75–82. SMU e-journal.\nJamie Anne Boschan and Caterina G. Roman (2024) “Hot Spots of Gun Violence in the Era of Focused Deterrence: A Space-Time Analysis of Shootings in South Philadelphia”, Social Sciences, Vol. 13.\nMinkyung Kim and Sangdon Lee (2023) “Identification of Emerging Roadkill Hotspots on Korean Expressways Using Space–Time Cubes”, Int. J. Environ. Res. Public Health, 20(6)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Synopsis",
    "section": "",
    "text": "Where should the next new business outlet be located in order to optimise the profit? What are the location factors that affect the resale prices of HDB housing units? Which are the economic or service activities such as IT professional firms, car workshops, fast food chains (ie. KFC, McDonalds), coffee outlets (Starbucks, Ya Kun Kaya Toast, Toast Box) that tend to be located close to one another and which are the ones that tend to be a distance apart? Do these observed patterns and processes occur at random or are they being constrained by geographical factors? These and many other related questions are the challenges faced by data scientists and data analysts today especially when geographical data are used.\nGeospatial Analytics offers the solutions to these questions by providing data scientists and analysts a problem-driven and data-centric analysis framework focusing on discovering actionable understanding from geographically referenced data. It makes extensive use of geospatial data wrangling, geoprocessing, spatial statistical, geospatial machine learning and spatial data visualisation techniques to support decision- and strategy-making.\nThis course provides students with an introduction to the concepts, principles and methods of geospatial analytics and their practical applications of geospatial analytics in real world operations. Emphasis will be placed on:\n\nperforming geospatial data science tasks such as importing, tidying, manipulating, transforming, projecting and processing geospatial data programmatically,\nvisualising, analysing and describing geographical patterns and processes using appropriate geovisualisation and thematic mapping techniques,\nconducting geospatial analysis by using appropriate spatial statistics and Geomachine learning methods and\ncommunicating the geospatial analysis pipeline in a reproducible report."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Synopsis",
    "section": "Course structure",
    "text": "Course structure\n\nBasic Modules\nThis course comprises ten integrated components as shown below:"
  },
  {
    "objectID": "syllabus.html#grading-summary",
    "href": "syllabus.html#grading-summary",
    "title": "Synopsis",
    "section": "Grading Summary",
    "text": "Grading Summary\n\n\n\nAssignments / Assessment categories\nWeighting\n\n\n\n\nPre-lesson Learning and Hands-on Exercise (1.5% x 10)\n15%\n\n\nClass participation and In-class Exercise (20% x 10)\n20%\n\n\nTimed Individual Assessment\n20%\n\n\nTake-home Exercise (Not more than 3)\n45%\n\n\n\n\nPre-lesson Learning (5%)\nPre-lesson videos and recommended readings exercises will be released one week before the weekly lesson starts. A strict requirement for each class meeting is to complete the assigned readings and video before coming to class. Students are required write down at least one question or issue encountered while viewing the video or reading the recommended articles and post them on Piazza for discussion.\nStudent sharing of insights from readings of assigned materials on Piazza will form a large part of the learning in this course.\n\n\nHands-on Exercise (10%)\nHands-on exercises aim to provide students to gain hands-on experience on using KNIME and GAEK to perform geospatial analysis with real world use cases. It is important for students to complete the hands-on exercises before class.\n\n\nClass Participation and In-class Exercise (20%)\nIn-class exercise and discussion will extend the methods learned from the hands-on exercise to advanced modelling. The in-class discussion will also focus on how to interpret the analysis results and to communicate the analysis results by using appropriate map and data visualisation techniques. Students may also be quizzed in class and thereby contribute to in-class exercise.\n\n\nTimed Individual Assessment (20%)\nA timed individual assessment is designed to evaluate each student’s ability to complete an assignment within a specified time limit. The assessment will involve, but is not limited to, understanding the business problem, assembling the analytical sandbox, performing the analysis, and interpreting and communicating the results. It will be conducted in class and will not exceed 30 minutes.\n\n\nTake-home Exercise (45%)\nThere are three take-home exercises that are due throughout the term. They aim to provide students the opportunities to apply the methods learned in class by working through mini real-world cases. Each take-home exercise is an extension of the hands-on and in-class exercises. What this means is that, for example, in Lesson 2, students will learn the concept of spatial point processes and the hands-on exercise will provide students step-by-step guide on how to use KNIME and GAEK to perform spatial point patterns analysis. The in-class discussion, beside clarification of the concepts and usage of R packages syntax and argument, it will focus more on how to interpret and communicate the analysis results. Then the take-home exercise will require students to synthesise what they have learned from the readings, hands-on exercise and in-class exercise. The estimated workload will be about 6-8 hours per week.\nEach take-home exercise will carry a same weightage of 15%. The deliverable format of the take-home exercises and marking rubric of will be provided on the handout of the take-home exercise. Feedback on take-home exercise will be provided weekly before the weekly lesson starts. This is to ensure that students will learn from mistakes made in the earlier take-home exercise and improve their work progressively in the subsequent take-home exercises.\nStudents may work together to help one another with computer or geospatial issues and discuss the materials that constitute the take-home exercise. However, each student is required to prepare and submit the take-home exercise (including any computer work) on their own. Cheating is strictly prohibited. Cheating includes but not limited to: plagiarism and submission of work that is not the student’s.\n\n\nFinal Exam\nThere will be no final examination for this course."
  },
  {
    "objectID": "syllabus.html#reference",
    "href": "syllabus.html#reference",
    "title": "Synopsis",
    "section": "Reference",
    "text": "Reference\n\nGimond, Manuel. (2018) Introduction to GIS and Spatial Analysis.\nFloch, J.M., Marcon, E. and Puech, F. (2018) Handbook of Spatial Analysis: Theory and Application with R.\nEdzer Pebesma & Roger Bivand (2025) Spatial Data Science with Applications in R.\nRobin Lovelace, Jakub Nowosad & Jannes Muenchow (2024) Geocomputation with R.\nMoraga, Paula. (2023). Spatial Statistics for Data Science: Theory and Practice with R. Chapman & Hall/CRC Data Science Series. ISBN 9781032633510"
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R Resources",
    "section": "",
    "text": "This page provides links to a selected resources to learn modern R packages, especially those related to tidyverse framework."
  },
  {
    "objectID": "R.html#getting-started-with-r",
    "href": "R.html#getting-started-with-r",
    "title": "R Resources",
    "section": "Getting Started with R",
    "text": "Getting Started with R\n\nR for Data Science by Garrett Grolemund and Hadley Wickham.\nModern R with the tidyverse by Bruno Rodrigues. Chapter 2 provides a detail discussion on R data objects.\nBrendan R. E. Ansell Introduction to R - tidyverse\nThe Comprehensive Guide to Installing R Packages from CRAN, Bioconductor, GitHub and Co.. This article provides useful tips on how to install R packages from different sources.\nR Workflow"
  },
  {
    "objectID": "R.html#blog-post",
    "href": "R.html#blog-post",
    "title": "R Resources",
    "section": "Blog post",
    "text": "Blog post\n\nR Workflow from Statistical Thinking."
  },
  {
    "objectID": "outline/workshop.html",
    "href": "outline/workshop.html",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/workshop.html#content",
    "href": "outline/workshop.html#content",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/workshop.html#workshop-slides-and-hands-on-notes",
    "href": "outline/workshop.html#workshop-slides-and-hands-on-notes",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Workshop Slides and Hands-on Notes",
    "text": "Workshop Slides and Hands-on Notes\n\nIntroducing R Shiny slides in html and pdf formats\nAdvanced R Shiny I slides in html and pdf formats\nManaging R Shiny Project slides in html and pdf formats"
  },
  {
    "objectID": "outline/workshop.html#must-do",
    "href": "outline/workshop.html#must-do",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Must Do!",
    "text": "Must Do!\n\nView the three parts series of Shiny Tutorial at this link."
  },
  {
    "objectID": "outline/workshop.html#readings",
    "href": "outline/workshop.html#readings",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Readings",
    "text": "Readings\n\nCore Readings\n\nShiny reference guide at CRAN.\nHadley Wickham (2021) Mastering Shiny, O’Reilly Media. This is a highly recommended book. You can find the online version with this link.\nPaula Moraga (2020) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny, Chapman & Hall/CRC. Chapter 13, 14 and 15.\n\n\n\nAdditional references\n\nColin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard (2020), Engineering Production-Grade Shiny Apps, Chapman & Hall. You can find the online version with this link.\nOutstanding User Interfaces with Shiny.\nHow to Build a Shiny Application from Scratch.\n\n\n\nDeploying Shiny Application on shinyapps.io\n\nGetting started with shinyapps.io\nshinyapps.io user guide"
  },
  {
    "objectID": "outline/workshop.html#gallery",
    "href": "outline/workshop.html#gallery",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Gallery",
    "text": "Gallery\n\nWinners of the 1st Shiny Contest\nWinners of the 2nd Annual Shiny Contest\nWinners of the 3rd annual Shiny Contest\nShiny Gallery\nFifteen New Zealand government Shiny web apps\nShinyApps Gallery\ndreamRs shiny gallery\nTools for Teaching Quantitative Thinking"
  },
  {
    "objectID": "outline/Lesson09_outline.html",
    "href": "outline/Lesson09_outline.html",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geographic Accessibility\nGravitational Law and Distance Decay Function\nIntroducing Potential Models\n\nThe classic model\n\nHansen Potential Accessibility Model\nTwo-step Floating Catchment Area (2SFCA) Method\nKernel Density Two-Step Floating Catchment Area (KD2SFCA) Method\nInterpreting and Visualising Modelling Results"
  },
  {
    "objectID": "outline/Lesson09_outline.html#content",
    "href": "outline/Lesson09_outline.html#content",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geographic Accessibility\nGravitational Law and Distance Decay Function\nIntroducing Potential Models\n\nThe classic model\n\nHansen Potential Accessibility Model\nTwo-step Floating Catchment Area (2SFCA) Method\nKernel Density Two-Step Floating Catchment Area (KD2SFCA) Method\nInterpreting and Visualising Modelling Results"
  },
  {
    "objectID": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 9 slides.\nHands-on Exercise 9 handout."
  },
  {
    "objectID": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nRead before lesson:\n\nLong, J. (2017). Modelling Accessibility. The Geographic Information Science & Technology Body of Knowledge (3rd Quarter 2017 Edition), John P. Wilson (ed.).\nHansen, W. G. (1959): “How Accessibility Shapes Land Use”. Journal of the American Institute of Planners, 25, 2, p. 73-76.\nLuo, W.; Wang, F. (2003) “Measures of spatial accessibility to health care in a GIS environment: synthesis and a case study in the Chicago region”. Environment and Planning B: Planning and Design. 30 (6): 865–884. doi:10.1068/b29120.\nLuo, W.; Qi, Y. (2009). “An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians”. Health & Place. 15 (4): 1100–1107."
  },
  {
    "objectID": "outline/Lesson09_outline.html#reference",
    "href": "outline/Lesson09_outline.html#reference",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Reference",
    "text": "Reference\n\nSection on “Transportation and Accessibility” in The Geography of Transport Systems.\nRich, D.C. (1980) Potential Models in Human Geography.\nOrpana, T./Lampinen, J. (2003) “Building spatial choice models from aggregate data”. Journal of Regional Science,43, 2, p. 319-347.\nTwo-step floating catchment area method.\nCheng, Gang et. al. (2016) “Spatial difference analysis for accessibility to high level hospitals based on travel time in Shenzhen, China” Habitat International, Vol.53, p.485-494.\nPolzin, Pierre ; Borges, José ; Coelho, António (2014) “An Extended Kernel Density Two-Step Floating Catchment Area Method to Analyze Access to Health Care” Environment and planning. B, Planning & design, Vol.41 (4), p.717-735."
  },
  {
    "objectID": "outline/Lesson09_outline.html#all-about-r",
    "href": "outline/Lesson09_outline.html#all-about-r",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "All About R:",
    "text": "All About R:\n\naccessibility\nSpatialAcc package.\nPotential package."
  },
  {
    "objectID": "outline/Lesson07_outline.html",
    "href": "outline/Lesson07_outline.html",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of geographically weighted regression."
  },
  {
    "objectID": "outline/Lesson07_outline.html#content",
    "href": "outline/Lesson07_outline.html#content",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nBasic concepts and principles of linear regression\n\nSimple linear regression\nMultiple linear regression\n\nThe spatial stationarity assumption of multiple linear regression.\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\nInterpreting and Visualising"
  },
  {
    "objectID": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 7 slides.\nHands-on Exercise 7."
  },
  {
    "objectID": "outline/Lesson07_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson07_outline.html#self-reading-before-meet-up",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524."
  },
  {
    "objectID": "outline/Lesson07_outline.html#references",
    "href": "outline/Lesson07_outline.html#references",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "References",
    "text": "References\n\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "outline/Lesson07_outline.html#all-about-r",
    "href": "outline/Lesson07_outline.html#all-about-r",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "All About R",
    "text": "All About R\n\nGWmodel package, especially\n\nGollini, I et. al. (2015) “GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models”, Journal of Statistical Software, Volume 63, Issue 17 and\nBinbin Lu, Paul Harris, Martin Charlton & Chris Brunsdon (2014) “The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models”, Geo-spatial Information Science, 17:2, 85-101, DOI: 10.1080/10095020.2014.917453.\n\nlctools package especially gw() and gwr() related functions.\nspgwr implements of geographically weighted regression methods for exploring possible non-stationarity.\ngwrr: its geographically weighted regression (GWR) models and has tools to diagnose and remediate collinearity in the GWR models. Also fits geographically weighted ridge regression (GWRR) and geographically weighted lasso (GWL) models."
  },
  {
    "objectID": "outline/Lesson05_outline.html",
    "href": "outline/Lesson05_outline.html",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this lesson, you will learn a collection of geospatial statistical methods specially designed for measuring global and local spatial association.\nThese spatial statistics are well suited for:"
  },
  {
    "objectID": "outline/Lesson05_outline.html#content",
    "href": "outline/Lesson05_outline.html#content",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Content",
    "text": "Content\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)"
  },
  {
    "objectID": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 5 slides.\nHands-on Exercise 5: Global Measures of Spatial Autocorrelation.\nHands-on Exercise 5: Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "outline/Lesson05_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson05_outline.html#self-reading-before-meet-up",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127.\nGetis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306.\n\nThese six papers are classics of Global and Local Spatial Autocorrelation. Be warned: All classic papers assume that the readers are academic researchers."
  },
  {
    "objectID": "outline/Lesson05_outline.html#references",
    "href": "outline/Lesson05_outline.html#references",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "References",
    "text": "References\n\nD. A. Griffith (2009) “Spatial autocorrelation”.\nGetis, A., 2010 “B.3 Spatial Autocorrelation” in Fischer, M.M., and Getis, A. 2010 Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications, Springer.\nAnselin, L. (1996) “The Moran scatterplot as an ESDA tool to assess local instability in spatial association”\nGriffith, Daniel (2009) “Modeling spatial autocorrelation in spatial interaction data: empirical evidence from 2002 Germany journey-to-work flows”. Journal of Geographical Systems, Vol.11(2), pp.117-140.\nCelebioglu, F., and Dall’erba, S. (2010) “Spatial disparities across the regions of Turkey: An exploratory spatial data analysis”. The Annals of Regional Science, 45:379–400.\nMack, Z.W.V. and Kam T.S. (2018) “Is There Space for Violence?: A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict” Proceedings of 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities (ACM SIGSPATIAL’18). Seattle, Washington, USA, 10 pages.\nTAN, Yong Ying and KAM, Tin Seong (2019). “Exploring and Visualizing Household Electricity Consumption Patterns in Singapore: A Geospatial Analytics Approach”, Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31–April 3, 2019, Proceedings. Pp 785-796.\nSingh A., Pathak P.K., Chauhan R.K., and Pan, W. (2011) “Infant and Child Mortality in India in the Last Two Decades: A Geospatial Analysis”. PLoS ONE 6(11), 1:19."
  },
  {
    "objectID": "outline/Lesson03_outline.html",
    "href": "outline/Lesson03_outline.html",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "",
    "text": "In this lesson, you will learn two advanced spatial point patterns analysis methods, they are: spatiotemporal KDE and the Network Constrained KDE. Using real-world use cases, you will also gain hands-on experience on using spNetwork to analyse spatial point patterns and temporal spatial point event along networks."
  },
  {
    "objectID": "outline/Lesson03_outline.html#content",
    "href": "outline/Lesson03_outline.html#content",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nSpatiotemporal Kernel Density Estimation\nNetwork Constrained Kernel Density Estimation (NCKDE)\n\nBasic concepts of network constrained spatial point patterns\nNetwork Constrained KDE methods\nThe Three versions of Network Constrained KDE\n\nTemporal Network Kernel Density Estimation (TNKED)\n\nTemporal dimension\nSpatial dimension\nSpatiotemporal point patterns\nThe Temporal Network Kernel Density Estimation method\n\n\n\nLesson Slides and Hands-on Notes\n\nLesson 3 slides\nHands-on Exercise 3: Network Constrained Spatial Point Patterns Analysis."
  },
  {
    "objectID": "outline/Lesson03_outline.html#references",
    "href": "outline/Lesson03_outline.html#references",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nATSUYUKI OKABE, TOSHIAKI SATOH & KOKICHI SUGIHARA (2009) “A kernel density estimation method for networks, its computational method and a GIS-based tool”, International Journal of Geographical Information Science, Vol. 23, No. 1, January 2009, pp. 7–32.\nIkuho Yamada & Jean-Claude Thill (2007) “Local Indicators of Network-Constrained Clusters in Spatial Point Patterns”, Geographical Analysis, Vol. 39, pp 268–292.\nJérémy Gelb & Philippe Apparicio (2023) “Temporal Network Kernel Density Estimation”, Geographical Analysis. (Online open access version)"
  },
  {
    "objectID": "outline/Lesson03_outline.html#all-about-r",
    "href": "outline/Lesson03_outline.html#all-about-r",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nspNetwork: An R package to perform spatial analysis on networks.\n\nDetails about NKDE\nNetwork k Functions\nNetwork Kernel Density Estimate\nTemporal Network Kernel Density Estimate"
  },
  {
    "objectID": "outline/Lesson01_outline.html",
    "href": "outline/Lesson01_outline.html",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of three parts. First, it provides an overview of geospatial analytics. Second, it explains the popular geospatial models used to store geographical data. The methods used to import, integrate, wrangle, process geospatial data will be discussed too. Lastly, the basic principles and concepts of thematic mapping and geovisualisation will be introduced.\nThe hands-on exercises will allow you to gaion hands-on experience on using:\n\nsf package to import and wrangle vector-based data,\nterra package to import and wrangle raster-based data, and\ntmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson01_outline.html#overview",
    "href": "outline/Lesson01_outline.html#overview",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of three parts. First, it provides an overview of geospatial analytics. Second, it explains the popular geospatial models used to store geographical data. The methods used to import, integrate, wrangle, process geospatial data will be discussed too. Lastly, the basic principles and concepts of thematic mapping and geovisualisation will be introduced.\nThe hands-on exercises will allow you to gaion hands-on experience on using:\n\nsf package to import and wrangle vector-based data,\nterra package to import and wrangle raster-based data, and\ntmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson01_outline.html#content",
    "href": "outline/Lesson01_outline.html#content",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geospatial Analytics\n\nDemystifying Geospatial Analytics\nMotivation of Geospatial Analytics\nA Tour Through the Geospatial Analytics Zoo\nGeospatial Analytics and Social Consciousness\n\nFundamentals of Geospatial Data Models\n\nVector and raster data model\nCoordinate systems and map projection\nHandling and wrangling vector data in R: sf methods ````- Handling and wrangling raster data in R: terra methods\n\nFundamentals of Geospatial Data Visualisation and tmap Methods\n\nClassification of maps\nPrinciples of map design\nThematic mapping techniques\ntmap methods"
  },
  {
    "objectID": "outline/Lesson01_outline.html#lesson-slides",
    "href": "outline/Lesson01_outline.html#lesson-slides",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 1: Introduction to Geospatial Analytics slides.\nLesson 1: Fundamentals of Geospatial Data Models and Modelling slides.\nLesson 1: Fundamentals of Geospatial Data Visualisation and tmap Methods slides."
  },
  {
    "objectID": "outline/Lesson01_outline.html#self-reading-before-lesson",
    "href": "outline/Lesson01_outline.html#self-reading-before-lesson",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Self-reading Before Lesson",
    "text": "Self-reading Before Lesson\n\n“Spatial Data, Spatial Analysis, Spatial Data Science” by Prof. Luc Anselin. (This is a long lecture 1hr 15minutes but don’t turn away just because it is lengthy.)\nXie, Yiqun et. al. (2017) “Transdisciplinary Foundations of Geospatial Data Science” ISPRS International Journal of Geo-information, 2017, Vol.6 (12), p.395."
  },
  {
    "objectID": "outline/Lesson01_outline.html#hands-on-exercise",
    "href": "outline/Lesson01_outline.html#hands-on-exercise",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nHands-on Exercise 1: Geospatial Data Wrangling with R\nHands-on Exercise 1: Choropleth Mapping with R"
  },
  {
    "objectID": "outline/Lesson01_outline.html#all-about-r",
    "href": "outline/Lesson01_outline.html#all-about-r",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "All About R",
    "text": "All About R\n\nR packages for Data Science\n\nsf package.\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\n\ntidyverse: a family of modern R packages specially designed to meet the tasks of Data Science in R.\n\nreadr: a fast and effective library to parse csv, txt, and tsv files as tibble data.frame in R. To get started, refer to Chapter 11 Data import of R for Data Science book.\n\ntidyr: an R package for tidying data. To get started, refer to Chapter 5 Data tidying of R for Data Science book.\n\ndplyr: a grammar of data manipulation. To get started, read articles under Getting Started and Articles tabs.\nggplot2: a grammar of graphics. To get started, read Chapter 1: Data Visualization, Chapter 10 Exploratory Data Analysis and Chapter 11 Communication of R for Data Science (2ed) book.\npipes: a powerful tool for clearly expressing a sequence of multiple operations. To get started, read Chapter 5 Workflow: pipes of R for Data Science (2ed) book.\n\n\n\n\nR Package for GeoVisualisation and Thematic Mapping\n\nTennekes, M. (2018) “tmap: Thematic Maps in R”, Journal of Statistical Software, Vol 84:6, 1-39.\ntmap: thematic maps in R package especially:\n\ntmap: get started!,\ntmap: version changes, and\nChapter 8 Making maps with R of Geocomputation with R."
  },
  {
    "objectID": "outline/Lesson01_outline.html#references",
    "href": "outline/Lesson01_outline.html#references",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "References",
    "text": "References\n\nGeospatial Analytics\n\nPaez, A., and Scott, D.M. (2004) “Spatial statistics for urban analysis: A review of techniques with examples”, GeoJournal, 61: 53-67. Available in SMU eLibrary.\n“Geospatial Analytics Will Eat The World, And You Won’t Even Know It”.\n\n\n\nGeoVisualisation and Thematic Mapping\n\nProportional Symbols\nChoropleth Maps\nThe Basics of Data Classification\nChoropleth Mapping with Exploratory Data Analysis\nThe Concept of Map Symbols\nChoropleth map\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html",
    "href": "lesson/Lesson10/Lesson10-SIM.html",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "",
    "text": "Characteristics of Spatial Interaction Data\nSpatial Interaction Models\n\nUnconstrained\nOrigin constrined\nDestination constrained\nDoubly constrained\n\nWhat is Spatial Econometrics?\nWhat is Spatial Econometric Interaction Models?\nIntroducing spflow package\nSpatial Econometric Modelling of O-D Flows: spflow application"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#content",
    "href": "lesson/Lesson10/Lesson10-SIM.html#content",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "",
    "text": "Characteristics of Spatial Interaction Data\nSpatial Interaction Models\n\nUnconstrained\nOrigin constrined\nDestination constrained\nDoubly constrained\n\nWhat is Spatial Econometrics?\nWhat is Spatial Econometric Interaction Models?\nIntroducing spflow package\nSpatial Econometric Modelling of O-D Flows: spflow application"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-spatial-interaction-models-are",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-spatial-interaction-models-are",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What Spatial Interaction Models are?",
    "text": "What Spatial Interaction Models are?\n\n\nSpatial interaction or “gravity models” estimate the flow of people, material, or information between locations in geographical space.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSpatial interaction models seek to explain existing spatial flows. As such it is possible to measure flows and predict the consequences of changes in the conditions generating them. When such attributes are known, it is possible to better allocate transport resources such as conveyances, infrastructure, and terminals.\n\n\n\n\n\n\nConditions for Spatial Flows\n\nThree interdependent conditions are necessary for a spatial interaction to occur:\n\n\n\n\nComplementarity. There must be a supply and a demand between the interacting locations. A residential zone is complementary to an employment zone because the first is supplying workers while the second is supplying jobs. The same can be said concerning the complementarity between a store and its customers and between an industry and its suppliers (movements of freight). An economic system is based on a large array of complementary activities.\nIntervening opportunity (lack of). Refers to a location that may offer a better alternative as a point of origin or as a point of destination. For instance, in order to have an interaction of a customer to a store, there must not be a closer store that offers a similar array of goods. Otherwise, the customer will likely patronize the closer store and the initial interaction will not take place.\nTransferability. Mobility must be supported by transport infrastructures, implying that the origin and the destination must be linked. Costs to overcome distance must not be higher than the benefits of the related interaction, even if there are complementarity and no alternative opportunity.\n\n\n\n\n\nRepresentation of a Movement as a Spatial Interaction\n\n\nRepresenting mobility as a spatial interaction involves several considerations:\n\nLocations: A movement is occurring between a location of origin and a location of destination. i generally denotes an origin while j is a destination.\nCentroid: An abstraction of the attributes of a zone at a point.\nFlows: Flows are generally expressed by a valued vector Tij representing an interaction between locations i and j.\nVectors: A vector Tij links two centroids and has a value assigned to it (50) which can represents movements.\n\n\n\n\n\n\n\nLocations. A movement is occurring between a location of origin and a location of destination. i generally denotes an origin while j is a destination. The representation of origins and destinations commonly involves centroids.\nCentroid. An abstraction of the attributes of a zone at a point. This is of particular relevance when the attributes generating mobility are zonal (e.g. ZIP codes, cities, states, etc.) while the graphic representation requires specific origins and destinations. For instance, showing flows between ZIP codes would implicitly require the generation of one centroid for each ZIP code.\nFlows. Flows are generally expressed by a valued vector Tij representing an interaction between locations i and j.\nVectors. On the above figure, two areas, zone i and zone j, are represented as two centroids, i and j. A vector Tij links two centroids and has a value assigned to it (50) which can represents movements such as tons of freight, numbers of passengers per day, or number of phone calls."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#constructing-an-od-matrix",
    "href": "lesson/Lesson10/Lesson10-SIM.html#constructing-an-od-matrix",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Constructing an O/D Matrix",
    "text": "Constructing an O/D Matrix\n\nThe construction of an origin / destination matrix requires directional flow information between a series of locations.\nFigure below represents movements (O/D pairs) between five locations (A, B, C, D and E). From this graph, an O/D matrix can be built where each O/D pair becomes a cell. A value of 0 is assigned for each O/D pair that does not have an observed flow."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#three-basic-types-of-interaction-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#three-basic-types-of-interaction-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Three Basic Types of Interaction Models",
    "text": "Three Basic Types of Interaction Models\n\nThe general formulation of the spatial interaction model is stated as Tij, which is the interaction between location i (origin) and location j (destination). Vi are the attributes of the location of origin i, Wj are the attributes of the location of destination j, and Sij are the attributes of separation between the location of origin i and the location of destination j.\nFrom this general formulation, three basic types of interaction models can be derived:"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#gravity-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#gravity-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Gravity Models",
    "text": "Gravity Models\n\n\n\nThe general formula (also known as unconstrained):\n\n\nTij is the transition/trip or flow, 𝑇, between origin 𝑖 (always the rows in a matrix) and destination 𝑗 (always the columns in a matrix). If you are not overly familiar with matrix notation, the 𝑖 and 𝑗 are just generic indexes to allow us to refer to any cell in the matrix.\n𝑉 is a vector (a 1 dimensional matrix – or, if you like, a single line of numbers) of origin attributes which relate to the emissivity of all origins in the dataset, 𝑖 – this could be any of the origin-related variables.\n\n\n\n𝑊 is a vector of destination attributes relating to the attractiveness of all destinations in the dataset, 𝑗 – similarly, this could be any of the destination related variables.\n𝑑 is a matrix of costs (frequently distances – hence, d) relating to the flows between 𝑖 and 𝑗.\n𝑘, 𝜇, 𝛼 and 𝛽 are all model parameters to be estimated. 𝛽 is assumed to be negative, as with an increase in cost/distance we would expect interaction to decrease.\n\n\n\n\n\n\nUnconstrained (Totally constrained) case\n\n\nThe O-D Matrix\n\nand distance matrix:\n\n\n\nThe estimated O-D matrix:\n\nand the calculation T11\n\n\n\n\n\n\nThe Origin (Production) Constrained Model\n\n\nIn the Origin Constrained Model,\n\n𝑂𝑖 does not have a parameter as it is a known constraint.\n𝐴𝑖 is known as a balancing factor and is a vector of values which relate to each origin 𝑖 which do the equivalent job as 𝑘 in the unconstrained/total constrained model but ensure that flow estimates from each origin sum to the know totals 𝑂𝑖 rather than just the overall total.\n\n\n\n\n\n\n\n\nOringin (Production) constrained case\n\n\nThe O-D Matrix\n\nand distance matrix:\n\n\n\nThe estimated O-D matrix:\n\nA1 is calculated as shown below:\n\nHence, T11 is calculated as shown below:\n\n\n\n\n\n\nThe Destination (Attraction) Constrained Model\n\n\n\n\n\n\n\n\nDestination (Attraction) constrained case\n\n\nThe O-D Matrix\n\nand distance matrix:\n\n\n\nThe estimated O-D matrix:\n\nB1 is calculated as shown below:\n\nHence, T11 is calculated as shown below:\n\n\n\n\n\n\nThe Doubly Constrained Model\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the calculation of 𝐴𝑖 relies on knowing 𝐵𝑗 and the calculation of 𝐵𝑗 relies on knowing 𝐴𝑖 – something of a conundrum to which the solution is elegantly described by Senior (1979), who sketches out a very useful algorithm for iteratively arriving at values for 𝐴𝑖 and 𝐵𝑗 by setting each to equal 1 initially and then continuing to calculate each in turn until the difference between successive iterations of the 𝐴𝑖 and 𝐵𝑗 values is small enough not to matter.\n\n\n\n\n\n\n\nDoubly constrained case\n\n\nThe O-D Matrix\n\nand distance matrix:\n\n\n\nThe estimated O-D matrix:\n\nHence, T11 is calculated as shown below:\n\nNotice that A1 and B1 are computed by using computer."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Interaction Models",
    "text": "Spatial Econometric Interaction Models\n\nLimitation of Spatial Interaction Models\n\nThe gravity model assumes independence among observations, and this assumption seems heroic for many fundamentally spatial problems.\n\n\n\n\n\nLimitation of Spatial Interaction Models\nThere are two technical limitations of SIM:\n\nMaximum-likelihood estimates require that the dependent variable vector follow a normal distribution or that it can be suitably transformed to achieve normality.\nMaximum-likelihood method is not appropriate when there are a large number of zero flows exist."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-model-for-origin-destination-flows",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-model-for-origin-destination-flows",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Model for Origin-Destination Flows",
    "text": "Spatial Econometric Model for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-model",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-model",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Interaction Model",
    "text": "Spatial Econometric Interaction Model\nThe general formula of Spatial Econometric Interaction Model is defined as follow:\n\nwhere by 𝐖𝓭, 𝐖𝒐 and 𝐖𝓌 are spatial weights of destination, origin and origin-destination."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Econometrics",
    "text": "What is Econometrics\n\nEconometrics is an application of statistical methods to economic data in order to give empirical content to economic relationships. More precisely, it is “the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference.\nA basic tool for econometrics is the multiple linear regression model.\nEconometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods.\nEconometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency.\nApplied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Spatial Econometrics?",
    "text": "What is Spatial Econometrics?\n\nA branch of economics that deals with the study of economic phenomena that exhibit spatial dependence.\nThis branch of economics has its roots in classical economics, which focused on the study of how economic activity was related to the location of factors of production.\nClassical economists developed theories of how businesses locate themselves in relation to their markets and to each other. These theories formed the basis for the development of modern spatial economics."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics-1",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics-1",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Spatial Econometrics?",
    "text": "What is Spatial Econometrics?\n\nIn a broader sense it is inclusive of the models and theoretical instruments of spatial statistics and spatial data analysis to analyze various economic effects such as externalities, interactions, spatial concentration and many others.\nDiscrete spatial data can take the form of points, lines and polygons. Point data refer to the position of the single economic agent observed at an individual level. Lines in space take the form of interactions between two spatial locations such as flows of goods, individuals and information. Finally data observed within polygons can take the form of predefined irregular portions of space, usually administrative partitions such as countries, regions or counties within one country."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-are-the-examples-of-applications-using-spatial-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-are-the-examples-of-applications-using-spatial-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What Are The Examples of Applications Using Spatial Econometrics ?",
    "text": "What Are The Examples of Applications Using Spatial Econometrics ?\nThere are many applications for spatial econometrics . Here are a few examples :\n\nEvaluating the impact of a new road or railway on property values.\nEstimating the effect of environmental regulations on firm location decisions.\nAnalyzing the determinants of crime rates across neighborhoods\nStudying the relationship between house prices and income levels in different regions.\nInvestigating the spread of infectious diseases through a population.\nModeling the relationship between land values and location-specific services.\nAnalyzing the relationship between proximity to facilities and job opportunities ."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-1",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-1",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-2",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-2",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html",
    "href": "lesson/Lesson08/Lesson08-GWRF.html",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#content",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#content",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#what-is-predictive-modelling",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#what-is-predictive-modelling",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "What is Predictive Modelling?",
    "text": "What is Predictive Modelling?\n\n\n\nPredictive modelling uses statistical learning or machine learning techniques to predict outcomes.\n\nBy and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#what-is-geospatial-predictive-modelling",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#what-is-geospatial-predictive-modelling",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "What is Geospatial Predictive Modelling",
    "text": "What is Geospatial Predictive Modelling\n\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution.\n\nWhen geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur.\nGeospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences.\n\n\n\n\nDifferences between Explanatory Modelling and Predictive Analytics"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#predictive-modelling-process",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#predictive-modelling-process",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Predictive Modelling Process",
    "text": "Predictive Modelling Process\n\n\n\n\n\n\n\nData Sampling in Predictive Analytics\n\n\n\n\nTraining dataset: This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.\nValidation dataset: This data set is used to give an estimate of model skill while tuning model’s hyperparameters. It aims to avoiding over-fitting the predictive model.\nTest dataset: The is also held back from the training of the model, but is instead used to give an unbiased estimate of the skill of the final tuned model when comparing or selecting between final models.\n\n\n\n\n\n\n\n\n\nComparing Predictive Performance\n\n\n\nThe need for model comparison arises from the wide choice of classifiers and predictive methods.\n\nNot only do we have several different methods, but even within a single method there are usually many options that can lead to completely different results.\n\nIn practice, modelers often use several tools, sometimes both graphical and numerical, to choose a best model.\n\n\n\n\nMean Squared Error (MSE) (also known as Average Squared Error (ASE))\nAkaike information criterion (AIC)\nBayesian Information Criterion (BIC)"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#introducing-recursive-partitioning",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#introducing-recursive-partitioning",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Introducing recursive partitioning",
    "text": "Introducing recursive partitioning\n\nA predictive methodology involving a dependent variable y and one and more predictors.\nThe dependent variable can be either a continuous or categorical scales.\nRules partition data into mutually exclusive groups.\nNo need to worry about transformations such as logs.\nNo prior distribution requirement.\n\n\n\nRecursive Partitioning as a Machine Learning engine\n\n\n\nAs a machine learning technique, recursive partitioning algorithms operate by building a model based on the training dataset and using that to make predictions or decisions, rather than following only explicitly programmed instructions.\n\n\n\n\n\n\n\n\n\nProperties of Recursive Partitioning\n\n\n\nIf the response is categorical, then it is fitting the probabilities estimated for the response levels, minimizing the residual log-likelihood chi-square [2*entropy].\n\n\n\nIf the response is continuous, then the platform fits means, minimizing the sum of squared errors. The earlier is popularly known as Classification Trees and the later is known as Regression Trees.\n\n\n\n\n\n\nProperties of Recursive Partitioning\n\n\n\nWorking with continuous predictor(s)\n\nIf a predictor is continuous, then the partition is done according to a splitting “cut” value for X. For example, Average Days Delinquent &lt; 12.3 or &gt;=12.3 as shown in the figure.\n\n\n\n\n\nWorking with categorical predictor(s)\n\nIf the predictor is categorical, then it divides the X categories into two groups of levels and considers all possible groupings into two levels.\n\n\n\n\n\n\n\n\nComponents of Classification and Regression Tree (CART)\n\n\nA CART is read from the top down starting at the root node.\n\nEach internal node represents a split based on the values of one of the inputs. The inputs can appear in any number of splits throughout the tree. Cases move down the branch that contains its input value.\nThe terminal nodes of the tree are called leaves. The leaves represent the predicted target. All cases reaching a particular leaf are given the same predicted value.\n\n\n\n\n\n\n\n\nSome useful features and advantages of Recursive Partitioning\n\nRecursive partitioning is nonparametric and therefore does not rely on data belonging to a particular type of distribution.\nRecursive partitioning is not significantly impacted by outliers in the input variables.\nYou can relax stopping rules to “overgrow” decision trees and then prune back the tree to the optimal size. This approach minimizes the probability that important structure in the data set will be overlooked by stopping too soon.\nRecursive partitioning incorporates both testing with a test data set and cross-validation to assess the goodness of fit more accurately.\nRecursive partitioning can use the same variables more than once in different parts of the tree. This capability can uncover complex interdependencies between sets of variables.\nRecursive partitioning can be used in conjunction with other prediction methods to select the input set of variables."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#advanced-recursive-partitioning-random-forest",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#advanced-recursive-partitioning-random-forest",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Advanced Recursive Partitioning: Random Forest",
    "text": "Advanced Recursive Partitioning: Random Forest\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble.\n\nEach individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction as shown the figure."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Introducing Geographically Weighted Random Forest (gwRF)",
    "text": "Introducing Geographically Weighted Random Forest (gwRF)\n\nGeographically Weighted Random Forest (GRF) is a spatial analysis method using a local version of the famous Machine Learning algorithm.\n\nThis technique adopts the idea of the Geographically Weighted Regression.\nThe main difference between a tradition (linear) GWR and GRF is that we can model non-stationarity coupled with a flexible non-linear model which is very hard to overfit due to its bootstrapping nature, thus relaxing the assumptions of traditional Gaussian statistics."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html",
    "href": "lesson/Lesson06/Lesson06.html",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "",
    "text": "Introduction to Geographic Segmentation\nSpatialising classic clustering methods\nSpatially Constrained Clustering - Hierarchical methods\n\nskater\nREDCAP\nclustGeo\n\nSpatially Constrained Clustering - Partitioning methods\n\nAutomatic zoning procedure (AZP)\nmax-p heuristic"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#content",
    "href": "lesson/Lesson06/Lesson06.html#content",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geographic Segmentation\nSpatialising classic clustering methods\nSpatially Constrained Clustering - Hierarchical methods\n\nskater\nREDCAP\nclustGeo\n\nSpatially Constrained Clustering - Partitioning methods\n\nAutomatic zoning procedure (AZP)\nmax-p heuristic"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#regionalisation-and-clustering",
    "href": "lesson/Lesson06/Lesson06.html#regionalisation-and-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Regionalisation and Clustering",
    "text": "Regionalisation and Clustering\n\n\n\nRegionalisation is a process of to group a large number of geographical units such as provinces, districts or counties spatial objects into a smaller number of subsets of objects also known as regions, which are internally homogeneous and occupy contiguous regions in space.\nThe process taking into consideration multivariates. Figure on the right shows regions delineated by using six ICT measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home of Shan State, Myanmar."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "href": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering\n\n\nA comparison of conventional non-spatially constrained clustering and spatially constrained clustering. Stars represent the centroids of sampled grid cells and polygons are Thiessen polygons that contain the centroids. Grey shading contrasts between polygons stand for the Simpson dissimilarity index (βsim) between them. Non-spatially constrained clustering produces two clusters, one of which contains polygons (C, D, and E) that are spatially disjoint. In contrast, the two clusters produced by the spatially constrained clustering form two spatially contiguous regions."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering",
    "href": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering\n\nSKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\nREDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning algorithm\nClustGeo algorithm"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "href": "lesson/Lesson06/Lesson06.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm",
    "text": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\n\n\nThe SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) builds off of a connectivity graph to represent spatial relationships between neighbouring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity between neighbouring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#redcap-algorithm",
    "href": "lesson/Lesson06/Lesson06.html#redcap-algorithm",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "REDCAP algorithm",
    "text": "REDCAP algorithm\n\nRegionalization with dynamically constrained agglomerative clustering and partitioning, in short REDCAP is specially developed by D. Guo (2008) to the limitation of SKATER discussed in previous slide.\nLike SKATER, REDCAP starts from building a spanning tree with 4 different ways (single-linkage, average-linkage, ward-linkage and the complete-linkage). The single-linkage way leads to build a minimum spanning tree. Then,REDCAP provides 2 different ways (first-order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is exactly the same with SKATER."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#clustgeo-package",
    "href": "lesson/Lesson06/Lesson06.html#clustgeo-package",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "ClustGeo Package",
    "text": "ClustGeo Package\nThe R package ClustGeo implements a Ward-like hierarchical clustering algorithm including spatial/geographical constraints.\n\nTwo dissimilarity matrices D0 and D1 are inputted, along with a mixing parameter alpha in [0,1]. The dissimilarities can be non-Euclidean and the weights of the observations can be non-uniform.\nThe first matrix gives the dissimilarities in the “feature space”” and the second matrix gives the dissimilarities in the “constraint space”.\nThe criterion minimized at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1. The idea is to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest i.e. those of the feature space."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "Introduction to Spatial Weights\nContiguity-Based Spatial Weights\n\nRook’s\nQueen’s\n\nDistance-Band Spatial Weights\nApplications of Spatial Weights\n\n\nThis lesson consists of three main sections. First, I am going to explain the basic concept of spatial weights. This is followed by a discussion on various methods used to compute spatial weights. Lastly, the applications of spatial wiehgts will be discussed.\n\n\n\n\ngeodemog &lt;- read_rds(\"data/geodemog.RDS\")\nglimpse(geodemog)\n\nRows: 323\nColumns: 12\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;fct&gt; MSSZ01, OTSZ01, SRSZ03, BMSZ08, BMSZ03, BMSZ07, BMSZ0…\n$ PLN_AREA_N       &lt;fct&gt; MARINA SOUTH, OUTRAM, SINGAPORE RIVER, BUKIT MERAH, B…\n$ PLN_AREA_C       &lt;fct&gt; MS, OT, SR, BM, BM, BM, BM, SR, QT, QT, QT, BM, ME, R…\n$ REGION_N         &lt;fct&gt; CENTRAL REGION, CENTRAL REGION, CENTRAL REGION, CENTR…\n$ REGION_C         &lt;fct&gt; CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, C…\n$ YOUNG            &lt;dbl&gt; NA, 1100, 0, 2620, 2840, 2910, 2850, 0, 1120, 30, NA,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; NA, 3420, 50, 7500, 6260, 7560, 8340, 50, 2750, 210, …\n$ AGED             &lt;dbl&gt; NA, 2110, 20, 3260, 1630, 3310, 3590, 10, 560, 50, NA…\n$ TOTAL            &lt;dbl&gt; NA, 6630, 70, 13380, 10730, 13780, 14780, 60, 4430, 2…\n$ DEPENDENCY       &lt;dbl&gt; NA, 0.9385965, 0.4000000, 0.7840000, 0.7140575, 0.822…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n\n\n\nA kind of data that is very similar to an ordinary data. The only difference is that each observation is associated with some form of geography such as numbers of aged population by planning subzone.\n\n\n\n\nThe dependency ratio values by planning subzone are normally distributed.\n\n\n\n\n\n\n\n\n\n\nIn statistics, we are interested to study if the observed distribution resemble normal distribution. To provide an answer to the question, we have to go beyond EDA and apply Confirmatory Data Analysis or also know as statistical test to confirm the observated pattern is indeed resemble normal distribution.\n\n\n\n\n\nAre the planning subzones with high proportion of dependency ratio randomly distributed over space?\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\nWhen come of geographically referenced attributes, beside the distribution of the value, we are also interested to find out if the observed geographical patterns are randomly distributed. Similarly to statistical, geovisualisation is not enough because the visual impression will change when different data classification methods is used.\n\n\n\n\n\nAre the planning subzones with high proportion of dependency ratio randomly distributed over space?\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\nAs a result, a formal statistical test of spatial randomness is needed.\n\n\n\n\n\n\nA way to define spatial neighbourhood.\n\n\n\nBefore we can perform statistics test of spatial randomness, we need to understand how spatial relationship among geographical areas can be defined mathematically.\n\n\n\n\n\n\nThere are at least two popular methods can be used to define spatial weights of geographical areas. They are contiguity and distance. Both of them are encoded in binary.\nAccording to contiguity principle, two geographical areas are consider as neighbour if they share a common boundary. Hence, if two geographical areas are neighbour, they will be indicated as 1. On the other hand, if the two geographical areas are not neighbour, they will be indicated as 0.\nAccording to distance principle, two geographical areas are consider as neighbour if their centroid as within a spefic distance. Hence, if two geographical areas are neighbour, they will be indicated as 1. On the other hand, if the two geographical areas are not neighbour, they will be indicated as 0.\n\n\n\n\n\n\nContiguity (common boundary)\nWhat is a “shared” boundary?\n\n\n\nThis figure shows three different ways to define contiguity neighbours. They are Rooks, Bishop and Queens methods. Rooks and Queens are the two commonly used methods.\nThe main different between Queens and Rooks is that Rooks only consider geographical areas that shared common boundaries but Queens method includes geographical areas touching at the tips of the target geographical area.\n\n\n\n\n\nThere are also second-order, third-order, forth-order, etc contiguity\n\n\nThe figure show contiguity neighbours beyond the first order. As you can see that the second-order contiguity neighbours are also defined according to Rooks and Queens case.\n\n\n\n\n\nQuiz: With reference to the figure below, list down the neighbour(s) of area 1202 using Rook case.\n\n\nThis figure shows a geographical area with nine locations. Let us focus on location 1202. According to Queens case, it has seven neighbours. They are 1101, 1201, 1301, 1102, 1302, 1000, and 0901. If Rooks case is consider, 1101 and 1301 will not be considered as neighbours because they do not share common boundaries with 1202.\n\n\n\n\n\nQuiz: With reference to the figure below, create a weights matrix for d = 650.\n\n\nIn this figure, a cut-off distance of 650m is used. In this case, locations B, C and D are considered as neighbours of location A because all of them are with the 650m distance from A. On the other hand, location E is not a neighbour of location A because their distance is 757m apart.\n\n\n\n\n\n\n\nIn this figure, spatial weights are calculated as the inversed function of the distance. In other word, two locations that are closer will be given higher weight than two locations that are further away. For example, the distance of AB is 353 and the diatnce of AE is 757 and the spatial weights are 0.00283 and 0.00132 respectively.\n\n\n\n\n\nIn practice, row-standardised weights instead of spatial weights will be used.\n\n\nRow-standardised weights increase the influence of links from observations with few neighbours, which binary weights vary the influence of observations. + Those with many neighbours are up-weighted compared to those with few. + In this case, raw 1 have four neighbours, hence each neighbour will have a value of 1/4. On the other hand, row number 2, has only two neighbour, hence each neighbour will be given a value of 1/2.\n\n\n\n\n\nFormally, for observation i, the spatial lag of yi, referred to as [Wy]i (the variable Wy observed for location i) is:\n\nwhere the weights wij consist of the elements of the i-th row of the matrix W, matched up with the corresponding elements of the vector y.\n\nWith a neighbor structure defined by the non-zero elements of the spatial weights matrix W, a spatially lagged variable is a weighted sum or a weighted average of the neighboring values for that variable. In most commonly used notation, the spatial lag of y is then expressed as Wy.\n\n\n\n\nSpatial lag with row-standardized weights.\n\n\nThis figure shows the spatially lagged of GPDPC summed up the GDPPC of all its neighbours except the target location itself.\n\n\n\n\n\nThe spatial window sum uses and includes the diagonal element.\n\n\nSpatial window sum, on the other hand, summed up the GDPPC of all neighbours and include the target location itself.\n\n\n\n\n\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "geodemog &lt;- read_rds(\"data/geodemog.RDS\")\nglimpse(geodemog)\n\nRows: 323\nColumns: 12\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;fct&gt; MSSZ01, OTSZ01, SRSZ03, BMSZ08, BMSZ03, BMSZ07, BMSZ0…\n$ PLN_AREA_N       &lt;fct&gt; MARINA SOUTH, OUTRAM, SINGAPORE RIVER, BUKIT MERAH, B…\n$ PLN_AREA_C       &lt;fct&gt; MS, OT, SR, BM, BM, BM, BM, SR, QT, QT, QT, BM, ME, R…\n$ REGION_N         &lt;fct&gt; CENTRAL REGION, CENTRAL REGION, CENTRAL REGION, CENTR…\n$ REGION_C         &lt;fct&gt; CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, C…\n$ YOUNG            &lt;dbl&gt; NA, 1100, 0, 2620, 2840, 2910, 2850, 0, 1120, 30, NA,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; NA, 3420, 50, 7500, 6260, 7560, 8340, 50, 2750, 210, …\n$ AGED             &lt;dbl&gt; NA, 2110, 20, 3260, 1630, 3310, 3590, 10, 560, 50, NA…\n$ TOTAL            &lt;dbl&gt; NA, 6630, 70, 13380, 10730, 13780, 14780, 60, 4430, 2…\n$ DEPENDENCY       &lt;dbl&gt; NA, 0.9385965, 0.4000000, 0.7840000, 0.7140575, 0.822…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n\n\n\nA kind of data that is very similar to an ordinary data. The only difference is that each observation is associated with some form of geography such as numbers of aged population by planning subzone.\n\n\n\n\nThe dependency ratio values by planning subzone are normally distributed.\n\n\n\n\n\n\n\n\n\n\nIn statistics, we are interested to study if the observed distribution resemble normal distribution. To provide an answer to the question, we have to go beyond EDA and apply Confirmatory Data Analysis or also know as statistical test to confirm the observated pattern is indeed resemble normal distribution.\n\n\n\n\n\nAre the planning subzones with high proportion of dependency ratio randomly distributed over space?\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\nWhen come of geographically referenced attributes, beside the distribution of the value, we are also interested to find out if the observed geographical patterns are randomly distributed. Similarly to statistical, geovisualisation is not enough because the visual impression will change when different data classification methods is used.\n\n\n\n\n\nAre the planning subzones with high proportion of dependency ratio randomly distributed over space?\n\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n[v3-&gt;v4] `tm_borders()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Blues\" is named\n\"brewer.blues\"\nMultiple palettes called \"blues\" found: \"brewer.blues\", \"matplotlib.blues\". The first one, \"brewer.blues\", is returned.\n\n\n\n\n\n\n\n\n\n\nAs a result, a formal statistical test of spatial randomness is needed."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-are-spatial-weights-wij",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-are-spatial-weights-wij",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "A way to define spatial neighbourhood.\n\n\n\nBefore we can perform statistics test of spatial randomness, we need to understand how spatial relationship among geographical areas can be defined mathematically.\n\n\n\n\n\n\nThere are at least two popular methods can be used to define spatial weights of geographical areas. They are contiguity and distance. Both of them are encoded in binary.\nAccording to contiguity principle, two geographical areas are consider as neighbour if they share a common boundary. Hence, if two geographical areas are neighbour, they will be indicated as 1. On the other hand, if the two geographical areas are not neighbour, they will be indicated as 0.\nAccording to distance principle, two geographical areas are consider as neighbour if their centroid as within a spefic distance. Hence, if two geographical areas are neighbour, they will be indicated as 1. On the other hand, if the two geographical areas are not neighbour, they will be indicated as 0.\n\n\n\n\n\n\nContiguity (common boundary)\nWhat is a “shared” boundary?\n\n\n\nThis figure shows three different ways to define contiguity neighbours. They are Rooks, Bishop and Queens methods. Rooks and Queens are the two commonly used methods.\nThe main different between Queens and Rooks is that Rooks only consider geographical areas that shared common boundaries but Queens method includes geographical areas touching at the tips of the target geographical area.\n\n\n\n\n\nThere are also second-order, third-order, forth-order, etc contiguity\n\n\nThe figure show contiguity neighbours beyond the first order. As you can see that the second-order contiguity neighbours are also defined according to Rooks and Queens case.\n\n\n\n\n\nQuiz: With reference to the figure below, list down the neighbour(s) of area 1202 using Rook case.\n\n\nThis figure shows a geographical area with nine locations. Let us focus on location 1202. According to Queens case, it has seven neighbours. They are 1101, 1201, 1301, 1102, 1302, 1000, and 0901. If Rooks case is consider, 1101 and 1301 will not be considered as neighbours because they do not share common boundaries with 1202.\n\n\n\n\n\nQuiz: With reference to the figure below, create a weights matrix for d = 650.\n\n\nIn this figure, a cut-off distance of 650m is used. In this case, locations B, C and D are considered as neighbours of location A because all of them are with the 650m distance from A. On the other hand, location E is not a neighbour of location A because their distance is 757m apart.\n\n\n\n\n\n\n\nIn this figure, spatial weights are calculated as the inversed function of the distance. In other word, two locations that are closer will be given higher weight than two locations that are further away. For example, the distance of AB is 353 and the diatnce of AE is 757 and the spatial weights are 0.00283 and 0.00132 respectively.\n\n\n\n\n\nIn practice, row-standardised weights instead of spatial weights will be used.\n\n\nRow-standardised weights increase the influence of links from observations with few neighbours, which binary weights vary the influence of observations. + Those with many neighbours are up-weighted compared to those with few. + In this case, raw 1 have four neighbours, hence each neighbour will have a value of 1/4. On the other hand, row number 2, has only two neighbour, hence each neighbour will be given a value of 1/2."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#applications-of-spatial-weights",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#applications-of-spatial-weights",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "Formally, for observation i, the spatial lag of yi, referred to as [Wy]i (the variable Wy observed for location i) is:\n\nwhere the weights wij consist of the elements of the i-th row of the matrix W, matched up with the corresponding elements of the vector y.\n\nWith a neighbor structure defined by the non-zero elements of the spatial weights matrix W, a spatially lagged variable is a weighted sum or a weighted average of the neighboring values for that variable. In most commonly used notation, the spatial lag of y is then expressed as Wy.\n\n\n\n\nSpatial lag with row-standardized weights.\n\n\nThis figure shows the spatially lagged of GPDPC summed up the GDPPC of all its neighbours except the target location itself.\n\n\n\n\n\nThe spatial window sum uses and includes the diagonal element.\n\n\nSpatial window sum, on the other hand, summed up the GDPPC of all neighbours and include the target location itself."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#references",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#references",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "Chapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html",
    "href": "lesson/Lesson02/Lesson02-SPPA.html",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Introducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nQuadrat analysis\nKernel density estimation\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#content",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#content",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\n\n\nIntroducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nQuadrat analysis\nKernel density estimation\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#what-is-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#what-is-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "What is Spatial Point Patterns",
    "text": "What is Spatial Point Patterns\n\nPoints as Events\nMapped pattern\n\nNot a sample\nSelection bias\n\nEvents are mapped, but non-events are not"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#real-world-question",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#real-world-question",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Real World Question",
    "text": "Real World Question\n\nLocation only\n\nare points randomly located or patterned\n\nLocation and value\n\nmarked point pattern\nis combination of location and value random or patterned\n\nWhat is the underlying process?\n\n\nIt is important note that SPPA is exploratory and confirmatory in nature. They are specially developed for describing the spatial point pattern and for confirming the observed patterns statistically. However, they are explanatory nor for prediction."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#points-on-a-plane",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#points-on-a-plane",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Points on a Plane",
    "text": "Points on a Plane\n\nClassic point pattern analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nstraight line only"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis",
    "text": "Spatial Point Patterns Analysis\n\nPoint pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space.\nThe simplest formulation is a set X = {x ∈ D} where D, which can be called the study region, is a subset of Rn, a n-dimensional Euclidean space.\nA fundamental problem of PPA is inferring whether a given arrangement is merely random or the result of some process."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#homogeneous-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#homogeneous-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Homogeneous Spatial Point Patterns",
    "text": "Homogeneous Spatial Point Patterns\n\n\nA homogeneous spatial point pattern assumes that the points are distributed uniformly across the study area. The intensity (expected number of points per unit area) is constant throughout the region.\n\n\nCharacteristics:\n\nThe probability of observing a point is the same across the entire space.\nThe process generating the points does not depend on location.\nThe points are randomly and independently distributed across the space, leading to a uniform density.\n\nModel:\n\nTypically modeled by a Homogeneous Poisson Process, where the intensity λ (the number of points per unit area) is constant."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#heterogeneous-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#heterogeneous-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Heterogeneous Spatial Point Patterns",
    "text": "Heterogeneous Spatial Point Patterns\n\n\nA heterogeneous spatial point pattern assumes that the intensity of points varies across the study area. The intensity function is not constant and may depend on spatial covariates, leading to non-uniform distribution.\n\n\nCharacteristics:\n\nThe probability of observing a point varies across the space, often depending on underlying factors like geography, environmental conditions, or other spatial variables.\nThe density of points can be higher in some regions and lower in others, leading to clusters or dispersed patterns.\n\nModel: - Modeled by an Inhomogeneous Poisson Process (IPP), where the intensity function λ(x,y) varies with location. The intensity can be a function of spatial covariates or other influences."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis-techniques",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis-techniques",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis Techniques",
    "text": "Spatial Point Patterns Analysis Techniques\n\nFirst-order vs Second-order Analysis of spatial point patterns.\n\n\nReference: 11.4 First and second order effects of Intro to GIS and Spatial Analysis\n\nThe first-order properties describe the way in which the expected value (mean or average) of the spatial point pattern varies across space (i.e., the intensity of the spatial point pattern). Such properties are usually measured with the so-called quadrat analysis, nearest neighbour index and kernel estimation. Second-order properties describe the covariance (or correlation) between values of the spatial point pattern at different regions in space and are usually measured with the G function, K function and L function. Applied to point event data, both properties could be used to explore the spatial variation in the risk of being victimized by a crime, spatial and space-time clustering of criminal activities, and the raised incidence of criminal activities around point sources, such as robberies around ATM machines, subway entrances and exits, etc."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#kernel-density-estimation-silverman-1986",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#kernel-density-estimation-silverman-1986",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Kernel density estimation (Silverman 1986)",
    "text": "Kernel density estimation (Silverman 1986)\n\nA method to compute the intensity of a point distribution.\n\n\n\nThe general formula:\n\n\nGraphically"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#distance-based-nearest-neighbour-index",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#distance-based-nearest-neighbour-index",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Distance-based: Nearest Neighbour Index",
    "text": "Distance-based: Nearest Neighbour Index\nWhat is Nearest Neighbour?\nDirect distance from a point to its nearest neighbour."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#nearest-neighbour-index",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#nearest-neighbour-index",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Nearest Neighbour Index",
    "text": "Nearest Neighbour Index\nThe Nearest Neighbour Index is expressed as the ratio of the Observed Mean Distance to the Expected Mean Distance."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#g-function",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#g-function",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "G function",
    "text": "G function\n\n\nThe formula"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#f-function",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#f-function",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "F function",
    "text": "F function\n\nSelect a sample of point locations anywhere in the study region at random\n\nDetermine minimum distance from each point to any event in the study area.\n\nThree steps:\n\nRandomly select m points (p1, p2, ….., pn),\nCalculate dmin(pi,s) as the minimum distance from location pi to any event in the point patterns, and\nCalculate F(d)."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#ripleys-k-function-ripley-1981",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#ripleys-k-function-ripley-1981",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Ripley’s K function (Ripley, 1981)",
    "text": "Ripley’s K function (Ripley, 1981)\n\nLimitation of nearest neighbor distance method is that it uses only nearest distance\nConsiders only the shortest scales of variation.\nK function uses more points.\n\nProvides an estimate of spatial dependence over a wider range of scales.\nBased on all the distances between events in the study area.\nAssumes isotropy over the region."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#the-l-function-besag-1977",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#the-l-function-besag-1977",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "The L function (Besag 1977)",
    "text": "The L function (Besag 1977)\n\n\nIn practice, K function will be normalised to obtained a benchmark of zero.\nThe formula:"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#references",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#references",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4\nGIS&T Body of Knowledge AM-07-Point Pattern Analysis\nGIS&T Body of Knowledge AM-08-Kernels and Density Estimation\nAnalyzing Patterns in Business Point Data, Directions Magazine March 17, 2005.\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "",
    "text": "Demystifying Geospatial Analytics\nMotivation of Geospatial Analytics\nThe Role of Geospatial Analytics\nGeospatial Analytics and Social Consciousness\n\n\n\n\nA Geographical Information System (GIS) is a toolkit for creating, managing, analysing, visualising, and sharing data of any kind according to where it’s located.\n\n\n\n\n\n\n\nGeospatial analytics is more than a GIS.\n\n\n\n\n\nGeospatial analytics is more than data visualisation\n\n\n\n\n\nSource: Singapore’s first disease map delivers real-time information on infectious diseases\n\n\n\n\n\n\n\n\n\nGeospatial analytics is more than just mapping.\n\n\n\n\n\n\nAbout 80% of all data maintained by organisations around the world has a location component.\n\n\n\n\n\n\n\n(Source: BusinessWeek Research Services, 2006)\n\n\n\n\n\nGeospatial information in Smart Nation.\n\n\n\n\n\n\n\nSee more at this link\n\n\n\n\n\nThe explosion in the availability of open geospatial data from both the public and private sectors at national and international levels.\n\n\n\n\n\n\n\n\n\n\nThe national geospatial master plan.\n\n\n\n\n\n\n\nSource: Singapore Geospatial Master Plan\n\n\n\n\n\nUncovering insights not found in statistical graphs and tables.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo reveal the untapped property of spatial contiguity in geographic knowledge discovery in databases.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo uncover the complexity of the real world relationship.\n\n\n\n\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny\n\n\n\n\n\nTo model spatial interactions and flows.\n\n\n\n\n\n\n\nSource: IS415 Bus Rider Flow Project\n\n\n\n\nThe true power of geospatial analytics is to provide decision makers and planners with data-driven and process information for better problem solving and more efficient use of resources."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nA Geographical Information System (GIS) is a toolkit for creating, managing, analysing, visualising, and sharing data of any kind according to where it’s located.\n\n\n\nGeospatial analytics is more than a GIS."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nGeospatial analytics is more than data visualisation\n\n\n\n\n\nSource: Singapore’s first disease map delivers real-time information on infectious diseases"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nGeospatial analytics is more than just mapping."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nAbout 80% of all data maintained by organisations around the world has a location component.\n\n\n\n(Source: BusinessWeek Research Services, 2006)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nGeospatial information in Smart Nation.\n\n\n\nSee more at this link"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nThe explosion in the availability of open geospatial data from both the public and private sectors at national and international levels."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nThe national geospatial master plan.\n\n\n\nSource: Singapore Geospatial Master Plan"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nUncovering insights not found in statistical graphs and tables.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo reveal the untapped property of spatial contiguity in geographic knowledge discovery in databases.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo uncover the complexity of the real world relationship.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo model spatial interactions and flows.\n\n\n\nSource: IS415 Bus Rider Flow Project"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Geospatial Analytics and Social Consciousness",
    "text": "Geospatial Analytics and Social Consciousness\nThe true power of geospatial analytics is to provide decision makers and planners with data-driven and process information for better problem solving and more efficient use of resources."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network Constrained Kernel Density Estimation (NCKDE)\n\nBasic concepts of network constrained spatial point patterns\nNetwork Constrained KDE methods\nThe Three versions of Network Constrained KDE\n\nTemporal Network Kernel Density Estimation (TNKED)\n\nTemporal dimension\nSpatial dimension\nSpatio-temporal point patterns\n\n\n\nWelcome to Lesson 3: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#content",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#content",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network Constrained Kernel Density Estimation (NCKDE)\n\nBasic concepts of network constrained spatial point patterns\nNetwork Constrained KDE methods\nThe Three versions of Network Constrained KDE\n\nTemporal Network Kernel Density Estimation (TNKED)\n\nTemporal dimension\nSpatial dimension\nSpatio-temporal point patterns\n\n\n\nWelcome to Lesson 3: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-point-processes",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-point-processes",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Point Processes",
    "text": "Network Constrained Point Processes\nMany real world point event are not randomly distributed. Their distribution, on the other hand, are constrained by network such as roads, rivers, and fault lines just to name a few of them.\n\n\nRoad traffic accidents within Bangkok City. \n\nLocation of banks at Central, Hong Kong"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Kernel Density Estimation (NKDE)",
    "text": "Network Constrained Kernel Density Estimation (NKDE)\nNetwork Kernel Density Estimation, in short, NKDE is a geospatial analytics methods specially designed to computer intensity of spatial point event either located along or occurred along linear networks.\n\n\nMathematically, NKDE can be defined as: \nwhere K must be a probability density function and verifies the two following conditions:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde-1",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde-1",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Kernel Density Estimation (NKDE)",
    "text": "Network Constrained Kernel Density Estimation (NKDE)\n\nPlanar KDE versus NKDE\n\n\nFigure on the right shows the basic differences between the Planar KDE and Network KDE for the same spatial point event data. To estimate th density values at a focal point X, the planar KDE treats the whole 2-D space as the context and finds four point events (black circle) within a search bandwidth (i.e. h), whereas the Network KDE only finds two point events within the same bandwidth in the network space based on network distance.\n\n\n\n\n\n\n\nNetwork Constrained Kernel Density Estimation (NKDE) method\nTo calculate a Network Kernel Density Estimate (NKDE), it is possible to:\n\nuse lixels instead of pixels. A lixel is a linear equivalent of a pixel on a network. The lines of the network are split into lixels according to a chosen resolution. The centres of the lixels are sampling points for which the density will be estimated.\ncalculate network distances between objects instead of Euclidean distances.\nadjust the kernel function to deal with the anisotropic space\n\n\n\n\nNKDE Method\n\nTo perform a NKDE, the events must be snapped on the network. The snapped events are shown here in green.\n\n\n\n\n\n\n\n\n\n\n\nNKDE Method\n\n\nThe mass of each event can be seen as a third dimension and is evaluated by a selected kernel function (K) within a specified bandwidth. The kernel function must satisfy the following conditions:\n\nThe total mass of an event is 1, and is spread according to the function K within the bandwidth.\n\nIn the figure below, we can see that the “influence” of each point is limited within the bandwidth and decreases when we move away from the event.\n\n\n\n\n\n\nNKDE Method\n\n\nWith this method, one can evaluate the density of the studied phenomenon at each location on the network. In the figure below, 3 sampling points (s1, s2 and s3) are added in blue.\n\n\nWhere S1, S2, and S3 are calculated by using the formulas below:\n\nand the general formular will be defined as:\n\n\n\n\n\nwith 𝒅𝑠𝑖 the density estimated at the sample point 𝑠𝒊, 𝒃𝑤 the bandwidth and 𝑒𝑗 an event.\n\n\n\n\n\nNKDE Method\n\n\nThe proposed kernel functions in the spNetwork package are:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\n\nSimple method\n\n\nThe first method was proposed by Xie and Yan (2008). Considering the planar KDE, they defined the NKDE with the following formula:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis method remains useful for two reasons:\n\nFor quick data visualization. With big datasets, it might be useful to use this simple method to do a primary investigation.\nIn a purely geographical view, this method is intuitive. In the case of crime analysis for example, one could argue that the strength of an event should not be affected by intersections on the network. In that case, the kernel function is seen as a distance decaying function."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-1",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-1",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\n\nDiscontinuous method\n\n\nThis method is introduced by Okabe and Sugihara (2012). The discontinuous NKDE is easily presented by a figure:\n Note that the density of the kernel function is equally divided at intersections.\n\n\n\n\n\n\n\nWarning\n\n\n\nAs one can see, the values of the NKDE are split at intersections to avoid the multiplication of the mass observed in the simple version. However, this creates a discontinuous NKDE, which is counter-intuitive. It leads to sharp differences between density values in the network, and could be problematic in networks with many intersections."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-2",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-2",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\n\nContinue method\n\n\nThe method merges the best of the two worlds: it adjusts the values of the NKDE at intersections to ensure that it integrates to 1 on its domain, and applies a backward correction to force the density values to be continuous.\nThis process is accomplished by a recursive function. This function is more time consuming, so it might be necessary to stop it when the recursion is too deep. Considering that the kernel density is divided at each intersection, stopping the function at deep level 16 should give results almost identical to the true values.\n\n\nNote that there are three different equations to calculate the kernel density depending on the situation (here, q1, q2, q3).\n\n\n\n\n\n\nNote\n\n\n\nAs one can see, the values of the NKDE are continuous, and the density values close to the events have been adjusted. This leads to smoother results than the discontinuous method."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#temporal-network-kernel-density-estimate",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#temporal-network-kernel-density-estimate",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Temporal Network Kernel Density Estimate",
    "text": "Temporal Network Kernel Density Estimate\n\n\nEvents recorded on a network often have a temporal dimension. In that context, one could estimate the density of events in both time and network spaces.\nThe spatio-temporal kernel is calculated as the product of the network kernel density and the time kernel density.\n\nFor a sample point at location l and time t, the Temporal Network Kernel Density Estimate (TNKDE) is calculated as follows:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#references",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#references",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nNetwork Kernel Density Estimate\nTemporal Network Kernel Density Estimate"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html",
    "href": "lesson/Lesson05/Lesson05-GLSA.html",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "",
    "text": "What is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\n\n\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)\n\nSpacetime and spacetime cubes\nMann-Kendall Test\nEHSA map"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#content",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#content",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "",
    "text": "What is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\n\n\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)\n\nSpacetime and spacetime cubes\nMann-Kendall Test\nEHSA map"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "What is Spatial Autocorrelation",
    "text": "What is Spatial Autocorrelation\n\nToble’s First Law of Geography\nSpatial Dependency\nSpatial Autocorrelation\n\nPositive autocorrelation\nNegative autocorrelation\n\n\n\n\nTobler’s First law of Geography\n\nEverything is related to everything else, but near things are more related than distant things.\n\n\n\nThe foundation of the fundamental concepts of:\n\nspatial dependence, and\nspatial autocorrelation\n\n\n\n\nReference: A Computer Movie Simulating Urban Growth in the Detroit Region\n\n\n\n\n\n\nSpatial Dependency\n\n\n\nSpatial dependence is the spatial relationship of variable values (for themes defined over space, such as rainfall) or locations (for themes defined as objects, such as cities).\nSpatial dependence is measured as the existence of statistical dependence in a collection of random variables, each of which is associated with a different geographical location.\n\n\n\n\n\n\n\n\nSpatial Autocorrelation\n\n\n\n\nSpatial autocorrelation is the term used to describe the presence of systematic spatial variation in a variable.\nThe variable can assume values either:\n\nat any point on a continuous surface (such as land use type or annual precipitation levels in a region);\nat a set of fixed sites located within a region (such as prices at a set of retail outlets); or\nacross a set of areas that subdivide a region (such as the count or proportion of households with two or more cars in a set of Census tracts that divide an urban region).\n\n\n\n\n\n\n\n\n\n\nPositive Spatial Autocorrelation\n\n\n\nClustering\n\nlike values tend to be in similar locations.\n\nNeighbours are similar\n\nmore alike than they would be under spatial randomness.\n\nCompatible with diffusion\n\nbut not necessary caused by diffusion.\n\n\n\n\n\n\n\n\n\nNegative Spatial Autocorrelation\n\n\n\nCheckerboard patterns\n\n“opposite” of clustering\n\nNeighbours are dissimilar\n\nmore dissimilar than they would be under spatial randomness\n\nCompatible to competition\n\nbut not necessary competition"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Measures of Global Spatial Autocorrelation",
    "text": "Measures of Global Spatial Autocorrelation\n\nMoran’s I\nGeary’s c\n\n\n\nMeasures of Global Spatial Autocorrelation: Moran’s I\n\n\nDescribe how features differ from the values in the study area as a whole.\n\n\n\n\n\n\nMoran I (Z value) is:\n\npositive (I&gt;0): Clustered, observations tend to be similar;\nnegative(I&lt;0): Dispersed, observations tend to be dissimilar;\napproximately zero: observations are arranged randomly over space.\n\n\n\n\n\n\n\nMeasures of Global Spatial Autocorrelation: Geary’s c\n\n\nDescribing how features differ from their immediate neighbours.\n\n\nGeary c (Z value) is:\n\nLarge c value (&gt;1) : Dispersed, observations tend to be dissimilar;\nSmall c value (&lt;1) : Clustered, observations tend to be similar;\nc = 1: observations are arranged randomly over space.\n\n\n\n\n\n\n\nRelationship of Moran’s I and Geary’s C\n\nC approaches 0 and I approaches 1 when similar values are clustered.\nC approaches 3 and I approaches -1 when dissimilar values tend to cluster.\nHigh values of C measures correspond to low values of I.\nSo the two measures are inversely related.\n\n\n\n\nz-score and p-value explained\n\n\n\nStatistically, we select the confident interval such as 95% =&gt; alpha value = 0.05.\nReject the Null hypothesis (H0) if p-value is smaller than alpha value.\nFailed to reject the Null Hypothesis (H0) if p-value is greater than alpha value.\n\n\n\n\nReference: Confidence Interval or P-Value?\n\n\n\n\n\n\nSpatial Randomness\nThe Null Hypothesis:\n\nObserved spatial pattern of values is equally likely as any other spatial pattern.\nValues at one location do not depend on values at other (neighbouring) locations.\nUnder spatial randomness, the location of values may be altered without affecting the information content of the data.\n\n\n\n\nWhat if my data violate the assumptions?\n\nIf you doubt that the assumptions of Moran’s I are true (normality and randomization), we can use a Monte Carlo simulation.\n\nSimulate Moran’s I n times under the assumption of no spatial pattern,\nAssigning all regions the mean value\nCalculate Moran’s I,\n\nCompare actual value of Moran’s I to randomly simulated distribution to obtain p-value (pseudo significance)."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Measures of Global High/Low Clustering: Getis-Ord Global G",
    "text": "Measures of Global High/Low Clustering: Getis-Ord Global G\n\n\n\nGetis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas.\nThe variable must contain only positive values to be used.\n\n\n\nSource: Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\n\n\n\n\n\nInterpretation of Getis-Ord Global G\n\nThe p-value is not statistically significant.\n\nYou cannot reject the null hypothesis. It is possible that the spatial distribution of feature attribute values is the result of random spatial processes. Said another way, the observed spatial pattern of values could be one of many possible versions of complete spatial randomness.\n\nThe p-value is statistically significant, and the z-score is positive.\n\nYou can reject the null hypothesis. The spatial distribution of high values in the dataset is more spatially clustered than would be expected if underlying spatial processes were truly random.\n\nThe p-value is statistically significant, and the z-score is negative.\n\nYou can reject the null hypothesis. The spatial distribution of low values in the dataset is more spatially clustered than would be expected if underlying spatial processes were truly random."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Spatial Autocorrelation Statistics",
    "text": "Local Spatial Autocorrelation Statistics\n\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas).\nCan be indecies decomposited from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*.\nThese spatial statistics are well suited for:\n\ndetecting clusters or outliers;\nidentifying hot spot or cold spot areas;\nassessing the assumptions of stationarity; and\nidentifying distances beyond which no discernible association obtains."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Indicator of Spatial Association (LISA)",
    "text": "Local Indicator of Spatial Association (LISA)\n\nA subset of localised geospatial statistics methods.\nAny spatial statistics that satisfies the following two requirements (Anselin, L. 1995):\n\nthe LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;\nthe sum of LISAs for all observations is proportional to a global indicator of spatial association.\n\n\n\n\nDetecting Spatial Clusters and Outliers\n\nGiven a set of geospatial features (i.e. points or polygons) and an analysis field, the spatial statistics identify spatial clusters of features with high or low values. The tool also identifies spatial outliers.\nlocal Moran’s I is the most popular spatial statistical method used, other methods include local Geary’s c.\nIn general, the analysis will calculate a local statistic value, a z-score, a pseudo p-value, and a code representing the cluster type for each statistically significant feature. The z-scores and pseudo p-values represent the statistical significance of the computed index values."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-morans-i",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-morans-i",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I\nGiven a geographically referenced attribute field, X the formula of local Moran’s I is:\n\n\n\nLocal Moran’s I and Moran’s I\nThe summation of local Moran is\n\n\nMoran’s I\n\n\n\n\n\nTest statistics of Local Moran\n\n\n\n\n\n\n\n\nInterpretation of Local Moran\n\nAn outlier: significant and negative if location i is associated with relatively low values in surrounding locations.\nA cluster: significant and positive if location i is associated with relatively high values of the surrounding locations.\nIn either instance, the p-value for the feature must be small enough for the cluster or outlier to be considered statistically significant.\nThe commonly used alpha-values are 0.1, 0.05, 0.01, 0.001 corresponding the 90%, 95, 99% and 99.9% confidence intervals respectively.\n\n\n\n\nInterpretation of Local Moran and Scatterplot\n\n\n\n\nOther forms of LISA\n\nLocal Geary"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Detecting hot and cold spot areas",
    "text": "Detecting hot and cold spot areas\n\n\n\nGiven a set of geospatial features (i.e. points or polygons) and an analysis field, the spatial statistics tell you where features with either high (i.e. hot spots) or low values (cold spots) cluster spatially.\nThe spatial statistic used is called Getis-Ord Gi* statistic (pronounced G-i-star).\nGetis and Ord (1992) define the local G and G∗ statistics for region i (i=1,···,n) as:\n\n\n\n\n\n\nGetis-Ord Gi*\n\n\nFor variable x to be nonpositive and the weight matrix W(d) to be nonbinary the G and G* statistics are defined as:\n\n\n\n\n\n\nInterpretation of Getis-Ord Gi and Gi*\n\n\n\nA hot spot area: significant and positive if location i is associated with relatively high values of the surrounding locations.\nA cold spot area: significant and negative if location i is associated with relatively low values in surrounding locations."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#fixed-weighting-scheme",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#fixed-weighting-scheme",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Fixed weighting scheme",
    "text": "Fixed weighting scheme\n\n\n\nThings to consider if fixed distance is used: - All features should have at least one neighbour.\n\nNo feature should have all other features as neighbours.\nEspecially if the values for the input field are skewed, you want features to have about eight neighbors each.\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters)."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Adaptive weighting schemes",
    "text": "Adaptive weighting schemes\n\n\n\nAdaptive schemes adjust itself according to the density of data\n\nShorter bandwidths where data are dense and longer where sparse.\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n]"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#best-practice-guidelines",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#best-practice-guidelines",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Best practice guidelines",
    "text": "Best practice guidelines\n\nResults are only reliable if the input feature class contains at least 30 features.\nThe input field mst be in continuous data type such as a count, rate, or other numeric measurement, no categorical attribute field is allowed.\n\n\n\nSelect an appropriate spatial weighting method\n\nThe polygon contiguity method is effective when polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity (the idea that if two polygons share a boundary, spatial interaction between them increases).\n\nWhen you select a polygon contiguity conceptualization, you will almost always want to select row standardization for tools that have the Row Standardization parameter.\n\nThe fixed distance method works well for point data. It is often a good option for polygon data when there is a large variation in polygon size (very large polygons at the edge of the study area and very small polygons at the center of the study area, for example), and you want to ensure a consistent scale of analysis.]\n\n\n\n\nSelect an appropriate spatial weighting method\n\nThe inverse distance method is most appropriate with continuous data or to model processes where the closer two features are in space, the more likely they are to interact/influence each other.\n\nBe warned that with this method, every feature is potentially a neighbour of every other feature, and with large datasets, the number of computations involved will be enormous.\n\n\n\n\n\nSelect an appropriate spatial weighting method\n\nThe k-nearest neighbours method is effective when you want to ensure you have a minimum number of neighbors for your analysis.\n\nEspecially when the values associated with your features are skewed (are not normally distributed), it is important that each feature is evaluated within the context of at least eight or so neighbors (this is a rule of thumb only).\nWhen the distribution of your data varies across your study area so that some features are far away from all other features, this method works well.\nNote, however, that the spatial context of your analysis changes depending on variations in the sparsity/density of your features.\nWhen fixing the scale of analysis is less important than fixing the number of neighbors, the k-nearest neighbours method is appropriate.\n\n\n\n\n\nFuther guide on selecting a fixed-distance band value\n\nSelect a distance based on what you know about the geographic extent of the spatial processes promoting clustering for the phenomena you are studying.\nUse a distance band that is large enough to ensure all features will have at least one neighbor, or results will not be valid.\nTry not to get stuck on the idea that there is only one correct distance band. Reality is never that simple. Most likely, there are multiple/interacting spatial processes promoting observed clustering.\nSelect an appropriate distance band or threshold distance.\n\nAll features should have at least one neighbour.\nNo feature should have all other features as a neighbour.\nEspecially if the values for the input field are skewed, each feature should have about eight neighbours."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Emerging Hot Spot Analysis (EHSA)",
    "text": "Emerging Hot Spot Analysis (EHSA)\n\nA technique that falls under exploratory spatial data analysis (ESDA).\nIt combines the traditional ESDA technique of hot spot analysis using the Getis-Ord Gi* statistic with the traditional time-series Mann-Kendall test for monotonic trends.\nThe goal of EHSA is to evaluate how hot and cold spots are changing over time. It helps us answer the questions: are they becoming increasingly hotter, are they cooling down, or are they staying the same?\n\n\n\nMann-Kendall test for trend overview\n\n\n\nThe Mann-Kendall statistical test for trend is used to assess whether a set of data values is increasing over time or decreasing over time, and whether the trend in either direction is statistically significant.\nIt is a non-parametric test, which means it works for all distributions (i.e. your data doesn’t have to meet the assumption of normality), but your data should have no serial correlation. If your data does follow a normal distribution, you can run simple linear regression instead.\nRefer to this article for a full tutorial of how to perform Mann-Kendall test manually.\n\n\n\n\n\n\n\n\n\nThe Hypothesis\n\n\n\nNull hypothesis: There is no monotonic trend in the series.\nAlternate hypothesis: A trend exists. This trend can be positive, negative, or non-null.\n\n\nThe Mann-Kendall test does NOT assess the magnitude of change.\nThe test can be used to find trends for as few as four samples. However, with only a few data points, the test has a high probability of not finding a trend when one would be present if more points were provided. The more data points you have the more likely the test is going to find a true trend (as opposed to one found by chance). The minimum number of recommended measurements is therefore at least 8 to 10.\n\n\n\n\n\nData requierements\nBefore running the test, you should ensure that:\n\nYour data isn’t collected seasonally (e.g. only during the summer and winter months), because the test won’t work if alternating upward and downward trends exist in the data. Another test—the Seasonal Kendall Test—is generally used for seasonally collected data.\nYour data does not have any covariates—other factors that could influence your data other than the ones you’re plotting.\nYou have only one data point per time period. If you have multiple points, use the median value.\n\n\n\n\nEHSA Patterns: Hot Spot Trends\n\n\n\n\n\n\n\n\nPattern Name\nDefination\n\n\n\n\nNo pattern detected\nDoes not fall into any of the hot or cold spot patterns defined below.\n\n\nNew Hot Spot\nA location that is a statistically significant hot spot for the final time step and has never been a statistically significant hot spot before.\n\n\nConsecutive Hot Spot\nA location with a single uninterrupted run of at least two statistically significant hot spot bins in the final time-step intervals. The location has never been a statistically significant hot spot prior to the final hot spot run and less than 90 percent of all bins are statistically significant hot spots.\n\n\nIntensifying Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of high counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering over time.\n\n\nDiminishing Hot Spot\nA location that has been a statistically significant hot spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Hot Spot\nA statistically significant hot spot for the final time-step interval with a history of also being an on-again and off-again hot spot. Less than 90 percent of the time-step intervals have been statistically significant hot spots and none of the time-step intervals have been statistically significant cold spots.\n\n\nOscillating Hot Spot\nA statistically significant hot spot for the final time-step interval that has a history of also being a statistically significant cold spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\nHistorical Hot Spot\nThe most recent time period is not hot, but at least 90 percent of the time-step intervals have been statistically significant hot spots.\n\n\n\n\n\n\n\nEHSA Patterns: Cold Spot Trends\n\n\n\n\n\n\n\n\nPattern Name\nDefinition\n\n\n\n\nNew Cold Spot\nA location that is a statistically significant cold spot for the final time step and has never been a statistically significant cold spot before.\n\n\nConsecutive Cold Spot\nA location with a single uninterrupted run of at least two statistically significant cold spot bins in the final time-step intervals. The location has never been a statistically significant cold spot prior to the final cold spot run and less than 90 percent of all bins are statistically significant cold spots.\n\n\nIntensifying Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is increasing overall and that increase is statistically significant.\n\n\nPersistent Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals with no discernible trend in the intensity of clustering of counts over time.\n\n\nDiminishing Cold Spot\nA location that has been a statistically significant cold spot for 90 percent of the time-step intervals, including the final time step. In addition, the intensity of clustering of low counts in each time step is decreasing overall and that decrease is statistically significant.\n\n\nSporadic Cold Spot\nA statistically significant cold spot for the final time-step interval with a history of also being an on-again and off-again cold spot. Less than 90 percent of the time-step intervals have been statistically significant cold spots and none of the time-step intervals have been statistically significant hot spots.\n\n\nOscillating Cold Spot\nA statistically significant cold spot for the final time-step interval that has a history of also being a statistically significant hot spot during a prior time step. Less than 90 percent of the time-step intervals have been statistically significant cold spots.\n\n\nHistorical Cold Spot\nThe most recent time period is not cold, but at least 90 percent of the time-step intervals have been statistically significant cold spots.\n\n\n\n\n\n\n\nA Spacetime Cubes\n\n\n\nA spacetime object is a spacetime cube if every location has a value for every time index. Another way of saying this is that each location contains a regular time-series.\nIn ESRI terminology, the basic unit of a spacetime cube is a bin.\n\nA bin is the unique combination of a location and time index. For each time index, the collection of every location is called a time slice.\nIn every location, the collection of every bin at each time index is referred to as a bin time-series."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#in-colclusion",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#in-colclusion",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "In colclusion",
    "text": "In colclusion\nSpatial statistics methods are not a blackbox. Before performing the analysis, a geospatial analyst should consider the followings:\n\nWhat is the geographical question?\nWhat is the geospatial feature?\nWhat is the analysis field?\nWhich conceptualization of spatial relationships is appropriate?"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#references",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#references",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "References",
    "text": "References\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127. - Moran’s I\nGeary’s c - Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html",
    "href": "lesson/Lesson07/Lesson07-gwr.html",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "",
    "text": "Introducing Regression Modelling\n\nSimple Linear Regression\nMultiple Linear Regression\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#content",
    "href": "lesson/Lesson07/Lesson07-gwr.html#content",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "",
    "text": "Introducing Regression Modelling\n\nSimple Linear Regression\nMultiple Linear Regression\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions",
    "href": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\n\nWhy some condominium units were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions-1",
    "href": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions-1",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\nWhy condominium units located at the central part of Singapore were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#what-is-regression-analysis",
    "href": "lesson/Lesson07/Lesson07-gwr.html#what-is-regression-analysis",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\nA set of statistical processes for explaining the relationships among variables.\nThe focus is on the relationship between a dependent variable (y) and one or more independent variables (x)\n\nDoes X affect Y? If so, how?\nWhat is the change in Y given a one unit change in X?\n\nEstimate outcomes based on the relationships modelled.\n\n\n\nA Simple Linear Regression Model\nThe formula:\n\n\n\n\nThe Least Squares Method\n\nThe sum of the vertical deviations (y axis) of the points from the line is minimal.\n\n\n\n\n\nMultiple Linear Regression\n\n\n\n\nAssessing the goodness of fit\n\n\n\n\nSignificance testing in regression\n\n\n\n\nGoodness of fit test\n\n\n\n\nIndividual parameter testing\n\n\n\n\nAssessing individual parameters\n\n\n\n\nAre there redundant explanatory variables?"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#assumptions-of-linear-regression-models",
    "href": "lesson/Lesson07/Lesson07-gwr.html#assumptions-of-linear-regression-models",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Assumptions of linear regression models",
    "text": "Assumptions of linear regression models\n\nLinearity assumption. The relationship between the dependent variable and independent variables is (approximately) linear.\nNormality assumption. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity).\nThe residuals are uncorrelated with each other.\n\nserial correlation, as with time series\n\n(Optional) The errors (residuals) are normally distributed and have a 0 population mean.]\n\n\n\nThe linearity assumption\n\n\n\n\nThe linearity assumption\nResiduals vs Fitted plot - Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.\n\n\n\n\nDemystifying the linearity assumption myth\n\n\nThe myth: - We should transform the values of the y variable when they are large.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sjPlot)\nlibrary(sjmisc)\n\n\nAttaching package: 'sjmisc'\n\nThe following object is masked from 'package:purrr':\n\n    is_empty\n\nThe following object is masked from 'package:tidyr':\n\n    replace_na\n\nThe following object is masked from 'package:tibble':\n\n    add_case\n\nlibrary(sjlabelled)\n\n\nAttaching package: 'sjlabelled'\n\nThe following object is masked from 'package:forcats':\n\n    as_factor\n\nThe following object is masked from 'package:dplyr':\n\n    as_label\n\nThe following object is masked from 'package:ggplot2':\n\n    as_label\n\nlibrary(olsrr)\n\n\nAttaching package: 'olsrr'\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n\ndata1 &lt;- read_delim(\"data/ex1_data.txt\", delim=\";\")\n\nRows: 100 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (1): group\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata1.lm &lt;- lm(formula=y ~ x, data = data1)\ntab_model(data1.lm)\n\n\n\n\n \ny\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n1.98\n1.66 – 2.30\n&lt;0.001\n\n\nx\n2.35\n2.13 – 2.57\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.819 / 0.817\n\n\n\n\n\n\n\nggplot(data=data1,  \n       aes(x=`x`, y=`y`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe linearity assumption\n\n\nDespite the values of the dependent variable is rather similar to the values of the independent variable, the diagnostic plot shows that the linearity assumption has been violated.\n\n\nols_plot_obs_fit(data1.lm, print_plot = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nData transformation come to rescue\n\n\n\ndata.lm2 &lt;- lm(formula=y ~ exp(x), data = data1)\ntab_model(data.lm2)\n\n\n\n\n \ny\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.51\n0.36 – 0.65\n&lt;0.001\n\n\nx [exp]\n1.01\n0.98 – 1.04\n&lt;0.001\n\n\nObservations\n100\n\n\nR2 / R2 adjusted\n0.977 / 0.977\n\n\n\n\n\n\n\nggplot(data=data1,  \n       aes(x=exp(x), y=`y`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe linearity assumption\n\n\nThe diagnostic plot on the right shows that the linearity assumption has been conformed.\n\n\nols_plot_obs_fit(data.lm2, print_plot = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe normality assumption\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is the test on the residual and not on the dependent variable.\n\n\n\n\n\n\n\nChecking for serial correlation\nThe purpose of this test is to ensure the residuals of a multiple regression are time independent."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#spatial-non-stationary",
    "href": "lesson/Lesson07/Lesson07-gwr.html#spatial-non-stationary",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Spatial Non-stationary",
    "text": "Spatial Non-stationary\n\nWhen applied to spatial data, as can be seen, it assumes a stationary spatial process.\n\nThe same stimulus provokes the same response in all parts of the study region.\nHighly untenable for spatial process.\n\n\n\n\nWhy do relationships vary spatially?\n\nSampling variation\n\nNuisance variation, not real spatial non-stationarity.\n\nRelationships intrinsically different across space\n\nReal spatial non-stationarity.\n\nModel misspecification\n\nCan significant local variations be removed?\n\n\n\n\n\nSome definitions\n\nSpatial non-stationarity: the same stimulus provokes a different response in different parts of the study region.\nGlobal models: statements about processes which are assumed to be stationary and as such are location independent.\nLocal models: spatial decompositions of global models, the results of local models are location dependent – a characteristic we usually anticipate from geographic (spatial) data.\n\n\n\n\nSpatial Autocorrelation assumption\nThe residuals are assumed to be distributed at random over geographical space.\n\n\n\n\nTest of spatial autocorrelation\nTo test if the relationships in the model are non-stationary.\n\nlm.morantest() of spdep package will be used."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#geographically-weighted-regression-gwr",
    "href": "lesson/Lesson07/Lesson07-gwr.html#geographically-weighted-regression-gwr",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Geographically Weighted Regression (GWR)",
    "text": "Geographically Weighted Regression (GWR)\n\nLocal statistical technique to analyze spatial variations in relationships.\nSpatial non-stationarity is assumed and will be tested.\nBased on the “First Law of Geography”: everything is related with everything else, but closer things are more related.\n\n\n\nGeographically Weighted Regression (GWR): The method\n\n\n\n\n\n\n\n\nCalibration of GWR\n\n\n\nLocal weighted least squares\n\nWeights are attached with locations\nBased on the “First Law of Geography”: everything is related with everything else, but closer things are more related than remote ones\n\n\n\n\n\n\n\nCalibration - Weighting functions\n\n\n\n\nCalibration - Weighting functions\n\n\n\n\nCalibration - Weighting schemes\n\nDetermines weights\n\nMost schemes tend to be Gaussian or Gaussian-like reflecting the type of dependency found in most spatial processes.\nIt can be either Fixed or Adaptive.\n\n\n\n\n\n\n\n\n\n\n\n\nCalibration - Determining Bandwidth\n\n\n\n\n\n\n\n\nGWR Report\n\nPackage Model\nResults of Global Regression\nResults of Geographically Weighted Regression\nSDF: A SpatialPointDataFrame\n\n\n\n\ngwr: local R2\n\n\n\n\ngwr: intercept"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#references",
    "href": "lesson/Lesson07/Lesson07-gwr.html#references",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "References",
    "text": "References\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) [“Some Notes on Parametric Significance Tests for Geographically Weighted Regression”](https://onlinelibrary-wiley-com.libproxy.smu.edu.sg/doi/abs/10.1111/0022-4146.00146. Journal of Regional Science, 39(3), 497-524.\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geography of Accessibility\nAccessibility Models\n\nStewart Potential model\nReilly model\nHuff model"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#content",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#content",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geography of Accessibility\nAccessibility Models\n\nStewart Potential model\nReilly model\nHuff model"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#what-is-geography-of-accessibility",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#what-is-geography-of-accessibility",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "What is Geography of Accessibility?",
    "text": "What is Geography of Accessibility?\n\n\n\nAccessibility is the measure of the capacity of a location to be reached from, or to be reached by, different locations. Therefore, the capacity and the arrangement of transport infrastructure are key elements in the determination of accessibility.\n\n\n\n\n\nWhy Model Geography of Accessibility?\n\n\n\nQuestions that can be answered by accessibility models:\n\nWhich part of the geographical areas are deprived from getting access to a social service, facility or job opportunity?\nWhich part of the geographical areas will be affected by a public policy or business decision such as merging JCs, secondary and primary schools."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#measuring-distances",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#measuring-distances",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Measuring Distances",
    "text": "Measuring Distances\n\nDifferent spatial and distance conceptualizations that are commonly employed when measuring and modelling accessibility.\n\n\n\n\nDistance Consideration\nCartesian distance versus Network distance.\n\n\nReference: Philippe Apparicio et. al. (2017) “The approaches to measuring the potential spatial access to urban health services revisited: distance types and aggregation‑error issues. International Journal of Health Geographics, pp. 16:32.\n\n\n\n\nThe distance friction\n\n\n\nModeling spatial interactions implies quantifying the distance friction or impedance.\nThe role of the distance can be interpreted as a disincentive to access desired destinations or opportunities (e.g. jobs, shops).\n\n\n\n\n\n\nThe perception of how far a destination is may not be a linear function of distance. People are more likely to shop at a place close to home than one far away. Distance is viewed as a nonlinear deterrent to movement. This phenomenon can be modeled by using a distance-decay function. The use of a power distance-decay function is borrowed from Newton’s law of gravitation, from which the term gravity model is derived. A distance-decay parameter, symbolized by the Greek letter beta, can be used to exaggerate the distance to destinations. Some activities, such as grocery shopping, have a large exponent, indicating that people will travel only a short distance for such things. Other activities, such as furniture shopping, have a small exponent, because people are willing to travel farther to shop for furniture.\n\n\n\n\nDistance Decay function.\n\n\nInverse distance decay, 𝜶∕𝒅_𝜷\n\n\nExponential distance decay, 𝜶𝒆^(−𝜷𝒅)"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#the-geographical-unit",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#the-geographical-unit",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "The Geographical Unit",
    "text": "The Geographical Unit\n\nThis issue of irregularly shaped polygons created arbitrarily (such as county boundaries or block groups that have been created from a political process).\n\n\n\n\nThe Geographical Unit\n\nUsing regular shaped geometry such as square, hexagon or triangle to define geographical unit.\n\n\n\nIn GIS analysis, regularly shaped grids is used for many reasons such as normalizing geography for mapping or to mitigate the issues of using irregularly shaped polygons created arbitrarily (such as county boundaries or block groups that have been created from a political process). Regularly shaped grids can only be comprised of equilateral triangles, squares, or hexagons, as these three polygon shapes are the only three that can tessellate (repeating the same shape over and over again, edge to edge, to cover an area without gaps or overlaps) to create an evenly spaced grid.\n\n\n\n\nThe Geographical Unit\n\nHexagons reduce sampling bias due to edge effects of the grid shape, this is related to the low perimeter-to-area ratio of the shape of the hexagon.\nA circle has the lowest ratio but cannot tessellate to form a continuous grid. Hexagons are the most circular-shaped polygon that can tessellate to form an evenly spaced grid.]\n\n\n\nThough the square (fishnet) grid is the predominantly used shape type in GIS analysis and thematic mapping, there are ways in which hexagons may be better suited for your analysis based on the nature of your question.\nHexagons reduce sampling bias due to edge effects of the grid shape, this is related to the low perimeter-to-area ratio of the shape of the hexagon. A circle has the lowest ratio but cannot tessellate to form a continuous grid. Hexagons are the most circular-shaped polygon that can tessellate to form an evenly spaced grid.\n\n\n\n\nThe Geographical Unit\n\nAn example of 250m radius hexagons covering Singapore main island.\n\n\n\n\n\nDistance to Nearest Location\n\n\nThe formula:\n\n\nLimitation of the method:\n\nDoes not consider the size/attractiveness of the closest location, thereby implicitly treating all locations as being equally attractive.\nDoes not consider the cumulative effect of multiple accessible locations (e.g. is a zone that is within 1.1 km of two MRT stations inferior to one that is within 1.0 km of a single station?"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#the-potential-model",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#the-potential-model",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "The Potential Model",
    "text": "The Potential Model\n\nThe classic model\n\n\n\n\nThe Modified Potential Formula\n\n\n\n\nReal world application of potential model\n\nAccessibility to urban functions study.\n\n\n\n\n\nReal world application of potential model\n\n\n\nAccessibility to shopping centres.\n\n\n\nAccessibility to health services\n\n\n\n\n\n\nReal world application of potential model\n\nOverall accessibility"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#two-step-floating-catchment-area-method-2sfca",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#two-step-floating-catchment-area-method-2sfca",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Two-step floating catchment area method (2SFCA)",
    "text": "Two-step floating catchment area method (2SFCA)\n\n\n\nA special case of a potential model for measuring spatial accessibility to primary social services and public facilities.\nIt was inspired by the spatial decomposition idea first proposed by Radke and Mu (2000).\n\n\nReference: Luo, W.; Wang, F. (2003b). “Measures of spatial accessibility to health care in a GIS environment: synthesis and a case study in the Chicago region”. Environment and Planning B: Planning and Design. 30 (6): 865–884.\n\n\nAn earlier version of 2SFCA\n\n\n\n\n\nTwo-step floating catchment area method (2SFCA)\n\n\nStep 1: For each physician location j, search all population locations (k) that are within a threshold travel time (d0) from location j (that is, catchment area j ), and compute the physician-to-population ratio, Rj, within the catchment area:\n\n\nAn earlier version of 2SFCA\n\n\n\n\n\n\nTwo-step floating catchment area method (2SFCA)\n\n\nStep 2: For each population location i, search all physician locations (j) that are within the threshold travel time (d0) from location i (that is, catchment area i), and sum up the physician-to-population ratios, Rj, at these locations:\n\n\nAn earlier version of 2SFCA.\n\n\n\n\n\n\nEnhanced Two-step Floating Catchment Area (E2SFCA)\n\n\n\nStep 1: The catchment of physician location j is defined as the area within 30-min driving zone(Lee, 1991). Within each catchment, compute three travel time zones with minute breaks of 0–10,10–20 and 20–30min (zones1–3,respectively). Search all population locations(k) that are within a threshold travel time zone (Dr) from location j (this is catchment area j), and compute the weighted physician-to-population ratio, Rj, within the catchment area as follows:\n\n\n\n\nStep 2: For each population location i, search all physician locations (j) that are within the 30min travel time zone from location i (that is,catchment area i), and sum up the physician-to-population ratios (calculated in step1), Rj, at these locations as follows:\n\n\n\n\n\n\n\nComparing 2SFCA and E2SFCA\n\n\nReference: Luo, Wei., Qi, Yi. (2009) “An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians”, Health & Place, 2009, Vol.15 (4), p.1100-1107.\n\n\n2SFCA method tends to over estimate accessibility because distance decay is not considered, and thus identifies smaller total shortage areas. The sharper distance decay weight (weight2) used in E2SFCA identifies greater total shortage area (in terms of both physical area and population) than as lower distance decay weight(weight1).The policy implication is that using the E2SFCA method would more explicitly identify and delineate HPSAs. This would help allocate the limited resources to the most needy places."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#spatial-accessibility-measure-sam",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#spatial-accessibility-measure-sam",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Spatial Accessibility Measure (SAM)",
    "text": "Spatial Accessibility Measure (SAM)\nThe formula:\n\nwhere\n\nAai is the accessibility in ED i,\nnj is the capacity of the target facility j.\npi is the demand of this ED, and\ndij is the network distance between the EDi and each facility j.\n\n\nReference: Stamatis Kalogirou & Ronan Foley (2006) “Health, place and Hanly: Modelling accessibility to hospitals in Ireland”, Irish Geography, Volume 39(1), 2006, 52-68."
  },
  {
    "objectID": "lesson.html",
    "href": "lesson.html",
    "title": "Weekly Lesson Plan",
    "section": "",
    "text": "Week\nDate\nTopic\nStudent Assignment\n\n\n\n\n1\n30/8/2025\nIntroduction to Geospatial Analytics\n\n\n\n2\n6/9/2025\nSpatial Point Patterns Analysis (SPAA)\n\n\n\n3\n13/9/2025\nAdvanced Spatial Point Patterns Analysis\n\n\n\n4\n20/9/2025\nSpatial Weights and Applications\n\n\n\n5\n27/9/2025\nGlobal and Local Measures of Spatial Association\n\n\n\n6\n4/10/2025\nGeographic Segmentation\n\n\n\n\n11/10/2025\n\n\n\n\n7\n18/10/2025\nGeographically Weighted Explanatory Models\n\n\n\n8\n25/10/2025\nGeographically Weighted Predictive Models\n\n\n\n9\n1/11/2025\nModelling Geographic of Accessibility\n\n\n\n10\n8/11/2025\nSpatial Interaction Models"
  },
  {
    "objectID": "outline/Lesson02_outline.html",
    "href": "outline/Lesson02_outline.html",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This lesson aims to share with you the basic concepts and methods of Spatial Point Pattern Analysis. You will also gain hands experience on using spatstat, an R package specially designed for Spatial Point Pattern Analysis."
  },
  {
    "objectID": "outline/Lesson02_outline.html#content",
    "href": "outline/Lesson02_outline.html#content",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nIntroducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nKernel Density Estimation (KDE)\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function"
  },
  {
    "objectID": "outline/Lesson02_outline.html#lesson-slides",
    "href": "outline/Lesson02_outline.html#lesson-slides",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 2 slides."
  },
  {
    "objectID": "outline/Lesson02_outline.html#hands-on-exercise",
    "href": "outline/Lesson02_outline.html#hands-on-exercise",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nChapter 4: 1st Order Spatial Point Patterns Analysis Methods\nChapter 5: 2nd Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "outline/Lesson02_outline.html#core-readings",
    "href": "outline/Lesson02_outline.html#core-readings",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Core Readings",
    "text": "Core Readings\n\nYuan, Y., Qiang, Y., Bin Asad, K., and Chow, T. E. (2020). Point Pattern Analysis. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.13.\nYin, P. (2020). Kernels and Density Estimation. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.12\nChapter 7 Spatial Point Pattern Analysis of Roger S. Bivand, Edzer Pebesma and Virgilio Gómez-Rubio (2013) Applied Spatial Data Analysis with R (2nd Edition), Springer.\n\n\nEnrichment Resources\nProf. Luc Anselin on point pattern analysis (YouTube):\n\nPoint Pattern Analysis Concepts\nPoint Pattern Analysis: Clustered, Regular and Dispersed Patterns\nPoint Pattern Analysis: Nearest Neighbor Statistics\nPoint Pattern Analysis: Quadrat Counts\nPoint Pattern Analysis: F and J Functions\nPoint Pattern Analysis: K, L and Kd Functions"
  },
  {
    "objectID": "outline/Lesson02_outline.html#references",
    "href": "outline/Lesson02_outline.html#references",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC.\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4 • Ripley’s K-function.\n\n\nApplications\n\nNaveen Donthu and Roland T. Rust (1989) “Estimating Geographic Customer Densities Using Kernel Density Estimation”, Marketing Science, Vol. 8, No. 2, pp. 191-203.\nJoseph Wartman and Nicholas E. Malasavage (2010). “Spatial Analysis for Identifying Concentrations of Urban Damage” in Methods and Techniques in Urban Engineering, Armando Carlos de Pina Filho and Aloisio Carlos dePina (Ed.), ISBN: 978-953-307-096-4, InTech.\nGiuseppe Borruso and Andrea Porceddu (2009) “A Tale of Two Cities: Density Analysis of CBD on Two Midsize Urban Areas in Northeastern Italy” in Murgante, Beniamino; Borruso, Giuseppe & Lapucci, Alessandra (2009) Studies in Computational Intelligence, Geocomputation and Urban Planning, pp.37-56."
  },
  {
    "objectID": "outline/Lesson02_outline.html#all-about-r",
    "href": "outline/Lesson02_outline.html#all-about-r",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nspatstat at R Cran\n\nspatstat resource."
  },
  {
    "objectID": "outline/Lesson04_outline.html",
    "href": "outline/Lesson04_outline.html",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this lesson, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights."
  },
  {
    "objectID": "outline/Lesson04_outline.html#content",
    "href": "outline/Lesson04_outline.html#content",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "Content",
    "text": "Content\n\nTobler’s First law of Geography\nPrinciples of Spatial Autocorrelation\nConcepts of Spatial Proximity and Spatial Weights\n\nContiguity-Based Spatial Weights: Rook’s & Queen’s\nDistance-Band Spatial Weights: fixed and adaptive\n\nApplications of Spatial Weights\n\nSpatially lagged variables\nGeographically Weighted Summary Statistics\nSpatially lagged rates"
  },
  {
    "objectID": "outline/Lesson04_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson04_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 4 slides.\nHands-on Exercise 4\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\n\nAlternatively\n\nChapter 9: Modelling Areal Data of Applied Spatial Data Analysis with R (2nd Edition). This book is available in smu digital library. Until section 9.3.1."
  },
  {
    "objectID": "outline/Lesson04_outline.html#references",
    "href": "outline/Lesson04_outline.html#references",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "References",
    "text": "References\n\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "outline/Lesson04_outline.html#all-about-r",
    "href": "outline/Lesson04_outline.html#all-about-r",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "All About R",
    "text": "All About R\n\nspdep package\n\ndnearneigh()\nknearneigh()"
  },
  {
    "objectID": "outline/Lesson06_outline.html#content",
    "href": "outline/Lesson06_outline.html#content",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Content",
    "text": "Content\n\nBasic concepts of geographic segmentation\nConventional cluster analysis techniques\nApproaches for clustering geographically referenced data\n\nHierarchical clustering with spatial constraints\nMinimum spanning trees\nClustGeo method"
  },
  {
    "objectID": "outline/Lesson06_outline.html#lesson-slides",
    "href": "outline/Lesson06_outline.html#lesson-slides",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Lesson slides",
    "text": "Lesson slides\n\nLesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis slides."
  },
  {
    "objectID": "outline/Lesson06_outline.html#hands-on-exercise",
    "href": "outline/Lesson06_outline.html#hands-on-exercise",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n12 Geographical Segmentation with Spatially Constrained Clustering Techniques\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nAssuncao, R. M., Neves, M.C., Camara, G. and Costa Freitas, C.D. 2006. “Efficient Regionalization Techniques for Socio-Economic Geographical Units Using Minimum Spanning Trees”. International Journal of Geographical Information Science 20: 797-811. (Available in SMU digital library)\nChavent, M., Kuentz-Simonet, V., Labenne,A. and Saracco, J. 2018. “ClustGeo: an R package for hierarchical clustering with spatial constraints” Computational Statistics, 33: 1799-1822. (Available in SMU digital library)\n\n\n\nReferences\n\nRovan, J. and Sambt, J. (2003) “Socio-economic Differences Among Slovenian Municipalities: A Cluster Analysis Approach”, Developments in Applied Statistics, pp. 265-278.\n\nDemeter, T. and Bratucu, G. (2013) “Statistical Analysis Of The EU Countries from A Touristic Point of View”, Bulletin of the Transilvania University of Braşov, 6(55): 121-130.\nBrown, N.S. & Watson, P. (2012) “What can a comprehensive plan really tell us about a region?: A cluster analysis of county comprehensive plans in Idaho”, Western Economics Forum. Pp.22-37.\nJaya, I.G.M., Ruchjana, B.N., Andriyana, Y. & Agata, R (2019) “Clustering with spatial constraints: The case of diarrhea in Bandung city, Indonesia”\nde Souza, D. C. & Taconeli, C. A. (2022) “Spatial and non-spatial clustering algorithm in the analysis of Brazilian educational data”, Communications in Statistics: Case Studies, Data Analysis, and Applications. Vol. 8, No. 4, 588-606. (Available in SMU digital library)\n\n\n\nAll About R\n\nHierarchical Cluster Analysis.\nskater: A function from spdep package that implements a SKATER procedure for spatial clustering analysis.\nClustGeo: Hierarchical Clustering with Spatial Constraints\n\nIntroduction to Clustgeo"
  },
  {
    "objectID": "outline/Lesson08_outline.html",
    "href": "outline/Lesson08_outline.html",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson08_outline.html#content",
    "href": "outline/Lesson08_outline.html#content",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson08_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson08_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 8 slides.\nHandout of Hands-on Exercise 8.\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nStefanos Georganos et. al. (2019) “Geographical Random Forests: A Spatial Extension of the Random Forest Algorithm to Address Spatial Heterogeneity in Remote Sensing and Population Modelling”, Geocarto International, DOI: 10.1080/10106049.2019.1595177.\nGeorganos, S. and Kalogirou, S. (2022) “A Forest of Forests: A Spatially Weighted and Computationally Efficient Formulation of Geographical Random Forests”. ISPRS, International Journal of Geo-Information, 2022, 11, 471. https://www.mdpi.com/2220-9964/11/9/471\n\n\n\nReferences\n\nGeorge Grekousis et. al. (2022) “Ranking the importance of demographic, socioeconomic, and underlying health factors on US COVID-19 deaths: A geographical random forest approach”, Health and Place, vol. 74, pp. 1-12.\nYaowen Luo, Jianguo Yan & Stephen McClure. (2020) “Distribution of the environmental and socioeconomic risk factors on COVID-19 death rate across continental USA: a spatial nonlinear analysis”, Environmental Science and Pollution Research, 28:6587–6599.\nEun-Hee Koh, Eunhee Lee, & Kang-Kun Lee (2020) “Application of geographically weighted regression models to predict spatial characteristics of nitrate contamination: Implications for an effective groundwater management strategy”, Journal of Environmental Management. Vol. 268\n\n\n\nAll About R\n\nSpatialML"
  },
  {
    "objectID": "outline/Lesson10_outline.html",
    "href": "outline/Lesson10_outline.html",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of Spatial Interaction Models."
  },
  {
    "objectID": "outline/Lesson10_outline.html#content",
    "href": "outline/Lesson10_outline.html#content",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Content",
    "text": "Content\n\nBasic Concepts of Spatial Interaction Models\n\nA Family of Gravity Models\n\nUnconstrained\nOrigin constrained\nDestination constrained\nDoubly constrained\n\nStatistical Approach\nInterpreting and Visualising Modelling Results\n\nSpatial Econometric Interaction Models\n\nThe general formula\n\nWhat is Econometrics?\nWhat is Spatial Econometrics?\n\nSpatial Econometric Interaction Model Specifications"
  },
  {
    "objectID": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 10 slides in html format.\nHands-on Exercise 10a: Processing and Visualising Flow Data.\nHands-on Exercise 10b: Calibrating Spatial Interaction Models with R."
  },
  {
    "objectID": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nRead before lesson:\n\nKingsley E. Haynes and A. Stewart Fotheringham (2020) Gravity and Spatial Interaction Models, Web Book of Regional Science. Chapter 1 and Sub-section 2.2.\nFarmer, C. and Oshan, T. (2017). “Spatial interaction”. The Geographic Information Science & Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2017.4.5 (link is external)\nFlowerdew R, Aitkin M (1982) “A method of fitting the gravity model based on the Poisson distribution”. Journal of Regional Science, 22: 191–202.\n” Chapter 6. Spatial econometrics - common models”, Handbook of Spatial Analysis.\nJames P. LeSage and R. Kelley Pace (2008) “Spatial Econometric Modeling of Origin-Destination Flows”. Journal of Regional Science, Vol. 48, No. 5, pp. 941-967."
  },
  {
    "objectID": "outline/Lesson10_outline.html#reference",
    "href": "outline/Lesson10_outline.html#reference",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Reference",
    "text": "Reference\n\nPavel KLAPKA, et. al. (2013) “The Footfall of Shopping Centre in Olomouc (CZECH REPUBLIC): An Application of The Gravity Model”, Moravian Geographical Reports, Vol 21, pp. 12-26.\n\nYang Yue et. al. (2012) “Exploratory calibration of a spatial interaction model using taxi GPS trajectories”, Computers, Environment and Urban Systems, 36 (2012) 140–153.\nMorito Tsutsumi and Kazuki Tamesue (2012) “Intraregional flow problem in spatial econometric model for origin-destination flows”, Environment and Planning B: Planning and Design, Vol. 39, pp. 1006-1015. Access via SMU e-journal."
  },
  {
    "objectID": "outline/Lesson10_outline.html#all-about-r",
    "href": "outline/Lesson10_outline.html#all-about-r",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "All About R:",
    "text": "All About R:\n\nPaul Roback and Julie Legler (2020) Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R, CRC Press. On line version (January 26, 2021). Chapter 4\nNegative Binomial Regression and Poisson Regression from UCLA Institute for Digital Research & Education.\nspflow\nHome-to-work commuting flows within the municipalities around Paris"
  },
  {
    "objectID": "Quarto.html",
    "href": "Quarto.html",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#overview",
    "href": "Quarto.html#overview",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#create-websites-and-blogs",
    "href": "Quarto.html#create-websites-and-blogs",
    "title": "All About Quarto",
    "section": "Create websites and blogs",
    "text": "Create websites and blogs\n\nCreating a Website\nWebsite Navigation\nDocument Listings"
  },
  {
    "objectID": "Quarto.html#authoring-guide",
    "href": "Quarto.html#authoring-guide",
    "title": "All About Quarto",
    "section": "Authoring Guide",
    "text": "Authoring Guide\n\nMarkdown Basics\nFigures\nTables\nDiagrams\nCitations & Footnotes\nCross References\nArticle Layout"
  },
  {
    "objectID": "Quarto.html#useful-web-resources",
    "href": "Quarto.html#useful-web-resources",
    "title": "All About Quarto",
    "section": "Useful web resources",
    "text": "Useful web resources\n\nAwesome Quarto. This github repository provides a comprehensive listing of Quarto resources. It should be the second stop (after Quarto homepage) if you need to look for revelent materials about Quarto."
  },
  {
    "objectID": "Take-home_Ex01.html",
    "href": "Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively."
  },
  {
    "objectID": "Take-home_Ex01.html#setting-the-scene",
    "href": "Take-home_Ex01.html#setting-the-scene",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively."
  },
  {
    "objectID": "Take-home_Ex01.html#objectives",
    "href": "Take-home_Ex01.html#objectives",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Objectives",
    "text": "Objectives\nBy and large, road traffic accidents can be attributed by two major factors, namely: behavioural and environmental factors. Behavioural factors in driving are considered to be major causes of traffic accidents either in direct or indirect manner (Lewin, 1982). These factors can be further grouped into two as, driver behavior (also called driver/driving style) and driver performance, in other words, driver/driving skills (Elander, West, & French, 1993). Environmental factors, on the other hand, includes but not limited to weather condition such as poor visibility during heavy rain or foggy and road conditions such as sharp bend road, slippery slope road, and blind spot.\nPrevious studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\nIn view of this, you are tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods."
  },
  {
    "objectID": "Take-home_Ex01.html#the-data",
    "href": "Take-home_Ex01.html#the-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this exercise, three basic data sets must be used, they are:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX.\n\nStudents are free to include other data sets if they help in the study."
  },
  {
    "objectID": "Take-home_Ex01.html#grading-criteria",
    "href": "Take-home_Ex01.html#grading-criteria",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readr, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them).\n\n\n\nGeospatial Analysis (25 marks): In this exercise, you are expected to utilize the geospatial analytics methods introduced in class, along with the R packages provided during the hands-on exercises, to perform your analysis. You will be assessed on your ability to apply these methods correctly and to provide accurate interpretations and discussions of the analysis results.\nGeovisualisation and Geocommunication (25 marks): In this section, your ability to effectively communicate complex geospatial analysis results through user-friendly visual representations will be assessed. Since this course is focused on geospatial analysis, it is crucial that you demonstrate proficiency in using appropriate geovisualization techniques to clearly convey the findings of your analysis.\nReproducibility (20 marks): This is a key learning outcome of this course. You will be assessed on your ability to thoroughly document the analysis procedures using code chunks within Quarto. It is important to note that simply providing the code chunks is insufficient; you must also include explanations of the purpose behind each step and the R function(s) used.\nBonus (10 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data. The methods used must be geospatial in nature."
  },
  {
    "objectID": "Take-home_Ex01.html#submission-instructions",
    "href": "Take-home_Ex01.html#submission-instructions",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nThe R project of the take-home exercise must be pushed onto your Github repository.\nYou are required to provide the links to Netlify service of the take-home exercise write-up and github repository on eLearn."
  },
  {
    "objectID": "Take-home_Ex01.html#due-date",
    "href": "Take-home_Ex01.html#due-date",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Due Date",
    "text": "Due Date\n22nd September 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex01.html#references",
    "href": "Take-home_Ex01.html#references",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "References",
    "text": "References\n\nWHO (2023) Road traffic injuries\nRoad traffic deaths and injuries in Thailand\nLewin, I. (1982). Driver training: A perceptual-motor skill approach. Ergonomics, 25(10), 917–924.\nElander, J., West, R., & French, D. (1993). Behavioral correlates of individual differences in road-traffic crash risk: An examination of methods and findings. Psychological Bulletin, 113(2), 279."
  },
  {
    "objectID": "Take-home_Ex01.html#survival-tips",
    "href": "Take-home_Ex01.html#survival-tips",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Survival Tips",
    "text": "Survival Tips"
  },
  {
    "objectID": "Take-home_Ex01.html#learning-from-seniors",
    "href": "Take-home_Ex01.html#learning-from-seniors",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Learning from seniors",
    "text": "Learning from seniors\n\nCHAI ZHIXUAN  This is one of the two submission that includes steps on how to download the Passenger O-D data by using LTA DataMall API and opensource Postmen. Refer to sub-section 3.1.1 Aspatial data. Although it is incomplete (Step 3 :)) but still one of the best.\n\nKRISTINE JOY PAAS  Have done well in all five grading criteria especially the reproducibility, geovisualisation and geocommunication criteria. Geospatial Analytics criterion can be improved by including a paragraph describing the purpose, concepts and methods of the geospatial analytics used.\nKYLIE TAN JING YI  Section 5: Spatial Association Analysis of this submission provides a comprehensive discussion of the methods used and analysis results.\nMUHAMAD AMEER NOOR  Have done well in all five grading criteria including a short write-up of the geospatial analytics methods used.\nNEO YI XIN This submission put function programming of R into good used. For example, subsection Processing the aspatial OD data for processing data with same structure repetitively, Task 1: Geovisulisation and Analysis to ensure that a same classification scale are used. Further more Sub-section Computing Distance-Based Spatial Weights Matrix serves as a good example on how to discussion geospatial analytics methods used. There are at least three other students did show the spatial weights map but they are way too messy."
  },
  {
    "objectID": "Take-home_Ex03a.html",
    "href": "Take-home_Ex03a.html",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 3. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex03a.html#setting-the-scene",
    "href": "Take-home_Ex03a.html#setting-the-scene",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nAccording to Wikipedia, financial inclusion is the availability and equality of opportunities to access financial services. It refers to processes by which individuals and businesses can access appropriate, affordable, and timely financial products and services - which include banking, loan, equity, and insurance products. It provides paths to enhance inclusiveness in economic growth by enabling the unbanked population to access the means for savings, investment, and insurance towards improving household income and reducing income inequality."
  },
  {
    "objectID": "Take-home_Ex03a.html#the-task",
    "href": "Take-home_Ex03a.html#the-task",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "The Task",
    "text": "The Task\nIn this take-home exercise, you are required to build an explanatory model to determine factors affecting financial inclusion by using geographical weighted regression methods."
  },
  {
    "objectID": "Take-home_Ex03a.html#the-data",
    "href": "Take-home_Ex03a.html#the-data",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, either FinScope Uganda 2023 or FinScope Tanzania 2023 should be used. The study should be conducted at the district level. The district level boundary GIS data can be downloaded from geoBoundaries portal."
  },
  {
    "objectID": "Take-home_Ex03a.html#grading-criteria",
    "href": "Take-home_Ex03a.html#grading-criteria",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate non-spatial regression method and geographically weighted regression methods learned in Lesson 7 to perform the analysis. You will be assessed on your ability:\n\nto describe the methods used correctly including model diagnostics, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex03a.html#submission-instructions",
    "href": "Take-home_Ex03a.html#submission-instructions",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex03a.html#due-date",
    "href": "Take-home_Ex03a.html#due-date",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Due Date",
    "text": "Due Date\n10th November 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex03a.html#learning-from-senior",
    "href": "Take-home_Ex03a.html#learning-from-senior",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors."
  },
  {
    "objectID": "Take-home_Ex03a.html#q-a",
    "href": "Take-home_Ex03a.html#q-a",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex03a.html#peer-learning",
    "href": "Take-home_Ex03a.html#peer-learning",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex03a.html#reference",
    "href": "Take-home_Ex03a.html#reference",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Reference",
    "text": "Reference\n\nFinScope Tanzania 2023\nFinScope Uganda 2023\n\n\nResearch articles\n\nKaliba, Aloyce R ; Bishagazi, Kaihula P ; Gongwe, Anne G (2023) “Financial Inclusion in Tanzania Determinants, Barriers, and Impact”, The Journal of developing areas, Vol.57 (2), pp.65-87. SMU library e-journal.\nJana S. Hamdan, Katharina Lehmann-Uschner & Lukas Menkhoff (2022) Mobile Money, Financial Inclusion, and Unmet Opportunities: Evidence from Uganda, The Journal of Development Studies, 58:4, 671-691. SMU library e-journal.\nNguyen, Nhan Thien, et. al. (2021) “The convergence of financial inclusion across provinces in Vietnam: A novel approach” PloS one, Vol.16 (8). SMU library e-journal."
  },
  {
    "objectID": "KNIME.html",
    "href": "KNIME.html",
    "title": "KNIME Resources",
    "section": "",
    "text": "This page provides links to a selected resources to learn KNIME and it’s extension."
  },
  {
    "objectID": "KNIME.html#knime",
    "href": "KNIME.html#knime",
    "title": "KNIME Resources",
    "section": "KNIME",
    "text": "KNIME\n\nGetting Started Guide"
  },
  {
    "objectID": "KNIME.html#knime-courses",
    "href": "KNIME.html#knime-courses",
    "title": "KNIME Resources",
    "section": "KNIME Courses",
    "text": "KNIME Courses\n\nL1-DS KNIME Analytics Platform for Data Scientists: Basics\nL1-DW KNIME Analytics Platform for Data Wranglers: Basics\nL2-DS KNIME Analytics Platform for Data Scientists: Advanced\nL2-DW KNIME Analytics Platform for Data Wranglers: Advanced"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#content",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#content",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Content",
    "text": "Content\n\nAn Overview of Geospatial Data Models\n\nVector and raster data model\nCoordinate systems and map projection\n\nVector Data Wrangling and Analysis Methods\nRaster Data Wrangling and Analysis Methods\n\n\nThis lesson consists of two parts. First, I will talk about Geospatial Data Models. For students who have taken SMT201 GIS for Urban Planning, this is not new at all. However, for students who did not read SMT201, this will be new. Anyway, the focus of this section will be on R. Hence, even for students who have taken SMT201 before, this will be a good revision.\nIn part two of this lesson, I will introduce sp package. It is a relatively new R package specially developed to handle geospatial data R using tidyverse principle."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-models",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-models",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Geospatial Data Models",
    "text": "Geospatial Data Models\nWhy should we worry about?\n\n\nIt is important for us to note that what ever data capture in a database is a model of the real world. When we say model, this means that it is a simplify version of the real world and not the real world themselves."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#basic-spatial-data-models",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#basic-spatial-data-models",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Basic Spatial Data Models",
    "text": "Basic Spatial Data Models\n\nVector - implementation of discrete object conceptual model\n\nPoint, line and polygon representations.\nWidely used in cartography, and network analysis.\n\nRaster – implementation of field conceptual model\n\nArray of cells used to represent objects.\nUseful as background maps and for spatial analysis.\n\n\n\nIn general, there are two types of geospatial data models, namely vector and raster data models.\nVector data model tends to be used to store geospatial data that are discrete in nature. For example bus stop, building footprint, planning area.\nRaster data model, one the other hands, are used to store continuous fenomena such as air polution, elevation and precipitation."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#coordinate-systems-and-map-projections",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#coordinate-systems-and-map-projections",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Coordinate Systems and Map Projections",
    "text": "Coordinate Systems and Map Projections\nWhat is a coordinate system?\n\n\nA coordinate system is an important property of an geospatial data. It provides a location reference to the geospatial data.\n\nThere are two common types of coordinate systems used in mapping, namely: geographic coordinate systems and projected coordinate system.\n\n\n\n\n\n\n\nFurther Reading\n\n\n\nRefer to this article and Chapter 9 Coordinate Systems to learn more about map projection.\n\n\n\n\n\n\n\n\n\nA coordinate system is a reference system used to represent the locations of geographic features, imagery, and observations such as GPS locations within a common geographic framework.\nEach coordinate system is defined by:\n\nIts measurement framework which is either geographic (in which spherical coordinates are measured from the earth’s center) or planimetric (in which the earth’s coordinates are projected onto a two-dimensional planar surface).\nUnit of measurement (typically feet or meters for projected coordinate systems or decimal degrees for latitude–longitude).\nThe definition of the map projection for projected coordinate systems.\nOther measurement system properties such as a spheroid of reference, a datum, and projection parameters like one or more standard parallels, a central meridian, and possible shifts in the x- and y-directions."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#standard-for-geospatial-data-handling-and-analysis",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#standard-for-geospatial-data-handling-and-analysis",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Standard for Geospatial Data Handling and Analysis",
    "text": "Standard for Geospatial Data Handling and Analysis\n\n\n\n\n\n\n\nFurther Reading\n\n\nFor more information, visit this link.\n\n\n\n\nThe OGC OpenGIS Implementation Standard for Geographic Information / ISO 19125 defines:\n\nGeometric objects which can be of type point, line, polygon, multi-point, etc, and are associated to a given Coordinate Reference System;\nMethods on geometric objects return properties like dimension, boundary, area, centroid, etc;\nMethods for testing spatial relations between geometric objects equals, disjoint, intersects, touches, crosses, within, contains, overlaps and relate, which returns TRUE or FALSE;\nMethods that support spatial analysis distance, which returns a distance, and buffer, convex hull, intersection, union, difference, and symmetric difference, which returns new geometric objects.\n\nSource: www.opengeospatial.org/standards/sfa"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#an-introduction-to-simple-features",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#an-introduction-to-simple-features",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "An introduction to simple features",
    "text": "An introduction to simple features\n\nfeature: abstraction of real world phenomena (type or instance); has a geometry and other attributes (properties)\nsimple feature: feature with all geometric attributes described piecewise by straight line or planar interpolation between sets of points (no curves)\nIt is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-object-framework",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-object-framework",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Geospatial Data Object Framework",
    "text": "Geospatial Data Object Framework\n\nTo begin with, all contributed packages for handling spatial data in R had different representations of the data. This made it difficult to exchange data both within R between packages, and between R and external le formats and applications.\nThe first general package to provide classes and methods for spatial data types that was developed for R is called sp. It was first released on CRAN in 2005.\nIn late October 2016, sf was first released on CRAN to provide standardised support for vector data in R."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#r-packages-that-support-spatial-classes",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#r-packages-that-support-spatial-classes",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "R packages that support spatial classes",
    "text": "R packages that support spatial classes\nIn general, three R packages will be used to handle vector-based geospatial data in spatial classes, they are:\n\nsp provides classes and methods for dealing with spatial data in R.\nrgdal allows R to understand the structure of a geospatial data file by providing functions to read and convert geospatial data into easy-to-work-with R dataframes.\nrgeos implements the methods of the OGC standard."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#introducing-sf-package",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#introducing-sf-package",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Introducing sf Package",
    "text": "Introducing sf Package\n\nsf package provides a syntax and data-structures which are coherent with the tidyverse.\nA quick introduction can be found here.\nFor more detail, visit this link."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#sf-functions",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#sf-functions",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "sf functions",
    "text": "sf functions\n\nGeospatial data handling\nGeometric confirmation\nGeometric operations\nGeometry creation\nGeometry operations\nGeometric measurement"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#references",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#references",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "References",
    "text": "References\nAll About sf package\n\n\nReference manual\nTidy spatial data analysis\n\nVignettes:\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\nPlotting Simple Features\nMiscellaneous\nSpherical geometry in sf using s2geometry\n\nOthers:\n\nR spatial follows GDAL and PROJ development"
  }
]
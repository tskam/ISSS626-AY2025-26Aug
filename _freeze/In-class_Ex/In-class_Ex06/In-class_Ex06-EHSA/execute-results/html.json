{
  "hash": "d0ae4904c13554c9464ee3a4a470b74c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class Exercise 6: Emerging Hot Spot Analysis\"\nauthor: \"Dr. Kam Tin Seong<br/>Assoc. Professor of Information Systems(Practice)\"\ninstitute: \"School of Computing and Information Systems,<br/>Singapore Management University\"\ndate: \"last-modified\"\nformat: \n  revealjs:\n    pdf: default\n    width: 1600\n    height: 900\n    show-notes: false\n    slide-number: true\n    show-slide-number: all\nexecute: \n  eval: false\n  echo: true\n  warning: false\n  freeze: true\neditor: visual\n---\n\n## Overview\n\n::::::: columns\n::::: {.column width=\"50%\"}\n:::: {style=\"font-size: 0.8em\"}\nEmerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\n-   Building a space-time cube,\n-   Calculating Getis-Ord local Gi\\* statistic for each bin by using an FDR correction,\n-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,\n-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\n::: callout-important\nIt is highly recommended to read [Emerging Hot Spot Analysis](https://sfdep.josiahparry.com/articles/understanding-emerging-hotspots) before you continue the exercise.\n:::\n::::\n:::::\n\n::: {.column width=\"50%\"}\n![](img/image3.png)\n:::\n:::::::\n\n## Getting started\n\n### Installing and Loading the R Packages\n\n::::: columns\n:::: {.column width=\"50%\"}\nAs usual, `p_load()` of **pacman** package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\n\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, plotly, and tidyverse.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, sfdep, tmap, \n               plotly, tidyverse, \n               Kendall)\n```\n:::\n\n:::\n::::\n:::::\n\n## The Data\n\n:::: columns\n::: {.column width=\"50%\"}\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\n-   Hunan, a geospatial data set in ESRI shapefile format, and\n-   Hunan_GDPPC, an attribute data set in csv format.\n\nBefore getting started, reveal the content of *Hunan_GDPPC.csv* by using Notepad and MS Excel.\n:::\n::::\n\n------------------------------------------------------------------------\n\n### Importing geospatial data\n\nIn the code chunk below, `st_read()` of **sf** package is used to import *Hunan* shapefile into R.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n```\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n### Importing attribute table\n\n::::: columns\n:::: {.column width=\"65%\"}\nIn the code chunk below, `read_csv()` of **readr** is used to import *Hunan_GDPPC.csv* into R.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nGDPPC <- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n```\n:::\n\n:::\n::::\n:::::\n\n## Creating a Time Series Cube\n\nBefore getting started, students must read this [article](https://sfdep.josiahparry.com/articles/spacetime-s3.html) to learn the basic concept of spatio-temporal cube and its implementation in sfdep package.\n\nIn the code chunk below, [`spacetime()`](https://sfdep.josiahparry.com/reference/spacetime.html) of sfdep ised used to create an spatio-temporal cube.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n```\n:::\n\n:::\n\nNext, `is_spacetime_cube()` of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nis_spacetime_cube(GDPPC_st)\n```\n:::\n\n:::\n\nThe **TRUE** return confirms that *GDPPC_st* object is indeed an time-space cube.\n\n## Computing Gi\\*\n\nNext, we will compute the local Gi\\* statistics.\n\n### Deriving the spatial weights\n\n::::::: columns\n:::: {.column width=\"60%\"}\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\") %>%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n```\n:::\n\n:::\n::::\n\n:::: {.column width=\"40%\"}\n::: callout-tip\n# Things to learn from the code chunk above\n\n-   `activate()` of dplyr package is used to activate the geometry context\n-   `mutate()` of dplyr package is used to create two new columns *nb* and *wt*.\n-   Then we will activate the data context again and copy over the nb and wt columns to each time-slice using `set_nbs()` and `set_wts()`\n    -   row order is very important so do not rearrange the observations after using `set_nbs()` or `set_wts()`.\n:::\n::::\n:::::::\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\n## Computing Gi\\*\n\n::::: columns\n:::: {.column width=\"60%\"}\nWe can use these new columns to manually calculate the local Gi\\* for each location. We can do this by grouping by *Year* and using `local_gstar_perm()` of sfdep package. After which, we `use unnest()` to unnest *gi_star* column of the newly created *gi_starts* data.frame.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ngi_stars <- GDPPC_nb %>% \n  group_by(Year) %>% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n```\n:::\n\n:::\n::::\n:::::\n\n## Mann-Kendall Test\n\nA **monotonic series** or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.\n\nH0: No monotonic trend\n\nH1: Monotonic trend is present\n\n**Interpretation**\n\n-   Reject the null-hypothesis null if the p-value is smaller than the alpha value (i.e. 1-confident level)\n-   Tau ranges between -1 and 1 where:\n    -   -1 is a perfectly decreasing series, and\n    -   1 is a perfectly increasing series.\n\n::: callout-important\nYou are encouraged to read [Mann-Kendall Test For Monotonic Trend](https://vsp.pnnl.gov/help/vsample/design_trend_mann_kendall.htm) to learn more about the concepts and method of Mann-Kendall test..\n:::\n\n------------------------------------------------------------------------\n\n### Mann-Kendall Test on Gi\n\n:::::::: columns\n::::: {.column width=\"50%\"}\nWith these Gi\\* measures we can then evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ncbg <- gi_stars %>% \n  ungroup() %>% \n  filter(County == \"Changsha\") |> \n  select(County, Year, gi_star)\n```\n:::\n\n:::\n\nNext, we plot the result by using ggplot2 functions.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n```\n:::\n\n:::\n:::::\n\n:::: {.column width=\"50%\"}\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n:::\n\n:::\n::::\n::::::::\n\n------------------------------------------------------------------------\n\n### Interactive Mann-Kendall Plot\n\n:::::: columns\n:::: {.column width=\"50%\"}\nWe can also create an interactive plot by using `ggplotly()` of **plotly** package.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\np <- ggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n```\n:::\n\n:::\n::::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n:::\n\n:::\n::::::\n\n------------------------------------------------------------------------\n\n### Printing Mann-Kendall test report\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\ncbg %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>% \n  tidyr::unnest_wider(mk)\n```\n:::\n\n:::\n\nIn the above result, **sl** is the p-value. With reference to the results, we will reject the hypothesis null and infer that a slight upward trend.\n\n------------------------------------------------------------------------\n\n### Mann-Kendall test data.frame\n\nWe can replicate this for each location by using `group_by()` of dplyr package.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa <- gi_stars %>%\n  group_by(County) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\nhead(ehsa)\n```\n:::\n\n:::\n\n------------------------------------------------------------------------\n\n### Mann-Kendall test data.frame\n\nWe can also sort to show significant emerging hot/cold spots\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:10)\nhead(emerging)\n```\n:::\n\n:::\n\n## Performing Emerging Hotspot Analysis\n\n:::::: columns\n::: {.column width=\"50%\"}\nLastly, we will perform EHSA analysis by using [`emerging_hotspot_analysis()`](https://sfdep.josiahparry.com/reference/emerging_hotspot_analysis.html) of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n:::\n\n:::: {.column width=\"50%\"}\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa <- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)\n```\n:::\n\n:::\n::::\n::::::\n\n------------------------------------------------------------------------\n\n### Visualising the distribution of EHSA classes\n\nIn the code chunk below, ggplot2 functions is used to reveal the distribution of EHSA classes as a bar chart.\n\n::: {style=\"font-size: 1.5em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n```\n:::\n\n:::\n\nFigure above shows that sporadic cold spots class has the high numbers of county.\n\n------------------------------------------------------------------------\n\n### Visualising EHSA\n\n::::::::: columns\n::::::: {.column width=\"50%\"}\n::: {style=\"font-size: 0.8em\"}\nIn this section, you will learn how to visualise the geographic distribution EHSA classes. However, before we can do so, we need to join both *hunan* and *ehsa* together by using the code chunk below.\n:::\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan_ehsa <- hunan %>%\n  left_join(ehsa,\n            by = join_by(County == location))\n```\n:::\n\n:::\n\n::: {style=\"font-size: 0.8em\"}\nNext, tmap functions will be used to plot a categorical choropleth map by using the code chunk below.\n:::\n\n::: {style=\"font-size: 1.2em\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nehsa_sig <- hunan_ehsa  %>%\n  filter(p_value < 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)\n```\n:::\n\n:::\n:::::::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n:::\n\n:::\n:::::::::\n\n------------------------------------------------------------------------\n\n### Interpretation of EHSA classes\n\n![](img/image4a.png)\n\n------------------------------------------------------------------------\n\n### Interpretation of EHSA classes\n\n![](img/image4b.png)\n\n------------------------------------------------------------------------\n\n### Interpretation of EHSA classes\n\n![](img/image4c.png)\n\n------------------------------------------------------------------------\n\n### Interpretation of EHSA classes\n\n![](img/image4d.png)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
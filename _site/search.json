[
  {
    "objectID": "Take-home_Ex03b.html",
    "href": "Take-home_Ex03b.html",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 2. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex03b.html#setting-the-scene",
    "href": "Take-home_Ex03b.html#setting-the-scene",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nHousing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.\nConventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, Geographical Weighted Models were introduced to better calibrate predictive models for housing resale prices."
  },
  {
    "objectID": "Take-home_Ex03b.html#the-task",
    "href": "Take-home_Ex03b.html#the-task",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "The Task",
    "text": "The Task\nIn this take-home exercise, you are required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023."
  },
  {
    "objectID": "Take-home_Ex03b.html#the-data",
    "href": "Take-home_Ex03b.html#the-data",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, HDB Resale Flat Prices provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.\nBelow is a list of recommended predictors to consider. However, students are free to include other appropriate independent variables.\n\nStructural factors\n\nArea of the unit\nFloor level\nRemaining lease\nAge of the unit\nMain Upgrading Program (MUP) completed (optional)\n\nLocational factors\n\nProxomity to CBD\nProximity to eldercare\nProximity to foodcourt/hawker centres\nProximity to MRT\nProximity to park\nProximity to good primary school\nProximity to shopping mall\nProximity to supermarket\nNumbers of kindergartens within 350m\nNumbers of childcare centres within 350m\nNumbers of bus stop within 350m\nNumbers of primary school within 1km"
  },
  {
    "objectID": "Take-home_Ex03b.html#grading-criteria",
    "href": "Take-home_Ex03b.html#grading-criteria",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use conventional multiple linear regression, random forest and geographically weighted random forest methods learned in class to calibrate the predictive models including model comparison. You will be assessed on your ability:\n\nto describe the methods used correctly, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex03b.html#submission-instructions",
    "href": "Take-home_Ex03b.html#submission-instructions",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex03b.html#due-date",
    "href": "Take-home_Ex03b.html#due-date",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Due Date",
    "text": "Due Date\n10th November 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex03b.html#learning-from-senior",
    "href": "Take-home_Ex03b.html#learning-from-senior",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors."
  },
  {
    "objectID": "Take-home_Ex03b.html#q-a",
    "href": "Take-home_Ex03b.html#q-a",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex03b.html#peer-learning",
    "href": "Take-home_Ex03b.html#peer-learning",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex03b.html#reference",
    "href": "Take-home_Ex03b.html#reference",
    "title": "Take-home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods",
    "section": "Reference",
    "text": "Reference\n\nSpatialML\nSpatialRF\n\n\nResearch articles\nWang, Shuli et. al. (2024) “Geographically weighted machine learning for modeling spatial heterogeneity in traffic crash frequency and determinants in US”, Accident analysis and prevention, Vol.199, p.107528-107528, Article 107528. SMU Library e-journal.\nLotfata, Aynaz & Georganos, Stefanos (2023) “Spatial machine learning for predicting physical inactivity prevalence from socioecological determinants in Chicago, Illinois, USA”, Journal of geographical systems, pp.1-21\nWu, Dongyu ; Zhang, Yingheng ; Xiang, Qiaojun (2024) “Geographically weighted random forests for macro-level crash frequency prediction”, Accident analysis and prevention, Vol.194, p.107370-107370, Article 107370."
  },
  {
    "objectID": "Take-home_Ex02.html",
    "href": "Take-home_Ex02.html",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 2. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex02.html#setting-the-scene",
    "href": "Take-home_Ex02.html#setting-the-scene",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nTourism is one of Thailand’s largest industries, accounting for some 20% of the gross domestic product (GDP). In 2019, Thailand earned 90 billion US$ from domestic and international tourism, but the COVID-19 pandemic caused revenues to crash to 24 billion US$ in 2020.\nFigure below shows the total revenue receipt from tourism sector from January 2019 until Feb 2023. The figure reveals that the revenue from tourism industry have been recovered gradually since September 2021.\n\nHowever, it is important to note that the tourism economy of Thailand are not evenly distributed. Figure below reveals that the tourism economy of Thailand are mainly focus on five provinces, namely Bangkok, Phuket, Chiang Mai, Sukhothai and Phetchaburi."
  },
  {
    "objectID": "Take-home_Ex02.html#objectives",
    "href": "Take-home_Ex02.html#objectives",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Objectives",
    "text": "Objectives\nAs a curious geospatial analytics green horn, you are interested to discover:\n\nif the key indicators of tourism economy of Thailand are independent from space and space and time.\n\nIf the tourism economy is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "Take-home_Ex02.html#the-task",
    "href": "Take-home_Ex02.html#the-task",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "The Task",
    "text": "The Task\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate function of sf and tidyverse, preparing the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at province level (including Bangkok) of Thailand.\na tourism economy indicators layer within the study area in sf polygon features.\na derived tourism economy indicator layer in spacetime s3 class of sfdep. Keep the time series at month and year levels.\n\nUsing the extracted data, perform global spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform local spatial autocorrelation analysis by using sfdep methods.\nUsing the extracted data, perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis above."
  },
  {
    "objectID": "Take-home_Ex02.html#the-data",
    "href": "Take-home_Ex02.html#the-data",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, two data sets shall be used, they are:\n\nThailand Domestic Tourism Statistics at Kaggle. You are required to use version 2 of the data set.\nThailand - Subnational Administrative Boundaries at HDX. You are required to use the province boundary data set."
  },
  {
    "objectID": "Take-home_Ex02.html#grading-criteria",
    "href": "Take-home_Ex02.html#grading-criteria",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate global and local measures of spatial autocorrelation, and emerging hot spot analysis methods in class to perform the analysis. You will be assessed on your ability:\n\nto describe the methods used correctly, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex02.html#submission-instructions",
    "href": "Take-home_Ex02.html#submission-instructions",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex02.html#due-date",
    "href": "Take-home_Ex02.html#due-date",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Due Date",
    "text": "Due Date\n13th October 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex02.html#learning-from-senior",
    "href": "Take-home_Ex02.html#learning-from-senior",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors.\n\nCHUA YAN TING: Have done well in all five grading criteria especially the geocommunication criterion.\nLIN SHUYAN Geospatial data wrangling is very comprehensively done especially identifying water points located outside Nigeria administrative boundary due to location precision issue.\nLOH SI YING Have done well in all five grading criteria especially the followings: (i) the geospatial wrangling are very comprehensively done including to exclude LGAs without water points from the analysis, (ii) managed to compute the p-values, (iii) Start each analysis by explaining the purpose of the analysis. Managed to relate the analysis results to the location context."
  },
  {
    "objectID": "Take-home_Ex02.html#learning-from-is415",
    "href": "Take-home_Ex02.html#learning-from-is415",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Learning from IS415",
    "text": "Learning from IS415\n\nKHANT MIN NAING: Very well done in all the five grading criteria especially the ability to provide a comprehensive overview of the analysis methods used and discussion on the analysis results.\nMATTHEW HO YIWEN Able to provide a clear and comprehensive discussion on the geospatial data wrangling process and to communicate the analysis results by using appropriate geovisualisation and data visualisation methods."
  },
  {
    "objectID": "Take-home_Ex02.html#q-a",
    "href": "Take-home_Ex02.html#q-a",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex02.html#peer-learning",
    "href": "Take-home_Ex02.html#peer-learning",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex02.html#reference",
    "href": "Take-home_Ex02.html#reference",
    "title": "Take-home Exercise 2: Discovering impacts of COVID-19 on Thailand tourism economy at the province level using spatial and spatio-temporal statistics",
    "section": "Reference",
    "text": "Reference\n\nIMPACT OF COVID-19 ON THAILAND’S TOURISM SECTOR\nCovid: Thailand tourism up but still below pre-pandemic level\nTourism in Thailand\n\n\nResearch articles\n\nUglješa Stankov et. al. (2017) “Spatial autocorrelation analysis of tourist arrivals using municipal data: A Serbian example”, Geographica Pannonica, Vol.21 (2), p.106-114. SMU e-journal.\nKhan, D. et. al. (2017) “Hot spots, cluster detection and spatial outlier analysis of teen birth rates in the U.S., 2003–2012”, Spatial and Spatio-temporal Epidemiology, Vol. 21, pp. 67–75.\nMuhammad Arif & Didit Purnomo (2017) “Measuring Spatial Cluster for Leading Industries in Surakarta with Exploratory Spatial Data Analysis (ESDA)”, Jurnal Ekonomi Pembangunan, Vol. 18 (1), pp. 64-81.\nJoshua T. Fergen * and Ryan D. Bergstrom (2021) “Social Vulnerability across the Great Lakes Basin: A County-Level Comparative and Spatial Analysis”, Sustainability, Vol. 13(13).\nV Putrenko, N Pashynska, and S Nazarenko (2018) “Data Mining of Network Events With Space-Time Cube Application”. In: R Westerholt, F-B Mocnik, and A Zipf (eds.), Proceedings of the 1st Workshop on Platial Analysis (PLATIAL’18), pp. 75–82. SMU e-journal.\nJamie Anne Boschan and Caterina G. Roman (2024) “Hot Spots of Gun Violence in the Era of Focused Deterrence: A Space-Time Analysis of Shootings in South Philadelphia”, Social Sciences, Vol. 13.\nMinkyung Kim and Sangdon Lee (2023) “Identification of Emerging Roadkill Hotspots on Korean Expressways Using Space–Time Cubes”, Int. J. Environ. Res. Public Health, 20(6)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Synopsis",
    "section": "",
    "text": "Where should the next new business outlet be located in order to optimise the profit? What are the location factors that affect the resale prices of HDB housing units? Which are the economic or service activities such as IT professional firms, car workshops, fast food chains (ie. KFC, McDonalds), coffee outlets (Starbucks, Ya Kun Kaya Toast, Toast Box) that tend to be located close to one another and which are the ones that tend to be a distance apart? Do these observed patterns and processes occur at random or are they being constrained by geographical factors? These and many other related questions are the challenges faced by data scientists and data analysts today especially when geographical data are used.\nGeospatial Analytics offers the solutions to these questions by providing data scientists and analysts a problem-driven and data-centric analysis framework focusing on discovering actionable understanding from geographically referenced data. It makes extensive use of geospatial data wrangling, geoprocessing, spatial statistical, geospatial machine learning and spatial data visualisation techniques to support decision- and strategy-making.\nThis course provides students with an introduction to the concepts, principles and methods of geospatial analytics and their practical applications of geospatial analytics in real world operations. Emphasis will be placed on:\n\nperforming geospatial data science tasks such as importing, tidying, manipulating, transforming, projecting and processing geospatial data programmatically,\nvisualising, analysing and describing geographical patterns and processes using appropriate geovisualisation and thematic mapping techniques,\nconducting geospatial analysis by using appropriate spatial statistics and Geomachine learning methods and\ncommunicating the geospatial analysis pipeline in a reproducible report."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Synopsis",
    "section": "Course structure",
    "text": "Course structure\n\nBasic Modules\nThis course comprises ten integrated components as shown below:"
  },
  {
    "objectID": "syllabus.html#grading-summary",
    "href": "syllabus.html#grading-summary",
    "title": "Synopsis",
    "section": "Grading Summary",
    "text": "Grading Summary\n\n\n\nAssignments / Assessment categories\nWeighting\n\n\n\n\nPre-lesson Learning and Hands-on Exercise (1.5% x 10)\n15%\n\n\nClass participation and In-class Exercise (20% x 10)\n20%\n\n\nTimed Individual Assessment\n20%\n\n\nTake-home Exercise (Not more than 3)\n45%\n\n\n\n\nPre-lesson Learning (5%)\nPre-lesson videos and recommended readings exercises will be released one week before the weekly lesson starts. A strict requirement for each class meeting is to complete the assigned readings and video before coming to class. Students are required write down at least one question or issue encountered while viewing the video or reading the recommended articles and post them on Piazza for discussion.\nStudent sharing of insights from readings of assigned materials on Piazza will form a large part of the learning in this course.\n\n\nHands-on Exercise (10%)\nHands-on exercises aim to provide students to gain hands-on experience on using KNIME and GAEK to perform geospatial analysis with real world use cases. It is important for students to complete the hands-on exercises before class.\n\n\nClass Participation and In-class Exercise (20%)\nIn-class exercise and discussion will extend the methods learned from the hands-on exercise to advanced modelling. The in-class discussion will also focus on how to interpret the analysis results and to communicate the analysis results by using appropriate map and data visualisation techniques. Students may also be quizzed in class and thereby contribute to in-class exercise.\n\n\nTimed Individual Assessment (20%)\nA timed individual assessment is designed to evaluate each student’s ability to complete an assignment within a specified time limit. The assessment will involve, but is not limited to, understanding the business problem, assembling the analytical sandbox, performing the analysis, and interpreting and communicating the results. It will be conducted in class and will not exceed 30 minutes.\n\n\nTake-home Exercise (45%)\nThere are three take-home exercises that are due throughout the term. They aim to provide students the opportunities to apply the methods learned in class by working through mini real-world cases. Each take-home exercise is an extension of the hands-on and in-class exercises. What this means is that, for example, in Lesson 2, students will learn the concept of spatial point processes and the hands-on exercise will provide students step-by-step guide on how to use KNIME and GAEK to perform spatial point patterns analysis. The in-class discussion, beside clarification of the concepts and usage of R packages syntax and argument, it will focus more on how to interpret and communicate the analysis results. Then the take-home exercise will require students to synthesise what they have learned from the readings, hands-on exercise and in-class exercise. The estimated workload will be about 6-8 hours per week.\nEach take-home exercise will carry a same weightage of 15%. The deliverable format of the take-home exercises and marking rubric of will be provided on the handout of the take-home exercise. Feedback on take-home exercise will be provided weekly before the weekly lesson starts. This is to ensure that students will learn from mistakes made in the earlier take-home exercise and improve their work progressively in the subsequent take-home exercises.\nStudents may work together to help one another with computer or geospatial issues and discuss the materials that constitute the take-home exercise. However, each student is required to prepare and submit the take-home exercise (including any computer work) on their own. Cheating is strictly prohibited. Cheating includes but not limited to: plagiarism and submission of work that is not the student’s.\n\n\nFinal Exam\nThere will be no final examination for this course."
  },
  {
    "objectID": "syllabus.html#reference",
    "href": "syllabus.html#reference",
    "title": "Synopsis",
    "section": "Reference",
    "text": "Reference\n\nGimond, Manuel. (2018) Introduction to GIS and Spatial Analysis.\nFloch, J.M., Marcon, E. and Puech, F. (2018) Handbook of Spatial Analysis: Theory and Application with R.\nEdzer Pebesma & Roger Bivand (2025) Spatial Data Science with Applications in R.\nRobin Lovelace, Jakub Nowosad & Jannes Muenchow (2024) Geocomputation with R.\nMoraga, Paula. (2023). Spatial Statistics for Data Science: Theory and Practice with R. Chapman & Hall/CRC Data Science Series. ISBN 9781032633510"
  },
  {
    "objectID": "Quarto.html",
    "href": "Quarto.html",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#overview",
    "href": "Quarto.html#overview",
    "title": "All About Quarto",
    "section": "",
    "text": "Getting Started\nRStudio IDE\nUsing R\nHTML Basics"
  },
  {
    "objectID": "Quarto.html#create-websites-and-blogs",
    "href": "Quarto.html#create-websites-and-blogs",
    "title": "All About Quarto",
    "section": "Create websites and blogs",
    "text": "Create websites and blogs\n\nCreating a Website\nWebsite Navigation\nDocument Listings"
  },
  {
    "objectID": "Quarto.html#authoring-guide",
    "href": "Quarto.html#authoring-guide",
    "title": "All About Quarto",
    "section": "Authoring Guide",
    "text": "Authoring Guide\n\nMarkdown Basics\nFigures\nTables\nDiagrams\nCitations & Footnotes\nCross References\nArticle Layout"
  },
  {
    "objectID": "Quarto.html#useful-web-resources",
    "href": "Quarto.html#useful-web-resources",
    "title": "All About Quarto",
    "section": "Useful web resources",
    "text": "Useful web resources\n\nAwesome Quarto. This github repository provides a comprehensive listing of Quarto resources. It should be the second stop (after Quarto homepage) if you need to look for revelent materials about Quarto."
  },
  {
    "objectID": "outline/Lesson10_outline.html",
    "href": "outline/Lesson10_outline.html",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of Spatial Interaction Models."
  },
  {
    "objectID": "outline/Lesson10_outline.html#content",
    "href": "outline/Lesson10_outline.html#content",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Content",
    "text": "Content\n\nBasic Concepts of Spatial Interaction Models\n\nA Family of Gravity Models\n\nUnconstrained\nOrigin constrained\nDestination constrained\nDoubly constrained\n\nStatistical Approach\nInterpreting and Visualising Modelling Results\n\nSpatial Econometric Interaction Models\n\nThe general formula\n\nWhat is Econometrics?\nWhat is Spatial Econometrics?\n\nSpatial Econometric Interaction Model Specifications"
  },
  {
    "objectID": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson10_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 10 slides in html format.\nHands-on Exercise 10a: Processing and Visualising Flow Data.\nHands-on Exercise 10b: Calibrating Spatial Interaction Models with R."
  },
  {
    "objectID": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson10_outline.html#self-reading-before-meet-up",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nRead before lesson:\n\nKingsley E. Haynes and A. Stewart Fotheringham (2020) Gravity and Spatial Interaction Models, Web Book of Regional Science. Chapter 1 and Sub-section 2.2.\nFarmer, C. and Oshan, T. (2017). “Spatial interaction”. The Geographic Information Science & Technology Body of Knowledge (4th Quarter 2017 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2017.4.5 (link is external)\nFlowerdew R, Aitkin M (1982) “A method of fitting the gravity model based on the Poisson distribution”. Journal of Regional Science, 22: 191–202.\n” Chapter 6. Spatial econometrics - common models”, Handbook of Spatial Analysis.\nJames P. LeSage and R. Kelley Pace (2008) “Spatial Econometric Modeling of Origin-Destination Flows”. Journal of Regional Science, Vol. 48, No. 5, pp. 941-967."
  },
  {
    "objectID": "outline/Lesson10_outline.html#reference",
    "href": "outline/Lesson10_outline.html#reference",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Reference",
    "text": "Reference\n\nPavel KLAPKA, et. al. (2013) “The Footfall of Shopping Centre in Olomouc (CZECH REPUBLIC): An Application of The Gravity Model”, Moravian Geographical Reports, Vol 21, pp. 12-26.\n\nYang Yue et. al. (2012) “Exploratory calibration of a spatial interaction model using taxi GPS trajectories”, Computers, Environment and Urban Systems, 36 (2012) 140–153.\nMorito Tsutsumi and Kazuki Tamesue (2012) “Intraregional flow problem in spatial econometric model for origin-destination flows”, Environment and Planning B: Planning and Design, Vol. 39, pp. 1006-1015. Access via SMU e-journal."
  },
  {
    "objectID": "outline/Lesson10_outline.html#all-about-r",
    "href": "outline/Lesson10_outline.html#all-about-r",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "All About R:",
    "text": "All About R:\n\nPaul Roback and Julie Legler (2020) Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R, CRC Press. On line version (January 26, 2021). Chapter 4\nNegative Binomial Regression and Poisson Regression from UCLA Institute for Digital Research & Education.\nspflow\nHome-to-work commuting flows within the municipalities around Paris"
  },
  {
    "objectID": "outline/Lesson08_outline.html",
    "href": "outline/Lesson08_outline.html",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson08_outline.html#content",
    "href": "outline/Lesson08_outline.html#content",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "",
    "text": "What is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "outline/Lesson08_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson08_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 8 slides.\nHandout of Hands-on Exercise 8.\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nStefanos Georganos et. al. (2019) “Geographical Random Forests: A Spatial Extension of the Random Forest Algorithm to Address Spatial Heterogeneity in Remote Sensing and Population Modelling”, Geocarto International, DOI: 10.1080/10106049.2019.1595177.\nGeorganos, S. and Kalogirou, S. (2022) “A Forest of Forests: A Spatially Weighted and Computationally Efficient Formulation of Geographical Random Forests”. ISPRS, International Journal of Geo-Information, 2022, 11, 471. https://www.mdpi.com/2220-9964/11/9/471\n\n\n\nReferences\n\nGeorge Grekousis et. al. (2022) “Ranking the importance of demographic, socioeconomic, and underlying health factors on US COVID-19 deaths: A geographical random forest approach”, Health and Place, vol. 74, pp. 1-12.\nYaowen Luo, Jianguo Yan & Stephen McClure. (2020) “Distribution of the environmental and socioeconomic risk factors on COVID-19 death rate across continental USA: a spatial nonlinear analysis”, Environmental Science and Pollution Research, 28:6587–6599.\nEun-Hee Koh, Eunhee Lee, & Kang-Kun Lee (2020) “Application of geographically weighted regression models to predict spatial characteristics of nitrate contamination: Implications for an effective groundwater management strategy”, Journal of Environmental Management. Vol. 268\n\n\n\nAll About R\n\nSpatialML"
  },
  {
    "objectID": "outline/Lesson06_outline.html#content",
    "href": "outline/Lesson06_outline.html#content",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Content",
    "text": "Content\n\nBasic concepts of geographic segmentation\nConventional cluster analysis techniques\nApproaches for clustering geographically referenced data\n\nHierarchical clustering with spatial constraints\nMinimum spanning trees\nClustGeo method"
  },
  {
    "objectID": "outline/Lesson06_outline.html#lesson-slides",
    "href": "outline/Lesson06_outline.html#lesson-slides",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Lesson slides",
    "text": "Lesson slides\n\nLesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis slides."
  },
  {
    "objectID": "outline/Lesson06_outline.html#hands-on-exercise",
    "href": "outline/Lesson06_outline.html#hands-on-exercise",
    "title": "Lesson 6: Geographic Segmentation with Spatially Constrained Cluster Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\n12 Geographical Segmentation with Spatially Constrained Clustering Techniques\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nAssuncao, R. M., Neves, M.C., Camara, G. and Costa Freitas, C.D. 2006. “Efficient Regionalization Techniques for Socio-Economic Geographical Units Using Minimum Spanning Trees”. International Journal of Geographical Information Science 20: 797-811. (Available in SMU digital library)\nChavent, M., Kuentz-Simonet, V., Labenne,A. and Saracco, J. 2018. “ClustGeo: an R package for hierarchical clustering with spatial constraints” Computational Statistics, 33: 1799-1822. (Available in SMU digital library)\n\n\n\nReferences\n\nRovan, J. and Sambt, J. (2003) “Socio-economic Differences Among Slovenian Municipalities: A Cluster Analysis Approach”, Developments in Applied Statistics, pp. 265-278.\n\nDemeter, T. and Bratucu, G. (2013) “Statistical Analysis Of The EU Countries from A Touristic Point of View”, Bulletin of the Transilvania University of Braşov, 6(55): 121-130.\nBrown, N.S. & Watson, P. (2012) “What can a comprehensive plan really tell us about a region?: A cluster analysis of county comprehensive plans in Idaho”, Western Economics Forum. Pp.22-37.\nJaya, I.G.M., Ruchjana, B.N., Andriyana, Y. & Agata, R (2019) “Clustering with spatial constraints: The case of diarrhea in Bandung city, Indonesia”\nde Souza, D. C. & Taconeli, C. A. (2022) “Spatial and non-spatial clustering algorithm in the analysis of Brazilian educational data”, Communications in Statistics: Case Studies, Data Analysis, and Applications. Vol. 8, No. 4, 588-606. (Available in SMU digital library)\n\n\n\nAll About R\n\nHierarchical Cluster Analysis.\nskater: A function from spdep package that implements a SKATER procedure for spatial clustering analysis.\nClustGeo: Hierarchical Clustering with Spatial Constraints\n\nIntroduction to Clustgeo"
  },
  {
    "objectID": "outline/Lesson04_outline.html",
    "href": "outline/Lesson04_outline.html",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "",
    "text": "Computing spatial weight is an essential step toward measuring the strength of the spatial relationships between objects. In this lesson, the basic concept of spatial weight will be introduced. This is followed by a discussion of methods to compute spatial weights."
  },
  {
    "objectID": "outline/Lesson04_outline.html#content",
    "href": "outline/Lesson04_outline.html#content",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "Content",
    "text": "Content\n\nTobler’s First law of Geography\nPrinciples of Spatial Autocorrelation\nConcepts of Spatial Proximity and Spatial Weights\n\nContiguity-Based Spatial Weights: Rook’s & Queen’s\nDistance-Band Spatial Weights: fixed and adaptive\n\nApplications of Spatial Weights\n\nSpatially lagged variables\nGeographically Weighted Summary Statistics\nSpatially lagged rates"
  },
  {
    "objectID": "outline/Lesson04_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson04_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 4 slides.\nHands-on Exercise 4\n\n\nSelf-reading Before Meet-up\nTo read before class:\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\n\nAlternatively\n\nChapter 9: Modelling Areal Data of Applied Spatial Data Analysis with R (2nd Edition). This book is available in smu digital library. Until section 9.3.1."
  },
  {
    "objectID": "outline/Lesson04_outline.html#references",
    "href": "outline/Lesson04_outline.html#references",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "References",
    "text": "References\n\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "outline/Lesson04_outline.html#all-about-r",
    "href": "outline/Lesson04_outline.html#all-about-r",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "All About R",
    "text": "All About R\n\nspdep package\n\ndnearneigh()\nknearneigh()"
  },
  {
    "objectID": "outline/Lesson02_outline.html",
    "href": "outline/Lesson02_outline.html",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern, or distribution, of a set of points on a surface. It can refer to the actual spatial or temporal location of these points or also include data from point sources. It is one of the most fundamental concepts in geography and spatial analysis. This lesson aims to share with you the basic concepts and methods of Spatial Point Pattern Analysis. You will also gain hands experience on using spatstat, an R package specially designed for Spatial Point Pattern Analysis."
  },
  {
    "objectID": "outline/Lesson02_outline.html#content",
    "href": "outline/Lesson02_outline.html#content",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nIntroducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nKernel Density Estimation (KDE)\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function"
  },
  {
    "objectID": "outline/Lesson02_outline.html#lesson-slides",
    "href": "outline/Lesson02_outline.html#lesson-slides",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 2 slides."
  },
  {
    "objectID": "outline/Lesson02_outline.html#hands-on-exercise",
    "href": "outline/Lesson02_outline.html#hands-on-exercise",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nChapter 4: 1st Order Spatial Point Patterns Analysis Methods\nChapter 5: 2nd Order Spatial Point Patterns Analysis Methods"
  },
  {
    "objectID": "outline/Lesson02_outline.html#core-readings",
    "href": "outline/Lesson02_outline.html#core-readings",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Core Readings",
    "text": "Core Readings\n\nYuan, Y., Qiang, Y., Bin Asad, K., and Chow, T. E. (2020). Point Pattern Analysis. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.13.\nYin, P. (2020). Kernels and Density Estimation. The Geographic Information Science & Technology Body of Knowledge (1st Quarter 2020 Edition), John P. Wilson (ed.). DOI: 10.22224/gistbok/2020.1.12\nChapter 7 Spatial Point Pattern Analysis of Roger S. Bivand, Edzer Pebesma and Virgilio Gómez-Rubio (2013) Applied Spatial Data Analysis with R (2nd Edition), Springer.\n\n\nEnrichment Resources\nProf. Luc Anselin on point pattern analysis (YouTube):\n\nPoint Pattern Analysis Concepts\nPoint Pattern Analysis: Clustered, Regular and Dispersed Patterns\nPoint Pattern Analysis: Nearest Neighbor Statistics\nPoint Pattern Analysis: Quadrat Counts\nPoint Pattern Analysis: F and J Functions\nPoint Pattern Analysis: K, L and Kd Functions"
  },
  {
    "objectID": "outline/Lesson02_outline.html#references",
    "href": "outline/Lesson02_outline.html#references",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC.\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4 • Ripley’s K-function.\n\n\nApplications\n\nNaveen Donthu and Roland T. Rust (1989) “Estimating Geographic Customer Densities Using Kernel Density Estimation”, Marketing Science, Vol. 8, No. 2, pp. 191-203.\nJoseph Wartman and Nicholas E. Malasavage (2010). “Spatial Analysis for Identifying Concentrations of Urban Damage” in Methods and Techniques in Urban Engineering, Armando Carlos de Pina Filho and Aloisio Carlos dePina (Ed.), ISBN: 978-953-307-096-4, InTech.\nGiuseppe Borruso and Andrea Porceddu (2009) “A Tale of Two Cities: Density Analysis of CBD on Two Midsize Urban Areas in Northeastern Italy” in Murgante, Beniamino; Borruso, Giuseppe & Lapucci, Alessandra (2009) Studies in Computational Intelligence, Geocomputation and Urban Planning, pp.37-56."
  },
  {
    "objectID": "outline/Lesson02_outline.html#all-about-r",
    "href": "outline/Lesson02_outline.html#all-about-r",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nspatstat at R Cran\n\nspatstat resource."
  },
  {
    "objectID": "lesson.html",
    "href": "lesson.html",
    "title": "Weekly Lesson Plan",
    "section": "",
    "text": "Week\nDate\nTopic\nStudent Assignment\n\n\n\n\n1\n30/8/2025\nIntroduction to Geospatial Analytics\n\n\n\n2\n6/9/2025\nSpatial Point Patterns Analysis (SPAA)\n\n\n\n3\n13/9/2025\nAdvanced Spatial Point Patterns Analysis\n\n\n\n4\n20/9/2025\nSpatial Weights and Applications\n\n\n\n5\n27/9/2025\nGlobal and Local Measures of Spatial Association\n\n\n\n6\n4/10/2025\nGeographic Segmentation\n\n\n\n\n11/10/2025\n\n\n\n\n7\n18/10/2025\nGeographically Weighted Explanatory Models\n\n\n\n8\n25/10/2025\nGeographically Weighted Predictive Models\n\n\n\n9\n1/11/2025\nModelling Geographic of Accessibility\n\n\n\n10\n8/11/2025\nSpatial Interaction Models"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#content",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#content",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Content",
    "text": "Content\n\nBasic Concepts of Geography of Accessibility\nAccessibility Models\n\nStewart Potential model\nReilly model\nHuff model"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#what-is-geography-of-accessibility",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#what-is-geography-of-accessibility",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "What is Geography of Accessibility?",
    "text": "What is Geography of Accessibility?\n\n\n\nAccessibility is the measure of the capacity of a location to be reached from, or to be reached by, different locations. Therefore, the capacity and the arrangement of transport infrastructure are key elements in the determination of accessibility."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#measuring-distances",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#measuring-distances",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Measuring Distances",
    "text": "Measuring Distances\n\nDifferent spatial and distance conceptualizations that are commonly employed when measuring and modelling accessibility."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#the-geographical-unit",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#the-geographical-unit",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "The Geographical Unit",
    "text": "The Geographical Unit\n\nThis issue of irregularly shaped polygons created arbitrarily (such as county boundaries or block groups that have been created from a political process)."
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#the-potential-model",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#the-potential-model",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "The Potential Model",
    "text": "The Potential Model\nThe classic model"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#two-step-floating-catchment-area-method-2sfca",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#two-step-floating-catchment-area-method-2sfca",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Two-step floating catchment area method (2SFCA)",
    "text": "Two-step floating catchment area method (2SFCA)\n\n\n\nA special case of a potential model for measuring spatial accessibility to primary social services and public facilities.\nIt was inspired by the spatial decomposition idea first proposed by Radke and Mu (2000).\n\n\nReference: Luo, W.; Wang, F. (2003b). “Measures of spatial accessibility to health care in a GIS environment: synthesis and a case study in the Chicago region”. Environment and Planning B: Planning and Design. 30 (6): 865–884.\n\n\nAn earlier version of 2SFCA"
  },
  {
    "objectID": "lesson/Lesson09/Lesson09-GeoAcc.html#spatial-accessibility-measure-sam",
    "href": "lesson/Lesson09/Lesson09-GeoAcc.html#spatial-accessibility-measure-sam",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Spatial Accessibility Measure (SAM)",
    "text": "Spatial Accessibility Measure (SAM)\nThe formula:\n\nwhere\n\nAai is the accessibility in ED i,\nnj is the capacity of the target facility j.\npi is the demand of this ED, and\ndij is the network distance between the EDi and each facility j.\n\n\nReference: Stamatis Kalogirou & Ronan Foley (2006) “Health, place and Hanly: Modelling accessibility to hospitals in Ireland”, Irish Geography, Volume 39(1), 2006, 52-68."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#content",
    "href": "lesson/Lesson07/Lesson07-gwr.html#content",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nIntroducing Regression Modelling\n\nSimple Linear Regression\nMultiple Linear Regression\n\nWhat is Spatial Non-stationary\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\n\nInterpreting and Visualising"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions",
    "href": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\n\nWhy some condominium units were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions-1",
    "href": "lesson/Lesson07/Lesson07-gwr.html#the-why-questions-1",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "The WHY Questions",
    "text": "The WHY Questions\nWhy condominium units located at the central part of Singapore were transacted at relatively higher prices than others?"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#what-is-regression-analysis",
    "href": "lesson/Lesson07/Lesson07-gwr.html#what-is-regression-analysis",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\nA set of statistical processes for explaining the relationships among variables.\nThe focus is on the relationship between a dependent variable (y) and one or more independent variables (x)\n\nDoes X affect Y? If so, how?\nWhat is the change in Y given a one unit change in X?\n\nEstimate outcomes based on the relationships modelled."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#assumptions-of-linear-regression-models",
    "href": "lesson/Lesson07/Lesson07-gwr.html#assumptions-of-linear-regression-models",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Assumptions of linear regression models",
    "text": "Assumptions of linear regression models\n\nLinearity assumption. The relationship between the dependent variable and independent variables is (approximately) linear.\nNormality assumption. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity).\nThe residuals are uncorrelated with each other.\n\nserial correlation, as with time series\n\n(Optional) The errors (residuals) are normally distributed and have a 0 population mean.]"
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#spatial-non-stationary",
    "href": "lesson/Lesson07/Lesson07-gwr.html#spatial-non-stationary",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Spatial Non-stationary",
    "text": "Spatial Non-stationary\n\nWhen applied to spatial data, as can be seen, it assumes a stationary spatial process.\n\nThe same stimulus provokes the same response in all parts of the study region.\nHighly untenable for spatial process."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#geographically-weighted-regression-gwr",
    "href": "lesson/Lesson07/Lesson07-gwr.html#geographically-weighted-regression-gwr",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Geographically Weighted Regression (GWR)",
    "text": "Geographically Weighted Regression (GWR)\n\nLocal statistical technique to analyze spatial variations in relationships.\nSpatial non-stationarity is assumed and will be tested.\nBased on the “First Law of Geography”: everything is related with everything else, but closer things are more related."
  },
  {
    "objectID": "lesson/Lesson07/Lesson07-gwr.html#references",
    "href": "lesson/Lesson07/Lesson07-gwr.html#references",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "References",
    "text": "References\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) [“Some Notes on Parametric Significance Tests for Geographically Weighted Regression”](https://onlinelibrary-wiley-com.libproxy.smu.edu.sg/doi/abs/10.1111/0022-4146.00146. Journal of Regional Science, 39(3), 497-524.\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#content",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#content",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Content",
    "text": "Content\n\n\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\n\n\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)\n\nSpacetime and spacetime cubes\nMann-Kendall Test\nEHSA map"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#what-is-spatial-autocorrelation",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "What is Spatial Autocorrelation",
    "text": "What is Spatial Autocorrelation\n\nToble’s First Law of Geography\nSpatial Dependency\nSpatial Autocorrelation\n\nPositive autocorrelation\nNegative autocorrelation"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-spatial-autocorrelation",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Measures of Global Spatial Autocorrelation",
    "text": "Measures of Global Spatial Autocorrelation\n\nMoran’s I\nGeary’s c"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#measures-of-global-highlow-clustering-getis-ord-global-g",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Measures of Global High/Low Clustering: Getis-Ord Global G",
    "text": "Measures of Global High/Low Clustering: Getis-Ord Global G\n\n\n\nGetis-Ord Global G statistic is concerned with the overall concentration or lack of concentration in all pairs that are neighbours given the definition of neighbouring areas.\nThe variable must contain only positive values to be used.\n\n\n\nSource: Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-spatial-autocorrelation-statistics",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Spatial Autocorrelation Statistics",
    "text": "Local Spatial Autocorrelation Statistics\n\nA collection of geospatial statistical analysis methods for analysing the location related tendency (clusters or outliers) in the attributes of geographically referenced data (points or areas).\nCan be indecies decomposited from their global measures such as local Moran’s I, local Geary’s c, and Getis-Ord Gi*.\nThese spatial statistics are well suited for:\n\ndetecting clusters or outliers;\nidentifying hot spot or cold spot areas;\nassessing the assumptions of stationarity; and\nidentifying distances beyond which no discernible association obtains."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-indicator-of-spatial-association-lisa",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Indicator of Spatial Association (LISA)",
    "text": "Local Indicator of Spatial Association (LISA)\n\nA subset of localised geospatial statistics methods.\nAny spatial statistics that satisfies the following two requirements (Anselin, L. 1995):\n\nthe LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation;\nthe sum of LISAs for all observations is proportional to a global indicator of spatial association."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#local-morans-i",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#local-morans-i",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Local Moran’s I",
    "text": "Local Moran’s I\nGiven a geographically referenced attribute field, X the formula of local Moran’s I is:"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#detecting-hot-and-cold-spot-areas",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Detecting hot and cold spot areas",
    "text": "Detecting hot and cold spot areas\n\n\n\nGiven a set of geospatial features (i.e. points or polygons) and an analysis field, the spatial statistics tell you where features with either high (i.e. hot spots) or low values (cold spots) cluster spatially.\nThe spatial statistic used is called Getis-Ord Gi* statistic (pronounced G-i-star).\nGetis and Ord (1992) define the local G and G∗ statistics for region i (i=1,···,n) as:"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#fixed-weighting-scheme",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#fixed-weighting-scheme",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Fixed weighting scheme",
    "text": "Fixed weighting scheme\n\n\n\nThings to consider if fixed distance is used: - All features should have at least one neighbour.\n\nNo feature should have all other features as neighbours.\nEspecially if the values for the input field are skewed, you want features to have about eight neighbors each.\nMight produce large estimate variances where data are sparse, while mask subtle local variations where data are dense.\nIn extreme condition, fixed schemes might not be able to calibrate in local areas where data are too sparse to satisfy the calibration requirements (observations must be more than parameters)."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#adaptive-weighting-schemes",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Adaptive weighting schemes",
    "text": "Adaptive weighting schemes\n\n\n\nAdaptive schemes adjust itself according to the density of data\n\nShorter bandwidths where data are dense and longer where sparse.\nFinding nearest neighbors are one of the often used approaches.\n\n\n\n]"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#best-practice-guidelines",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#best-practice-guidelines",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Best practice guidelines",
    "text": "Best practice guidelines\n\nResults are only reliable if the input feature class contains at least 30 features.\nThe input field mst be in continuous data type such as a count, rate, or other numeric measurement, no categorical attribute field is allowed."
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#emerging-hot-spot-analysis-ehsa",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "Emerging Hot Spot Analysis (EHSA)",
    "text": "Emerging Hot Spot Analysis (EHSA)\n\nA technique that falls under exploratory spatial data analysis (ESDA).\nIt combines the traditional ESDA technique of hot spot analysis using the Getis-Ord Gi* statistic with the traditional time-series Mann-Kendall test for monotonic trends.\nThe goal of EHSA is to evaluate how hot and cold spots are changing over time. It helps us answer the questions: are they becoming increasingly hotter, are they cooling down, or are they staying the same?"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#in-colclusion",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#in-colclusion",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "In colclusion",
    "text": "In colclusion\nSpatial statistics methods are not a blackbox. Before performing the analysis, a geospatial analyst should consider the followings:\n\nWhat is the geographical question?\nWhat is the geospatial feature?\nWhat is the analysis field?\nWhich conceptualization of spatial relationships is appropriate?"
  },
  {
    "objectID": "lesson/Lesson05/Lesson05-GLSA.html#references",
    "href": "lesson/Lesson05/Lesson05-GLSA.html#references",
    "title": "Lesson 5: Global and Local Measures of Spatial Association",
    "section": "References",
    "text": "References\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127. - Moran’s I\nGeary’s c - Getis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#content",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#content",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nNetwork Constrained Kernel Density Estimation (NCKDE)\n\nBasic concepts of network constrained spatial point patterns\nNetwork Constrained KDE methods\nThe Three versions of Network Constrained KDE\n\nTemporal Network Kernel Density Estimation (TNKED)\n\nTemporal dimension\nSpatial dimension\nSpatio-temporal point patterns\n\n\n\nWelcome to Lesson 3: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-point-processes",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-point-processes",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Point Processes",
    "text": "Network Constrained Point Processes\nMany real world point event are not randomly distributed. Their distribution, on the other hand, are constrained by network such as roads, rivers, and fault lines just to name a few of them.\n\n\nRoad traffic accidents within Bangkok City. \n\nLocation of banks at Central, Hong Kong"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Kernel Density Estimation (NKDE)",
    "text": "Network Constrained Kernel Density Estimation (NKDE)\nNetwork Kernel Density Estimation, in short, NKDE is a geospatial analytics methods specially designed to computer intensity of spatial point event either located along or occurred along linear networks.\n\n\nMathematically, NKDE can be defined as: \nwhere K must be a probability density function and verifies the two following conditions:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde-1",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#network-constrained-kernel-density-estimation-nkde-1",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Network Constrained Kernel Density Estimation (NKDE)",
    "text": "Network Constrained Kernel Density Estimation (NKDE)\nPlanar KDE versus NKDE\n\n\nFigure on the right shows the basic differences between the Planar KDE and Network KDE for the same spatial point event data. To estimate th density values at a focal point X, the planar KDE treats the whole 2-D space as the context and finds four point events (black circle) within a search bandwidth (i.e. h), whereas the Network KDE only finds two point events within the same bandwidth in the network space based on network distance."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\nSimple method\n\n\nThe first method was proposed by Xie and Yan (2008). Considering the planar KDE, they defined the NKDE with the following formula:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThis method remains useful for two reasons:\n\nFor quick data visualization. With big datasets, it might be useful to use this simple method to do a primary investigation.\nIn a purely geographical view, this method is intuitive. In the case of crime analysis for example, one could argue that the strength of an event should not be affected by intersections on the network. In that case, the kernel function is seen as a distance decaying function."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-1",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-1",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\nDiscontinuous method\n\n\nThis method is introduced by Okabe and Sugihara (2012). The discontinuous NKDE is easily presented by a figure:\n Note that the density of the kernel function is equally divided at intersections.\n\n\n\n\n\n\n\nWarning\n\n\nAs one can see, the values of the NKDE are split at intersections to avoid the multiplication of the mass observed in the simple version. However, this creates a discontinuous NKDE, which is counter-intuitive. It leads to sharp differences between density values in the network, and could be problematic in networks with many intersections."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-2",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#the-three-version-of-nkde-2",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "The Three version of NKDE",
    "text": "The Three version of NKDE\nContinue method\n\n\nThe method merges the best of the two worlds: it adjusts the values of the NKDE at intersections to ensure that it integrates to 1 on its domain, and applies a backward correction to force the density values to be continuous.\nThis process is accomplished by a recursive function. This function is more time consuming, so it might be necessary to stop it when the recursion is too deep. Considering that the kernel density is divided at each intersection, stopping the function at deep level 16 should give results almost identical to the true values.\n\n\nNote that there are three different equations to calculate the kernel density depending on the situation (here, q1, q2, q3).\n\n\n\n\n\n\nNote\n\n\nAs one can see, the values of the NKDE are continuous, and the density values close to the events have been adjusted. This leads to smoother results than the discontinuous method."
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#temporal-network-kernel-density-estimate",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#temporal-network-kernel-density-estimate",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Temporal Network Kernel Density Estimate",
    "text": "Temporal Network Kernel Density Estimate\n\n\nEvents recorded on a network often have a temporal dimension. In that context, one could estimate the density of events in both time and network spaces.\nThe spatio-temporal kernel is calculated as the product of the network kernel density and the time kernel density.\n\nFor a sample point at location l and time t, the Temporal Network Kernel Density Estimate (TNKDE) is calculated as follows:"
  },
  {
    "objectID": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#references",
    "href": "lesson/Lesson03/Lesson03-Advanced_SPPA.html#references",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nNetwork Kernel Density Estimate\nTemporal Network Kernel Density Estimate"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#content",
    "href": "lesson/Lesson01/Lesson01-tmap.html#content",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Content",
    "text": "Content\n\nIntroducing maps\nTypology of maps\n\nReference maps\nThematic maps\n\nProportional Symbol Map\nChoropleth Mapping\nIntroduction to tmap Methods\n\n\nThis lesson consists of two parts. First, I will share with you the concepts and design principles of choropleth maps. Next, I will introduce you to tmap, an R package specially designed for thematic mapping based on Layered Grammar of Graphics"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#what-is-a-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#what-is-a-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "What is a Map?",
    "text": "What is a Map?\nA model of real world depict by a collection of cartographic symbols or/and visual abstraction."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#typology-of-maps",
    "href": "lesson/Lesson01/Lesson01-tmap.html#typology-of-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Typology of Maps",
    "text": "Typology of Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#thematic-mapping-principles-and-methods",
    "href": "lesson/Lesson01/Lesson01-tmap.html#thematic-mapping-principles-and-methods",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Thematic Mapping: Principles and Methods",
    "text": "Thematic Mapping: Principles and Methods\n\nDisplaying\n\nQualitative data\nQuantitative data\n\nChoosing -Appropriate classification method for displaying data\n\nAppropriate number of classes\n\nTechniques in data analysis\n\nUsing the classification histogram\nNormalizing data"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-maps",
    "href": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Maps",
    "text": "Qualitative Thematic Maps\nVisual Variables and Cartographic Symbols\n\n\n\nQualitative visual variables are used for nominal scale data.\nThe goal of qualitative visual variables is to show how entities differ from each other.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right for examples of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nPoint symbol map\n\n\n\nDifferent point symbols are used to represent school types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map-1",
    "href": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nLine symbol map\n\n\n\nA road map is an example of a thematic map. It shows the road network of an area. In this map, lines with different colour intensity and tickness are used to differentiate hierarchy of roads."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map-2",
    "href": "lesson/Lesson01/Lesson01-tmap.html#qualitative-thematic-map-2",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nArea map\n\n\n\nLand use map below is a good example of a discrete thematic map. In this map, different colours are use to represent different land use types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#quantitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#quantitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Quantitative Thematic Map",
    "text": "Quantitative Thematic Map\nVisual Variables and Cartographic Symbols\n\n\n\nQuantitative visual variables are used to display ordinal, interval or ratio scale data.\n\nThe goal of the quantitative visual variable is to show relative magnitude or order between entities.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right shows of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#proportional-symbol-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#proportional-symbol-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Proportional Symbol Map",
    "text": "Proportional Symbol Map\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map.\n\n\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map. For example, the proportional maps above use circle with different sizes to represent millions of people. There are two types of point features that are typically depicted with proportional symbols: features for which the data represents a geographic position directly (e.g., gallons of oil from individual oil wells), and features that are geographic areas to which data are aggregated and the data magnitudes are assigned to a representative point within the area (e.g., the geographic centroid of a state as in the examples above). In either case, the area of the symbol is scaled to represent the data magnitude, sometimes with a bit of exaggeration to adjust for a general tendency of human vision to underestimate differences in area. A variant on this direct data-to-symbol scaling groups values into categories first, then scales the symbol to represent the mean for the category, assigning a symbol to each place to represent the category range that the mean for the place falls within"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#dot-density-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#dot-density-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Dot Density Map",
    "text": "Dot Density Map\nA dot-density map is a type of thematic map that uses dots or other symbols on the map to show the values of one or more numeric data fields. Each dot on a dot-density map represents some amount of data.\n\n\nOne dot represent 100 households.\n\n\nReference: Dot distribution map at wiki and Dot Density Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#choropleth-map",
    "href": "lesson/Lesson01/Lesson01-tmap.html#choropleth-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Choropleth Map",
    "text": "Choropleth Map\nA choropleth map is a type of thematic map in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population or per-capita income."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#data-classification",
    "href": "lesson/Lesson01/Lesson01-tmap.html#data-classification",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Data classification",
    "text": "Data classification\n\n\nNot sure how many classes to use? Have a look at the distribution of your data in a histogram (see examples below): Are there obvious clusters within your data? Are there large gaps in your data range that suggest nice compact data classes? If so, pick that number of classes and place those class breaks around those clusters."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#colour-scheme",
    "href": "lesson/Lesson01/Lesson01-tmap.html#colour-scheme",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Colour scheme",
    "text": "Colour scheme\nColorBrewer"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#mapping-packages-in-r",
    "href": "lesson/Lesson01/Lesson01-tmap.html#mapping-packages-in-r",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Mapping packages in R",
    "text": "Mapping packages in R\n\n\nSelected popular mapping packages\nCRAN Task View: Analysis of Spatial Data\n\ntmap\nmapsf\nleaflet\nggplot2. Read Chapter 6: Maps of ‘ggplot2: Elegant Graphics for Data Analysis’ for more detail.\nggmap\nquickmapr\nmapview\n\n\nOther packages\n\nRColorBrewer\nclassInt"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#introducing-tmap",
    "href": "lesson/Lesson01/Lesson01-tmap.html#introducing-tmap",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Introducing tmap",
    "text": "Introducing tmap\n\n\n\ntmap is a R package specially designed for creating thematic maps using the pricinples of the Grammar of Graphics.\nIt offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and proportional symbol maps.\nIt supports two modes: tm_plot() for static maps and tm_view() for interactive maps.\nIt provides shiny integration with renderTmap(), tmapOutput(), tmapProxy() and tm_remove_layer()."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements",
    "href": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\ntm_shape()\n\nThe first element to start with is tm_shape(), which specifies the shape object."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-1",
    "href": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nNext, one, or a combination of the following drawing layers should be specified:\n\n\n\nLinks to tm_polygons(), tm_symbols(), tm_lines(), tm_raster() and tm_text()"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-2",
    "href": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-2",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nBase layers\n\nEach of these functions specifies the geometry, mapping, and scaling component of the LGTM.\nAn aesthetic can take a constant value, a data variable name, or a vector consisting of values or variable names.\nIf a data variable is provided, the scale is automatically configured according to the values of this variable, but can be adjusted with several arguments. For instance, the main scaling arguments for a color aesthetic are color palette, the preferred number of classes, and a style to create classes.\nAlso, for each aesthetic, except for the text labels, a legend is automatically created.\nIf a vector of variable names is provided, small multiples are created, which will be explained further below."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-3",
    "href": "lesson/Lesson01/Lesson01-tmap.html#tmap-elements-3",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "tmap elements",
    "text": "tmap elements\nDerived layers\n\n\nEach aesthetic can take a constant value or a data variable name. For instance, tm_fill(col=\"blue\") colors all polygons blue, while tm_fill(col=\"var1\"), where “var1” is the name of a data variable in the shape object, creates a choropleth.\n\nThe supported derived layers are as follows:"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#data-classification-methods-of-tmap",
    "href": "lesson/Lesson01/Lesson01-tmap.html#data-classification-methods-of-tmap",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Data classification methods of tmap",
    "text": "Data classification methods of tmap\n\n\nMost choropleth maps employ some method of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\n\n\n\n\n\n\nNote\n\n\n\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\nThe choropleth map on the right shows a quantile data classification with 8 classes are used.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 8,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#colour-scheme-1",
    "href": "lesson/Lesson01/Lesson01-tmap.html#colour-scheme-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Colour Scheme",
    "text": "Colour Scheme\n\n\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n\n\n\n\n\nNote\n\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill().\nNotice that the word blues is used instead of blue and the alphabet b is in uppercase.\n\n\n\n\n\n\n\ntm_shape(mpszpop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#reference",
    "href": "lesson/Lesson01/Lesson01-tmap.html#reference",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "Reference",
    "text": "Reference\nPrinciples, Concepts and Methods of Choropleth Maps Design\nCore Reading\n\nChoropleth Maps\nThe Basics of Data Classification\n\nAdditional Readings\n\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps\nChoropleth Mapping with Exploratory Data Analysis"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-tmap.html#references",
    "href": "lesson/Lesson01/Lesson01-tmap.html#references",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation and tmap Methods",
    "section": "References",
    "text": "References\nAll About tmap package\n\ntmap: Thematic Maps in R\nDevelopment site\ntmap Reference\ntmap: get started!\ntmap: version changes\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#content",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#content",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Content",
    "text": "Content\n\nIntroducing maps\nTypology of maps\n\nReference maps\nThematic maps\n\nProportional Symbol Map\nChoropleth Mapping\nIntroduction to tmap Methods\n\n\nThis lesson consists of two parts. First, I will share with you the concepts and design principles of choropleth maps. Next, I will introduce you to tmap, an R package specially designed for thematic mapping based on Layered Grammar of Graphics"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#what-is-a-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#what-is-a-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "What is a Map?",
    "text": "What is a Map?\nA model of real world depict by a collection of cartographic symbols or/and visual abstraction."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#typology-of-maps",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#typology-of-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Typology of Maps",
    "text": "Typology of Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#thematic-mapping-principles-and-methods",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#thematic-mapping-principles-and-methods",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Thematic Mapping: Principles and Methods",
    "text": "Thematic Mapping: Principles and Methods\n\nDisplaying\n\nQualitative data\nQuantitative data\n\nChoosing -Appropriate classification method for displaying data\n\nAppropriate number of classes\n\nTechniques in data analysis\n\nUsing the classification histogram\nNormalizing data"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-maps",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-maps",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Maps",
    "text": "Qualitative Thematic Maps\nVisual Variables and Cartographic Symbols\n\n\n\nQualitative visual variables are used for nominal scale data.\nThe goal of qualitative visual variables is to show how entities differ from each other.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right for examples of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nPoint symbol map\n\n\n\nDifferent point symbols are used to represent school types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-1",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-1",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nLine symbol map\n\n\n\nA road map is an example of a thematic map. It shows the road network of an area. In this map, lines with different colour intensity and tickness are used to differentiate hierarchy of roads."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-2",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#qualitative-thematic-map-2",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Qualitative Thematic Map",
    "text": "Qualitative Thematic Map\nArea map\n\n\n\nLand use map below is a good example of a discrete thematic map. In this map, different colours are use to represent different land use types."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#quantitative-thematic-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#quantitative-thematic-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Quantitative Thematic Map",
    "text": "Quantitative Thematic Map\nVisual Variables and Cartographic Symbols\n\n\n\nQuantitative visual variables are used to display ordinal, interval or ratio scale data.\n\nThe goal of the quantitative visual variable is to show relative magnitude or order between entities.\nThe visual variables that do a good job of showing ordinal differences are: colour value, colour saturation, size and texture/grain.\n\nFigure on the right shows of these four ordinal visual variables used each in point, linear and areal symbols."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#proportional-symbol-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#proportional-symbol-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Proportional Symbol Map",
    "text": "Proportional Symbol Map\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map.\n\n\n\nThe proportional symbol technique uses symbols of different sizes to represent data associated with different areas or locations within the map. For example, the proportional maps above use circle with different sizes to represent millions of people. There are two types of point features that are typically depicted with proportional symbols: features for which the data represents a geographic position directly (e.g., gallons of oil from individual oil wells), and features that are geographic areas to which data are aggregated and the data magnitudes are assigned to a representative point within the area (e.g., the geographic centroid of a state as in the examples above). In either case, the area of the symbol is scaled to represent the data magnitude, sometimes with a bit of exaggeration to adjust for a general tendency of human vision to underestimate differences in area. A variant on this direct data-to-symbol scaling groups values into categories first, then scales the symbol to represent the mean for the category, assigning a symbol to each place to represent the category range that the mean for the place falls within"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#dot-density-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#dot-density-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Dot Density Map",
    "text": "Dot Density Map\nA dot-density map is a type of thematic map that uses dots or other symbols on the map to show the values of one or more numeric data fields. Each dot on a dot-density map represents some amount of data.\n\n\nOne dot represent 100 households.\n\n\nReference: Dot distribution map at wiki and Dot Density Maps"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#choropleth-map",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#choropleth-map",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Choropleth Map",
    "text": "Choropleth Map\nA choropleth map is a type of thematic map in which areas are shaded or patterned in proportion to a statistical variable that represents an aggregate summary of a geographic characteristic within each area, such as population or per-capita income."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#data-classification",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Data classification",
    "text": "Data classification\n\n\nNot sure how many classes to use? Have a look at the distribution of your data in a histogram (see examples below): Are there obvious clusters within your data? Are there large gaps in your data range that suggest nice compact data classes? If so, pick that number of classes and place those class breaks around those clusters."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#colour-scheme",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Colour scheme",
    "text": "Colour scheme\nColorBrewer"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#mapping-packages-in-r",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#mapping-packages-in-r",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Mapping packages in R",
    "text": "Mapping packages in R\n\n\nSelected popular mapping packages\nCRAN Task View: Analysis of Spatial Data\n\ntmap\nmapsf\nleaflet\nggplot2. Read Chapter 6: Maps of ‘ggplot2: Elegant Graphics for Data Analysis’ for more detail.\nggmap\nquickmapr\nmapview\n\n\nOther packages\n\nRColorBrewer\nclassInt"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#introducing-tmap",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#introducing-tmap",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Introducing tmap",
    "text": "Introducing tmap\n\n\n\ntmap is a R package specially designed for creating thematic maps using the pricinples of the Grammar of Graphics.\nIt offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and proportional symbol maps.\nIt supports two modes: tm_plot() for static maps and tm_view() for interactive maps.\nIt provides shiny integration with renderTmap(), tmapOutput(), tmapProxy() and tm_remove_layer()."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#reference",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#reference",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "Reference",
    "text": "Reference\nPrinciples, Concepts and Methods of Choropleth Maps Design\nCore Reading\n\nChoropleth Maps\nThe Basics of Data Classification\n\nAdditional Readings\n\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps\nChoropleth Mapping with Exploratory Data Analysis"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-GeoVis.html#references",
    "href": "lesson/Lesson01/Lesson01-GeoVis.html#references",
    "title": "Lesson 1: Fundamental of Geospatial Data Visualisation",
    "section": "References",
    "text": "References\nAll About tmap package\n\ntmap: Thematic Maps in R\ntmap home page\nSpatial Data Visualization with tmap: A Practical Guide to Thematic Mapping in R\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "KNIME.html",
    "href": "KNIME.html",
    "title": "KNIME Resources",
    "section": "",
    "text": "This page provides links to a selected resources to learn KNIME and it’s extension."
  },
  {
    "objectID": "KNIME.html#knime",
    "href": "KNIME.html#knime",
    "title": "KNIME Resources",
    "section": "KNIME",
    "text": "KNIME\n\nGetting Started Guide"
  },
  {
    "objectID": "KNIME.html#knime-courses",
    "href": "KNIME.html#knime-courses",
    "title": "KNIME Resources",
    "section": "KNIME Courses",
    "text": "KNIME Courses\n\nL1-DS KNIME Analytics Platform for Data Scientists: Basics\nL1-DW KNIME Analytics Platform for Data Wranglers: Basics\nL2-DS KNIME Analytics Platform for Data Scientists: Advanced\nL2-DW KNIME Analytics Platform for Data Wranglers: Advanced"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#learning-outcome",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#learning-outcome",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Learning Outcome",
    "text": "Learning Outcome\nBy the end of this hands-on exercise, you will be able to:\n\nPreparing data downloaded from REALIS portal for geocoding,\nGeocoding by using SLA OneMap API,\nConverting the geocoded transaction data into sf point feature data.frame, and\nWrangling the sf point features to avoid overlapping point features."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#loading-the-r-package",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#loading-the-r-package",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Loading the R package",
    "text": "Loading the R package\n\n\npacman::p_load(tidyverse, sf, tmap, httr, performance)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#importing-data",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#importing-data",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Importing data",
    "text": "Importing data\nThe code chunk below imports multiple csv files in a specified folder and append them into a single tibble data frame.\n\n\nfolder_path &lt;- \"data/aspatial\"\nfile_list &lt;- list.files(path = folder_path, \n                        pattern = \"^realis.*\\\\.csv$\", \n                        full.names = TRUE)\n\nrealis_data &lt;- file_list %&gt;%\n  map_dfr(read_csv)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#wrangling-data",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#wrangling-data",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Wrangling data",
    "text": "Wrangling data\n\nThe taskThe code\n\n\nWrite a code chunk to perform the followings: - converting values in Sale Date field from character to numerical date format, and - extracting resale and condominium transaction records.\n\n\n\n\ncondo_resale &lt;- realis_data %&gt;%\n  mutate(`Sale Date` = dmy(`Sale Date`)) %&gt;%\n  filter(`Type of Sale` == \"Resale\" &\n           `Property Type` == \"Condominium\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#geocoding",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#geocoding",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Geocoding",
    "text": "Geocoding\n\nPreparing dataGeocoding\n\n\n\n\npostcode &lt;- unique(condo_resale$`Postal Code`)\n\n\n\n\n\n\nurl &lt;- \"https://onemap.gov.sg/api/common/elastic/search\"\nfound &lt;- data.frame()\nnot_found &lt;- data.frame()\n\nfor (postcode in postcode){\n  query &lt;- list('searchVal'=postcode, 'returnGeom'='Y', \n                'getAddrDetails'='Y', 'pageNum'='1')\n  res &lt;- GET(url, query=query)\n  if ((content(res)$found)!=0){\n    found &lt;- rbind(found, data.frame(content(res))[4:13])\n  } else {not_found = data.frame(postcode)\n  }\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#tidying-field-names",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#tidying-field-names",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Tidying field names",
    "text": "Tidying field names\n\n\nfound &lt;- found %&gt;%\n  select(c(6:8)) %&gt;%\n  rename(POSTAL = `results.POSTAL`,\n         XCOORD = `results.X`,\n         YCOORD = `results.Y`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#converting-to-point-feature-data-frame",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#converting-to-point-feature-data-frame",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Converting to Point Feature Data Frame",
    "text": "Converting to Point Feature Data Frame\n\nThe tasksJoining tablesConvering to sf\n\n\n\nWrite a code chunk to join condo_resale and found. Name the output condo_resale_geocoded.\nWrite a code chunk to convert condo_resale_geocoded from tibble data frame to sf point feature data frame.\n\n\n\n\n\ncondo_resale_geocoded = left_join(\n  condo_resale, found, \n  by = c('Postal Code' = 'POSTAL'))\n\n\n\n\n\n\ncondo_resale_sf &lt;- st_as_sf(condo_resale_geocoded, \n                            coords = c(\"XCOORD\",\n                                       \"YCOORD\"),\n                            crs=3414)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#cleaning-spatial-data",
    "href": "In-class_Ex/In-class_Ex10/In-class_Ex10.html#cleaning-spatial-data",
    "title": "In-class Exercise 10: Working with Open Government Data",
    "section": "Cleaning Spatial Data",
    "text": "Cleaning Spatial Data\n\nChecking for overlapping point featuresSpatial jittering\n\n\nThe code chunk below is used to check if there are overlapping point features.\n\n\noverlapping_points &lt;- condo_resale_sf %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)\n\n\n\n\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5m to avoid overlapping point features.\n\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  st_jitter(amount = 2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#getting-started",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#getting-started",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Getting Started",
    "text": "Getting Started\nInstalling and Loading R packages\n\n\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse,\n               knitr, kableExtra)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#preparing-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#preparing-data",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Preparing Data",
    "text": "Preparing Data\n\nData importData SamplingChecking of overlapping pointSpatial jitter\n\n\n\n\nmdata &lt;- read_rds(\"data/rds/mdata.rds\")\n\n\n\n\nCalibrating predictive models are computational intensive, especially random forest method is used. For quick prototyping, a 10% sample will be selected at random from the data by using the code chunk below.\n\n\nset.seed(1234)\nHDB_sample &lt;- mdata %&gt;%\n  sample_n(1500)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nWhen using GWmodel to calibrate explanatory or predictive models, it is very important to ensure that there are no overlapping point features\n\n\n\nThe code chunk below is used to check if there are overlapping point features.\n\n\noverlapping_points &lt;- HDB_sample %&gt;%\n  mutate(overlap = lengths(st_equals(., .)) &gt; 1)\n\n\n\n\nIn the code code chunk below, st_jitter() of sf package is used to move the point features by 5m to avoid overlapping point features.\n\n\nHDB_sample &lt;- HDB_sample %&gt;%\n  st_jitter(amount = 5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-sampling-1",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#data-sampling-1",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Data Sampling",
    "text": "Data Sampling\nThe entire data are split into training and test data sets with 65% and 35% respectively by using initial_split() of rsample package. rsample is one of the package of tigymodels.\n\n\nset.seed(1234)\nresale_split &lt;- initial_split(HDB_sample, \n                              prop = 6.67/10,)\ntrain_data &lt;- training(resale_split)\ntest_data &lt;- testing(resale_split)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#building-a-non-spatial-multiple-linear-regression",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#building-a-non-spatial-multiple-linear-regression",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Building a non-spatial multiple linear regression",
    "text": "Building a non-spatial multiple linear regression\n\nThe reportThe code chunk\n\n\n\n\n\n\n\nprice_mlr &lt;- lm(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                data=train_data)\nolsrr::ols_regress(price_mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-gwr",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-gwr",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling with gwr",
    "text": "Predictive Modelling with gwr\nComputing adaptive bandwidth\n\nThe codeThe output\n\n\n\n\nbw_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=train_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\n\n\n\n\nbw_adaptive"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-mlr",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-with-mlr",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling with MLR",
    "text": "Predictive Modelling with MLR\nPredicting with test data\n\nTest data bwPredicting\n\n\n\n\ngwr_bw_test_adaptive &lt;- bw.gwr(resale_price ~ floor_area_sqm +\n                  storey_order + remaining_lease_mths +\n                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +\n                  PROX_MRT + PROX_PARK + PROX_MALL + \n                  PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                  WITHIN_1KM_PRISCH,\n                  data=test_data,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n\n\n\n\n\n\ngwr_pred &lt;- gwr.predict(formula = resale_price ~\n                          floor_area_sqm + storey_order +\n                          remaining_lease_mths + PROX_CBD + \n                          PROX_ELDERLYCARE + PROX_HAWKER + \n                          PROX_MRT + PROX_PARK + PROX_MALL + \n                          PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                          WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n                          WITHIN_1KM_PRISCH, \n                        data=train_data, \n                        predictdata = test_data, \n                        bw=bw_adaptive, \n                        kernel = 'gaussian', \n                        adaptive=TRUE, \n                        longlat = FALSE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-rf-method",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-rf-method",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling: RF method",
    "text": "Predictive Modelling: RF method\n\nData preparationCalibrating RF modelModel output\n\n\nFirstly, code chunk below is used to extract the coordinates of training and test data sets\n\n\ncoords &lt;- st_coordinates(HDB_sample)\ncoords_train &lt;- st_coordinates(train_data)\ncoords_test &lt;- st_coordinates(test_data)\n\n\nNext, code chunk below is used to drop the geometry column of both training and test data sets.\n\n\ntrain_data_nogeom &lt;- train_data %&gt;%\n  st_drop_geometry()\n\n\n\n\n\n\nset.seed(1234)\nrf &lt;- ranger(resale_price ~ floor_area_sqm + storey_order + \n               remaining_lease_mths + PROX_CBD + PROX_ELDERLYCARE + \n               PROX_HAWKER + PROX_MRT + PROX_PARK + PROX_MALL + \n               PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n               WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n               WITHIN_1KM_PRISCH,\n             data=train_data_nogeom)\n\n\n\n\n\n\nrf"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-spatialml-method",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predictive-modelling-spatialml-method",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predictive Modelling: SpatialML method",
    "text": "Predictive Modelling: SpatialML method\n\nDetermining bandwidthCalibrating with grf\n\n\n\nset.seed(1234)\ngwRF_bw &lt;- grf.bw(formula = resale_price ~ floor_area_sqm + \n                       storey_order + remaining_lease_mths + \n                       PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n                       PROX_MRT + PROX_PARK + PROX_MALL + \n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dataset=train_data_nogeom, \n                     kernel=\"adaptive\",\n                     coords=coords_train)\n\n\n\n\n\nset.seed(1234)\ngwRF_adaptive &lt;- grf(formula = resale_price ~ floor_area_sqm + \n                       storey_order + remaining_lease_mths + \n                       PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n                       PROX_MRT + PROX_PARK + PROX_MALL + \n                       PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +\n                       WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +\n                       WITHIN_1KM_PRISCH,\n                     dframe=train_data_nogeom, \n                     bw=55,\n                     kernel=\"adaptive\",\n                     coords=coords_train)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predicting-by-using-the-test-data",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#predicting-by-using-the-test-data",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Predicting by using the test data",
    "text": "Predicting by using the test data\n\nPreparing the test dataPredicting with the test dataCreating DF\n\n\n\n\ntest_data_nogeom &lt;- cbind(\n  test_data, coords_test) %&gt;%\n  st_drop_geometry()\n\n\n\n\nIn the code chunk below, predict.grf() of spatialML for predicting re-sale prices in the test data set (i.e. test_data_nogeom)\n\n\ngwRF_pred &lt;- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n\n\n\n\nNext, the code chunk below is used to convert the output from predict.grf() into a data.frame.\n\n\nGRF_pred_df &lt;- as.data.frame(gwRF_pred)\n\n\nThen, cbind() is used to append fields in GRF_pred_df data.frame onto test_data.\n\n\ntest_data_pred &lt;- cbind(test_data, \n                        GRF_pred_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-the-predicted-values",
    "href": "In-class_Ex/In-class_Ex08/In-class_Ex08.html#visualising-the-predicted-values",
    "title": "In-class Exercise 8: Supplement to Hands-on Exercise 8",
    "section": "Visualising the predicted values",
    "text": "Visualising the predicted values\n\nThe plotThe code chunk\n\n\n\n\n\n\n\nggplot(data = test_data_pred,\n       aes(x = GRF_pred,\n           y = resale_price)) +\n  geom_point()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#content",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#content",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Content",
    "text": "Content\n\nIntroducing sfdep.\n\nsfdep creates an sf and tidyverse friendly interface to the package as well as introduces new functionality that is not present in spdep.\nsfdep utilizes list columns extensively to make this interface possible.”"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#getting-started",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Getting started",
    "text": "Getting started\nInstalling and Loading the R Packages\nFour R packages will be used for this in-class exercise, they are: sf, sfdep, tmap and tidyverse.\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, install and load sf, tmap, sfdep and tidyverse packages into R environment.\n\n\n\n\npacman::p_load(sf, sfdep, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#the-data",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#the-data",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format.\n\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan shapefile into R environment as an sf data frame.\n\n\n\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#importing-attribute-table",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#importing-attribute-table",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Importing Attribute Table",
    "text": "Importing Attribute Table\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, import Hunan_2012.csv into R environment as an tibble data frame.\n\n\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#combining-both-data-frame-by-using-left-join",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#combining-both-data-frame-by-using-left-join",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Combining both data frame by using left join",
    "text": "Combining both data frame by using left join\n\nDo It Yourself!The code\n\n\nUsing the steps you learned in previous lesson, combine the Hunan sf data frame and Hunan_2012 data frame. Ensure that the output is an sf data frame.\n\n\n\n\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\n\nNote\n\n\nFor the purpose of this exercise, we only retain column 1 to 4, column 7 and column 15. You should examine the output sf data.frame to learn know what are these fields.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn order to retain the geospatial properties, the left data frame must the sf data.frame (i.e. hunan)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#plotting-a-choropleth-map",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#plotting-a-choropleth-map",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Plotting a choropleth map",
    "text": "Plotting a choropleth map\n\nDo It Yourself!The plotThe code\n\n\nUsing the steps you learned in previous lesson, plot a choropleth map showing the distribution of GDPPC of Hunan Province.\n\n\n\n\n\n\n\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by county, Hunan Province\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#global-measures-of-spatial-association",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Global Measures of Spatial Association",
    "text": "Global Measures of Spatial Association\nStep 1: Deriving Queen’s contiguity weights: sfdep methods\n\n\n\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1) \n\n\n\n\nNotice that st_weights() provides tree arguments, they are:\n\nnb: A neighbor list object as created by st_neighbors().\nstyle: Default “W” for row standardized weights. This value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nallow_zero: If TRUE, assigns zero as lagged value to zone without neighbors."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#lisa-map",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#lisa-map",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "LISA map",
    "text": "LISA map\n\n\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low cluaters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Computing local Moran’s I",
    "text": "Computing local Moran’s I\nIn this section, you will learn how to compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package.\n\nThe codeThe output\n\n\n\n\nlisa &lt;- wm_q %&gt;% \n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\n\n\n\n\nThe output of local_moran() is a sf data.frame containing the columns ii, eii, var_ii, z_ii, p_ii, p_ii_sim, and p_folded_sim.\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations p_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations p_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= -p_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05-GLSA.html#hot-spot-and-cold-spot-area-analysis-hcsa",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation: sfdep methods",
    "section": "Hot Spot and Cold Spot Area Analysis (HCSA)",
    "text": "Hot Spot and Cold Spot Area Analysis (HCSA)\n\n\nHCSA uses spatial weights to identify locations of statistically significant hot spots and cold spots in an spatially weighted attribute that are in proximity to one another based on a calculated distance. The analysis groups features when similar high (hot) or low (cold) values are found in a cluster. The polygon features usually represent administration boundaries or a custom grid structure."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#overview",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "",
    "text": "Network constrained Spatial Point Patterns Analysis (NetSPAA) is a collection of spatial point patterns analysis methods special developed for analysing spatial point event occurs on or alongside network. The spatial point event can be locations of traffic accident or childcare centre for example. The network, on the other hand can be a road network or river network.\nIn this hands-on exercise, you are going to gain hands-on experience on using appropriate functions of spNetwork package:\n\nto derive network kernel density estimation (NKDE), and\nto perform network G-function and k-function analysis"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#the-data",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "The Data",
    "text": "The Data\nIn this study, we will analyse the spatial distribution of childcare centre in Punggol planning area. For the purpose of this study, two geospatial data sets will be used. They are:\n\nPunggol_St, a line features geospatial data which store the road network within Punggol Planning Area.\nPunggol_CC, a point feature geospatial data which store the location of childcare centres within Punggol Planning Area.\n\nBoth data sets are in ESRI shapefile format."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-launching-the-r-packages",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Installing and launching the R packages",
    "text": "Installing and launching the R packages\nIn this hands-on exercise, four R packages will be used, they are:\n\nspNetwork, which provides functions to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nsf package provides functions to manage, processing, and manipulate Simple Features, a formal geospatial data standard that specifies a storage and access model of spatial geometries such as points, lines, and polygons.\ntmap which provides functions for plotting cartographic quality static point patterns maps or interactive maps by using leaflet API.\n\nUse the code chunk below to install and launch the four R packages.\n\npacman::p_load(sf, spNetwork, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import-and-preparation",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\nThe code chunk below uses st_read() of sf package to important Punggol_St and Punggol_CC geospatial data sets into RStudio as sf data frames.\n\nnetwork &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_St\")\n\n\nchildcare &lt;- st_read(dsn=\"data/rawdata\",\n                     layer=\"Punggol_CC\")\n\nWe can examine the structure of the output simple features data tables in RStudio. Alternative, code chunk below can be used to print the content of network and childcare simple features objects by using the code chunk below.\n\nChildcareNetwork\n\n\n\nchildcare\n\n\n\n\nnetwork\n\n\n\n\nWhen I exploring spNetwork’s functions, it came to my attention that spNetwork is expecting the geospatial data contains complete CRS information."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#visualising-the-geospatial-data",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Visualising the Geospatial Data",
    "text": "Visualising the Geospatial Data\nBefore we jump into the analysis, it is a good practice to visualise the geospatial data. There are at least two ways to visualise the geospatial data. One way is by using plot() of Base R as shown in the code chunk below.\n\nplot(st_geometry(network))\nplot(childcare,add=T,col='red',pch = 19)\n\nTo visualise the geospatial data with high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare) + \n  tm_dots() + \n  tm_shape(network) +\n  tm_lines()\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-kde-nkde-analysis",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Network KDE (NKDE) Analysis",
    "text": "Network KDE (NKDE) Analysis\nIn this section, we will perform NKDE analysis by using appropriate functions provided in spNetwork package.\n\nPreparing the lixels objects\nBefore computing NKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\nlixels &lt;- lixelize_lines(network, \n                         700, \n                         mindist = 350)\n\nWhat can we learned from the code chunk above:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If NULL, then mindist = maxdist/10. Also note that the segments that are already shorter than the minimum distance are not modified\nNote: There is another function called lixelize_lines.mc() which provide multicore support.\n\n\nGenerating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\nsamples &lt;- lines_center(lixels) \n\n\ntmap_mode('view')\ntm_shape(lixels) + \n  tm_lines() + \ntm_shape(samples) +\n  tm_dots(size = 0.01)\ntmap_mode('plot')\n\nThe points are located at center of the line based on the length of the line.\n\n\nPerforming NKDE\nWe are ready to computer the NKDE by using the code chunk below.\n\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1, nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\nWhat can we learn from the code chunk above?\n\nkernel_name argument indicates that quartic kernel is used. Are possible kernel methods supported by spNetwork are: triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\nmethod argument indicates that simple method is used to calculate the NKDE. Currently, spNetwork support three popular methods, they are:\n\nmethod=“simple”. This first method was presented by Xie et al. (2008) and proposes an intuitive solution. The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\nmethod=“discontinuous”. The method is proposed by Okabe et al (2008), which equally “divides” the mass density of an event at intersections of lixels.\nmethod=“continuous”. If the discontinuous method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive. Okabe et al (2008) proposed another version of the kernel, that divide the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\nThe user guide of spNetwork package provide a comprehensive discussion of nkde(). You should read them at least once to have a basic understanding of the various parameters that can be used to calibrate the NKDE model.\n\nVisualising NKDE\nBefore we can visualise the NKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\nSince svy21 projection system is in meter, the computed density values are very small i.e. 0.0000005. The code chunk below is used to resale the density values from number of events per meter to number of events per kilometer.\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()\ntmap_mode('plot')\n\nThe interactive map above effectively reveals road segments (darker color) with relatively higher density of childcare centres than road segments with relatively lower density of childcare centres (lighter color)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-g--and-k-function-analysis",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Network Constrained G- and K-Function Analysis",
    "text": "Network Constrained G- and K-Function Analysis\nIn this section, we are going to perform complete spatial randomness (CSR) test by using kfunctions() of spNetwork package. The null hypothesis is defined as:\nHo: The observed spatial point events (i.e distribution of childcare centres) are uniformly distributed over a street network in Punggol Planning Area.\nThe CSR test is based on the assumption of the binomial point process which implies the hypothesis that the childcare centres are randomly and independently distributed over the street network.\nIf this hypothesis is rejected, we may infer that the distribution of childcare centres are spatially interacting and dependent on each other; as a result, they may form nonrandom patterns.\n\nkfun_childcare &lt;- kfunctions(network, \n                             childcare,\n                             start = 0, \n                             end = 1000, \n                             step = 50, \n                             width = 50, \n                             nsim = 50, \n                             resolution = 50,\n                             verbose = FALSE, \n                             conf_int = 0.05)\n\nWhat can we learn from the code chunk above?\nThere are ten arguments used in the code chunk above they are:\n\nlines: A SpatialLinesDataFrame with the sampling points. The geometries must be a SpatialLinesDataFrame (may crash if some geometries are invalid).\npoints: A SpatialPointsDataFrame representing the points on the network. These points will be snapped on the network.\nstart: A double, the start value for evaluating the k and g functions.\nend: A double, the last value for evaluating the k and g functions.\nstep: A double, the jump between two evaluations of the k and g function.\nwidth: The width of each donut for the g-function.\nnsim: An integer indicating the number of Monte Carlo simulations required. In the above example, 50 simulation was performed. Note: most of the time, more simulations are required for inference\nresolution: When simulating random points on the network, selecting a resolution will reduce greatly the calculation time. When resolution is null the random points can occur everywhere on the graph. If a value is specified, the edges are split according to this value and the random points are selected vertices on the new network.\nconf_int: A double indicating the width confidence interval (default = 0.05).\n\nFor the usage of other arguments, you should refer to the user guide of spNetwork package.\nThe output of kfunctions() is a list with the following values:\n\nplotkA, a ggplot2 object representing the values of the k-function\nplotgA, a ggplot2 object representing the values of the g-function\nvaluesA, a DataFrame with the values used to build the plots\n\nFor example, we can visualise the ggplot2 object of k-function by using the code chunk below.\n\nkfun_childcare$plotk\n\nThe blue line is the empirical network K-function of the childcare centres in Punggol planning area. The gray envelop represents the results of the 50 simulations in the interval 2.5% - 97.5%. Because the blue line between the distance of 250m-400m are below the gray area, we can infer that the childcare centres in Punggol planning area resemble regular pattern at the distance of 250m-400m."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#bus-stop",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#bus-stop",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "Bus Stop",
    "text": "Bus Stop\n\nbusstop &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nroad &lt;- st_read(dsn=\"data/rawdata\", \n                   layer=\"Punggol_Road\") %&gt;%\n  st_transform(crs = 3414)\n\n\nlixels &lt;- lixelize_lines(road, \n                         700, \n                         mindist = 350)\n\n\nsamples &lt;- lines_center(lixels) \n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"discontinuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(road, \n                  events = busstop,\n                  w = rep(1, nrow(busstop)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"continuous\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\n# rescaling to help the mapping\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(busstop)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndata(\"mtl_network\")\ndata(\"bike_accidents\")\n\n\ntm_shape(mtl_network) +\n  tm_lines(\"black\") +\ntm_shape(bike_accidents) +\n  tm_dots(\"red\",\n          size = 0.2)\n\n\nlixels &lt;- lixelize_lines(mtl_network,\n                         200,\n                         mindist = 50)\n\n\nsamples &lt;- lines_center(lixels)\n\n\ndensities &lt;- nkde(mtl_network, \n                  events = bike_accidents,\n                  w = rep(1,nrow(bike_accidents)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, div= \"bw\", \n                  method = \"discontinuous\", digits = 1, tol = 1,\n                  grid_shape = c(1,1), max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nsamples$density &lt;- densities * 1000\nlixels$density &lt;- densities*1000\n\n\ntmap_mode('plot')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(bike_accidents)+\n  tm_dots()\ntmap_mode('plot')\n\n\ndensities &lt;- nkde(mtl_network, \n                  events = bike_accidents,\n                  w = rep(1,nrow(bike_accidents)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, div= \"bw\", \n                  method = \"continuous\", digits = 1, tol = 1,\n                  grid_shape = c(1,1), max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nsamples$density &lt;- densities * 1000\nlixels$density &lt;- densities*1000\n\n\ntmap_mode('pplot')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(bike_accidents)+\n  tm_dots()\ntmap_mode('plot')\n\n\nConverting the Date field to a numeric field (counting days)\n\nbike_accidents$Time &lt;- as.POSIXct(bike_accidents$Date, format = \"%Y/%m/%d\")\nstart &lt;- as.POSIXct(\"2016/01/01\", format = \"%Y/%m/%d\")\nbike_accidents$Time &lt;- difftime(bike_accidents$Time, start, units = \"days\")\nbike_accidents$Time &lt;- as.numeric(bike_accidents$Time)\n\n\nmonths &lt;- as.character(1:12)\nmonths &lt;- ifelse(nchar(months)==1, paste0(\"0\", months), months)\nmonths_starts_labs &lt;- paste(\"2016/\",months,\"/01\", sep = \"\")\nmonths_starts_num &lt;- as.POSIXct(months_starts_labs, format = \"%Y/%m/%d\")\nmonths_starts_num &lt;- difftime(months_starts_num, start, units = \"days\")\nmonths_starts_num &lt;- as.numeric(months_starts_num)\nmonths_starts_labs &lt;- gsub(\"2016/\", \"\", months_starts_labs, fixed = TRUE)\n\n\nggplot(bike_accidents) + \n  geom_histogram(aes(x = Time), \n                 bins = 30, \n                 color = \"white\") + \n  scale_x_continuous(breaks = months_starts_num, \n                     labels = months_starts_labs)\n\n\nbike_accidents &lt;- subset(bike_accidents, bike_accidents$Time &gt;= 90)\n\n\nw &lt;- rep(1,nrow(bike_accidents))\nsamples &lt;- seq(0, max(bike_accidents$Time), 0.5)\n\ntime_kernel_values &lt;- data.frame(\n  bw_10 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 10, kernel_name = \"quartic\"),\n  bw_20 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 20, kernel_name = \"quartic\"),\n  bw_30 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 30, kernel_name = \"quartic\"),\n  bw_40 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 40, kernel_name = \"quartic\"),\n  bw_50 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 50, kernel_name = \"quartic\"),\n  bw_60 = tkde(bike_accidents$Time, w = w, samples = samples, bw = 60, kernel_name = \"quartic\"),\n  time = samples\n)\n\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = months_starts_num, \n                     labels = months_starts_labs) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\") + \n  theme(axis.text = element_text(size = 5))\n\n\nbw1 &lt;- bw.bcv(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\nbw2 &lt;- bw.ucv(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\nbw3 &lt;- bw.SJ(bike_accidents$Time, nb = 1000, lower = 1, upper = 80)\n\n\ntime_kernel_values &lt;- data.frame(\n  bw_bcv = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw1, kernel_name = \"quartic\"),\n  bw_ucv = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw2, kernel_name = \"quartic\"),\n  bw_SJ = tkde(bike_accidents$Time, w = w, samples = samples, bw = bw3, kernel_name = \"quartic\"),\n  time = samples\n)\n\ndf_time &lt;- reshape2::melt(time_kernel_values,id.vars = \"time\")\ndf_time$variable &lt;- as.factor(df_time$variable)\n\nggplot(data = df_time) + \n  geom_line(aes(x = time, y = value)) +\n  scale_x_continuous(breaks = months_starts_num, labels = months_starts_labs) +\n  facet_wrap(vars(variable), ncol=2, scales = \"free\")  + \n  theme(axis.text = element_text(size = 5))\n\n\ntm_shape(mtl_network) + \n  tm_lines(col = \"black\") + \n  tm_shape(bike_accidents) + \n  tm_dots(col = \"red\", size = 0.1)\n\n\n# creating sample points\nlixels &lt;- lixelize_lines(mtl_network, 50)\nsample_points &lt;- lines_center(lixels)\n\n# calculating the densities\nnkde_densities &lt;- nkde(lines = mtl_network,\n                       events = bike_accidents,\n                       w = rep(1,nrow(bike_accidents)),\n                       samples = sample_points,\n                       kernel_name = \"quartic\",\n                       bw = 450,\n                       adaptive = TRUE, trim_bw = 900,\n                       method = \"discontinuous\",\n                       div = \"bw\",\n                       max_depth = 10,\n                       digits = 2, tol = 0.1, agg = 5,\n                       grid_shape = c(1,1),\n                       verbose = FALSE)\n\nsample_points$density &lt;- nkde_densities$k * 1000\n\ntm_shape(sample_points) + \n  tm_dots(col = \"density\", style = \"kmeans\", n = 8, palette = \"viridis\", size = 0.05) + \n  tm_layout(legend.outside = TRUE)\n\n\n# creating sample points\nlixels &lt;- lixelize_lines(mtl_network, 50)\nsample_points &lt;- lines_center(lixels)\n\n# calculating the densities\nnkde_densities &lt;- nkde(lines = mtl_network,\n                       events = bike_accidents,\n                       w = rep(1,nrow(bike_accidents)),\n                       samples = sample_points,\n                       kernel_name = \"quartic\",\n                       bw = 450,\n                       adaptive = TRUE, trim_bw = 900,\n                       method = \"discontinuous\",\n                       div = \"bw\",\n                       max_depth = 10,\n                       digits = 2, tol = 0.1, agg = 5,\n                       grid_shape = c(1,1),\n                       verbose = FALSE)\n\n\nsample_points$density &lt;- nkde_densities$k * 1000\n\n\ntm_shape(sample_points) + \n  tm_dots(col = \"density\", \n          style = \"kmeans\", \n          n = 8, \n          palette = \"viridis\", \n          size = 0.05) + \n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#references",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#references",
    "title": "Network Constrained Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nspNetwork: Spatial Analysis on Network\nNetwork Kernel Density Estimate\nDetails about NKDE\nNetwork k Functions"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/data/rawdata/Punggol_BusStop.html",
    "href": "In-class_Ex/In-class_Ex03/data/rawdata/Punggol_BusStop.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  GDM.BusStop  ENG dataset\n\nGDM.BusStop\n\nREQUIRED: A brief narrative summary of the data set.\nREQUIRED: A summary of the intentions with which the data set was developed.  REQUIRED: Common-use word or phrase used to describe the subject of the data set.       REQUIRED: Restrictions and legal prerequisites for accessing the data set. REQUIRED: Restrictions and legal prerequisites for using the data set after access is granted.      0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#getting-started",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Getting started",
    "text": "Getting started\n\nLaunch the coursework project with RStudio\nCreate a new folder called In-class_Ex.\nCreate a new sub-folder inside the newly created In-class_Ex folder. Name the sub-folder In-class_Ex01.\nCreate a new Quarto document. Save the newly create qmd file in In-class_Ex01 sub-folder. Call the file In-class_Ex01."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#loading-the-r-packages",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Loading the R packages",
    "text": "Loading the R packages\n\nThe taskThe code\n\n\nFor the purpose of this in-class exercise, the following R packages will be used:\n\ntidyverse\nsf\ntmap\nggstatsplot\n\nWrite a code chunk to check if these two packages have been installed in R. If yes, load them in R environment.\n\n\n\n\npacman::p_load(tidyverse, sf, tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nCreate a sub-folder called data in In-class_Ex01 folder.\nIf necessary visit data.gov.sg and download Master Plan 2014 Subzone Boundary (Web) from the portal. You are required to download both the ESRI shapefile and kml file.\nWrite a code chunk to import Master Plan 2014 Subzone Boundary (Web) in shapefile and kml save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports shapefile.\n\nmpsz14_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nThis code chunk imports kml file.\n\nmpsz14_kml &lt;- st_read(\"data/MasterPlan2014SubzoneBoundaryWebKML.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-planning-sub-zone-data-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan Planning Sub-zone Data",
    "text": "Working with Master Plan Planning Sub-zone Data\n\nThe taskThe code\n\n\n\nWrite a code chunk to export mpsz14_shp sf data.frame into kml file save the output in data sub-folder. Name the output file MP14_SUBZONE_WEB_PL.\n\n\n\n\n\nst_write(mpsz14_shp, \n         \"data/MP14_SUBZONE_WEB_PL.kml\",\n         delete_dsn = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-pre-school-location-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Pre-school Location Data",
    "text": "Working with Pre-school Location Data\n\nThe taskThe code\n\n\n\nIf necessary visit data.gov.sg and download Pre-Schools Location from the portal. You are required to download both the kml and geojson files.\nWrite a code chunk to import Pre-Schools Location in kml geojson save them in sf simple features data frame.\n\n\n\n\nThis code chunk imports kml file.\n\npreschool_kml &lt;- st_read(\"data/PreSchoolsLocation.kml\")\n\nThis code chunk imports geojson file.\n\npreschool_geojson &lt;- st_read(\"data/PreSchoolsLocation.geojson\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-2019-subzone-boundary-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-master-plan-2019-subzone-boundary-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Master Plan 2019 Subzone Boundary Data",
    "text": "Working with Master Plan 2019 Subzone Boundary Data\n\nThe taskTo import shapefileTo import kml\n\n\n\nVisit data.gov.sg and download Master Plan 2019 Subzone Boundary (No Sea) from the portal. You are required to download both the kml file.\nMove MPSZ-2019 shapefile provided for In-class Exercise 1 folder on elearn to data sub-folder of In-class_Ex02.\nWrite a code chunk to import Master Plan 2019 Subzone Boundary (No SEA) kml and MPSZ-2019 into sf simple feature data.frame.\n\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\")\n\n\n\n\n\n\nmpsz19_kml &lt;- st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nChecking coordinate system\n\nThe taskThe code\n\n\nWrite a code chunk to check the project of the imported sf objects.\n\n\n\n\nst_crs(mpsz19_shp)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#handling-coordinate-systems-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Handling Coordinate Systems",
    "text": "Handling Coordinate Systems\nTransforming coordinate system\n\nThe taskTo import MPSZ-2019To import PreSchoolsLocation.kml\n\n\nRe-write the code chunk to import the Master Plan Sub-zone 2019 and Pre-schools Location with proper transformation\n\n\n\n\nmpsz19_shp &lt;- st_read(dsn = \"data/\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\npreschool &lt;- st_read(\"data/PreSchoolsLocation.kml\") %&gt;%\n  st_transform(crs = 3414)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nPoint-in-Polygon count\n\nThe taskThe code\n\n\nWrite a code chunk to count the number of pre-schools in each planning sub-zone.\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(`PreSch Count` = lengths(\n    st_intersects(mpsz19_shp, preschool)))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#geospatial-data-wrangling-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling\nComputing density\n\nThe taskThe code\n\n\nWrite a single line code to perform the following tasks:\n\nDerive the area of each planning sub-zone.\nDrop the unit of measurement of the area (i.e. m^2)\nCalculate the density of pre-school at the planning sub-zone level.\n\n\n\n\n\nmpsz19_shp &lt;- mpsz19_shp %&gt;%\n  mutate(Area = units::drop_units(\n    st_area(.)),\n    `PreSch Density` = `PreSch Count` / Area * 1000000\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#statistical-analysis",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Statistical Analysis",
    "text": "Statistical Analysis\n\nThe taskThe codeThe plot\n\n\nUsing appropriate Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA) methods to explore and confirm the statistical relationship between Pre-school Density and Pre-school count.\nTip: Refer to ggscatterstats() of ggstatsplot package.\n\n\n\n\nmpsz$`PreSch Density` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Density`))\nmpsz$`PreSch Count` &lt;- as.numeric(as.character(mpsz19_shp$`PreSch Count`)) \nmpsz19_shp &lt;- as.data.frame(mpsz19_shp)\n\nggscatterstats(data = mpsz19_shp,\n               x = `PreSch Density`,\n               y = `PreSch Count`,\n               type = \"parametric\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#working-with-population-data",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Working with Population Data",
    "text": "Working with Population Data\n\nThe taskThe code\n\n\n\nVisit and extract the latest Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling from Singstat homepage.\n\n\n\n\npopdata &lt;- read_csv(\"data/respopagesextod2023.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-wrangling",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-wrangling",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nThe taskThe code\n\n\n\nWrite a code chunk to prepare a data.frame showing population by Planning Area and Planning subzone\n\n\n\n\npopdata2023 &lt;- popdata %&gt;% \n  group_by(PA, SZ, AG) %&gt;% \n  summarise(`POP`=sum(`Pop`)) %&gt;%  \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG,\n              values_from = POP)\n\ncolnames(popdata2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#data-processing",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Data Processing",
    "text": "Data Processing\n\nThe taskThe code\n\n\nWrite a code chunk to derive a tibble data.framewith the following fields PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY where by:\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\n\n\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate(YOUNG=rowSums(.[3:6]) # Aged 0 - 24, 10 - 24\n         +rowSums(.[14])) %&gt;% # Aged 5 - 9\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+ # Aged 25 - 59\n  rowSums(.[15])) %&gt;%  # Aged 60 -64\n  mutate(`AGED`=rowSums(.[16:21])) %&gt;%\n  mutate(`TOTAL`=rowSums(.[3:21])) %&gt;%\n  mutate(`DEPENDENCY`=(`YOUNG` + `AGED`)\n  / `ECONOMY ACTIVE`) %&gt;% \n  select(`PA`, `SZ`, `YOUNG`, \n         `ECONOMY ACTIVE`, `AGED`,\n         `TOTAL`, `DEPENDENCY`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#joining-popdata2023-and-mpsz19_shp",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Joining popdata2023 and mpsz19_shp",
    "text": "Joining popdata2023 and mpsz19_shp\n\npopdata2023 &lt;- popdata2023 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) \n\n\nmpsz_pop2023 &lt;- left_join(mpsz19_shp, popdata2023,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\npop2023_mpsz &lt;- left_join(popdata2023, mpsz19_shp, \n                          by = c(\"SZ\" = \"SUBZONE_N\"))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#choropleth-map-of-dependency-ratio-by-planning-subzone",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Choropleth Map of Dependency Ratio by Planning Subzone",
    "text": "Choropleth Map of Dependency Ratio by Planning Subzone\n\nThe mapThe code"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nThe concept\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 1: Data Preparation\nThe code chunk below excludes records with NA by using the code chunk below.\n\n\nmpsz_pop2023 &lt;- mpsz_pop2023 %&gt;%\n  drop_na()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-2",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-2",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 2: The get function\nThe code chunk below defines a function to get the input data and field to be used for creating the percentile map.\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-3",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-3",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 3: A percentile mapping function\nThe code chunk below creates a function for computing and plotting the percentile map.\n\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(mpsz_pop2023) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-4",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-percentile-map-4",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Percentile Map",
    "text": "Analytical Map: Percentile Map\nStep 4: Running the functions\nThe code chunk below runs the percentile map function.\n\n\npercentmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\n\n\nThe Concept\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\n\nggplot(data = mpsz_pop2023,\n       aes(x = \"\",\n           y = DEPENDENCY)) +\n  geom_boxplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-1",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-1",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 1: Creating the boxbreaks function\n\n\nThe code chunk on the right is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analyticsal-map-box-map",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analyticsal-map-box-map",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analyticsal Map: Box Map",
    "text": "Analyticsal Map: Box Map\nStep 2: Creating the get.var function\n\n\nThe code chunk on the right an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-2",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-2",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 3: Boxmap function\n\n\nThe code chunk on the right is an R function to create a box map.\n\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\n\nreturns:\n\na tmap-element (plots a map)\n\n\n\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-3",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-3",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nStep 4: Plotting Box Map\n\nboxmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-4",
    "href": "In-class_Ex/In-class_Ex01/In-class_Ex01.html#analytical-map-box-map-4",
    "title": "In-class Exercise 1: Geospatial Data Science with R",
    "section": "Analytical Map: Box Map",
    "text": "Analytical Map: Box Map\nPlotting Interactive Box Map\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nboxmap(\"DEPENDENCY\", mpsz_pop2023)"
  },
  {
    "objectID": "git.html",
    "href": "git.html",
    "title": "Git and Github",
    "section": "",
    "text": "Happy Git and GitHub for the useR. Highly recommended to beginners."
  },
  {
    "objectID": "git.html#book",
    "href": "git.html#book",
    "title": "Git and Github",
    "section": "",
    "text": "Happy Git and GitHub for the useR. Highly recommended to beginners."
  },
  {
    "objectID": "git.html#gitgithub",
    "href": "git.html#gitgithub",
    "title": "Git and Github",
    "section": "git/github",
    "text": "git/github\n\ngithub doc\nGitHub and RStudio"
  },
  {
    "objectID": "git.html#blog-post",
    "href": "git.html#blog-post",
    "title": "Git and Github",
    "section": "Blog post",
    "text": "Blog post\n\ngit in RStudio\nSolving git(GitHub) token issue\nGetting starting with git and GitHub using RStudio\ngit\nTransform a folder as git project synchronized on Github or Gitlab\nRecovering from common Git predicaments"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex01/data/MPSZ-2019.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-1-installing-maptools",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-1-installing-maptools",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 1: Installing maptools",
    "text": "Issue 1: Installing maptools\nmaptools is retired and binary is removed from CRAN. However, we can download from Posit Public Package Manager snapshots by using the code chunk below.\n\n\ninstall.packages(\"maptools\", \n                 repos = \"https://packagemanager.posit.co/cran/2023-10-13\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-1-installing-maptools-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-1-installing-maptools-1",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 1: Installing maptools",
    "text": "Issue 1: Installing maptools\n\n\nAfter the installation is completed, it is important to edit the code chunk as shown below in order to avoid maptools being download and install repetitively every time the Quarto document been rendered."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-2-creating-coastal-outline",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#issue-2-creating-coastal-outline",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Issue 2: Creating coastal outline",
    "text": "Issue 2: Creating coastal outline\nIn sf package, there are two functions allow us to combine multiple simple features into one simple features. They are st_combine() and st_union().\n\nst_combine() returns a single, combined geometry, with no resolved boundaries; returned geometries may well be invalid.\nIf y is missing, st_union(x) returns a single geometry with resolved boundaries, else the geometries for all unioned pairs of x[i] and y[j]."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#introducing-spatstat-package",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#introducing-spatstat-package",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Introducing spatstat package",
    "text": "Introducing spatstat package\nspatstat R package is a comprehensive open-source toolbox for analysing Spatial Point Patterns. Focused mainly on two-dimensional point patterns, including multitype or marked points, in any spatial region."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#spatstat",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#spatstat",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "spatstat",
    "text": "spatstat\nspatstat sub-packages\n\n\nThe spatstat package now contains only documentation and introductory material. It provides beginner’s introductions, vignettes, interactive demonstration scripts, and a few help files summarising the package.\nThe spatstat.data package now contains all the datasets for spatstat.\nThe spatstat.utils package contains basic utility functions for spatstat.\nThe spatstat.univar package contains functions for estimating and manipulating probability distributions of one-dimensional random variables.\nThe spatstat.sparse package contains functions for manipulating sparse arrays and performing linear algebra.\nThe spatstat.geom package contains definitions of spatial objects (such as point patterns, windows and pixel images) and code which performs geometrical operations.\nThe spatstat.random package contains functions for random generation of spatial patterns and random simulation of models.\nThe spatstat.explore package contains the code for exploratory data analysis and nonparametric analysis of spatial data.\nThe spatstat.model package contains the code for model-fitting, model diagnostics, and formal inference.\nThe spatstat.linnet package defines spatial data on a linear network, and performs geometrical operations and statistical analysis on such data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#creating-ppp-objects-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#creating-ppp-objects-from-sf-data.frame",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Creating ppp objects from sf data.frame",
    "text": "Creating ppp objects from sf data.frame\nInstead of using the two steps approaches discussed in Hands-on Exercise 3 to create the ppp objects, in this section you will learn how to work with sf data.frame.\n\n\nIn the code chunk below, as.ppp() of spatstat.geom package is used to derive an ppp object layer directly from a sf tibble data.frame.\n\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\nplot(childcare_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1925 symbols are shown in the symbol map\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1925 symbols are shown in the symbol map\n\n\n\n\n\n\n\n\n\n\n\nNext, summary() can be used to reveal the properties of the newly created ppp objects.\n\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nMark variables: Name, Description\nSummary:\n     Name           Description       \n Length:1925        Length:1925       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#creating-owin-object-from-sf-data.frame",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#creating-owin-object-from-sf-data.frame",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Creating owin object from sf data.frame",
    "text": "Creating owin object from sf data.frame\n\n\nIn the code chunk as.owin() of spatstat.geom is used to create an owin object class from polygon sf tibble data.frame.\n\n\nsg_owin &lt;- as.owin(sg_sf)\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\n\nNext, summary() function is used to display the summary information of the owin object class.\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#combining-point-events-object-and-owin-object",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#combining-point-events-object-and-owin-object",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\n\nThe taskThe codeThe output\n\n\nUsing the step you learned from Hands-on Exercise 3, create an ppp object by combining childcare_ppp and sg_owin.\n\n\n\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nplot(childcareSG_ppp)\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1925 symbols are shown in the symbol map\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 1925 symbols are shown in the symbol map"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation-of-spatial-point-event",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation-of-spatial-point-event",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation of Spatial Point Event",
    "text": "Kernel Density Estimation of Spatial Point Event\nThe code chunk below re-scale the unit of measurement from metre to kilometre before performing KDE.\n\n\nchildcareSG_ppp.km &lt;- rescale.ppp(childcareSG_ppp, \n                                  1000, \n                                  \"km\")\n\nkde_childcareSG_adaptive &lt;- adaptive.density(\n  childcareSG_ppp.km, \n  method=\"kernel\")\nplot(kde_childcareSG_adaptive)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nCode chunk shown two different ways to convert KDE output into grid object\n\nmaptools methodspatstat.geom method\n\n\n\n\npar(bg = '#E4D5C9')\n\ngridded_kde_childcareSG_ad &lt;- maptools::as.SpatialGridDataFrame.im(\n  kde_childcareSG_adaptive)\nspplot(gridded_kde_childcareSG_ad)\n\n\n\n\n\n\ngridded_kde_childcareSG_ad &lt;- as(\n  kde_childcareSG_adaptive,\n  \"SpatialGridDataFrame\")\nspplot(gridded_kde_childcareSG_ad)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation-1",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#kernel-density-estimation-1",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nVisualising KDE using tmap\nThe code chunk below is used to plot the output raster by using tmap functions.\n\n\ntm_shape(kde_childcareSG_ad_raster) + \n  tm_raster(palette = \"viridis\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            frame = FALSE,\n            bg.color = \"#E4D5C9\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_raster()`: migrate the argument(s) related to the scale of the\nvisual variable `col` namely 'palette' (rename to 'values') to col.scale =\ntm_scale(&lt;HERE&gt;)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#extracting-study-area-using-sf-objects",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#extracting-study-area-using-sf-objects",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Extracting study area using sf objects",
    "text": "Extracting study area using sf objects\n\nThe taskThe code\n\n\nExtract and create an ppp object showing child care services and within Punggol Planning Area\n\n\nOn the other hand, filter() of dplyr package should be used to extract the target planning areas as shown in the code chunk below.\n\n\npg_owin &lt;- mpsz_sf %&gt;%\n  filter(PLN_AREA_N == \"PUNGGOL\") %&gt;%\n  as.owin()\n\nchildcare_pg = childcare_ppp[pg_owin]\n\nplot(childcare_pg)  \n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 72 symbols are shown in the symbol map\n\n\nWarning in default.charmap(ntypes, chars): Too many types to display every type\nas a different character\n\n\nWarning: Only 10 out of 72 symbols are shown in the symbol map"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#monte-carlo-simulation",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#monte-carlo-simulation",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Monte Carlo Simulation",
    "text": "Monte Carlo Simulation\n\n\n\n\n\n\n\n\nTip\n\n\nIn order to ensure reproducibility, it is important to include the code chunk below before using spatstat functions involve Monte Carlo simulation\n\n\n\n\n\nset.seed(1234)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#edge-correction-methods-of-spatstat",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#edge-correction-methods-of-spatstat",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Edge correction methods of spatstat",
    "text": "Edge correction methods of spatstat\nIn spatstat, edge correction methods are used to handle biases that arise when estimating spatial statistics near the boundaries of a study region. These corrections are essential for ensuring accurate estimates in spatial point pattern analysis, especially for summary statistics like the K-function, L-function, pair correlation function, etc.\n\nCommon Edge Correction Methods in spatstat\n\n“none”: No edge correction is applied. This method assumes that there is no bias at the edges, which may lead to underestimation of statistics near the boundaries.\n“isotropic”: This method corrects for edge effects by assuming that the point pattern is isotropic (uniform in all directions). It compensates for missing neighbors outside the boundary by adjusting the distances accordingly.\n“translate” (Translation Correction): This method uses a translation correction, which involves translating the observation window so that every point lies entirely within it. The statistic is then averaged over all possible translations.\n“Ripley” (Ripley’s Correction): Similar to the isotropic correction but specifically tailored for Ripley’s K-function and related functions. It adjusts the expected number of neighbors for points near the edges based on the shape and size of the observation window.\n“border”: Border correction reduces bias by only considering points far enough from the boundary so that their neighborhood is fully contained within the window. This can be quite conservative but reduces the influence of edge effects."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#geospatial-analytics-for-social-good-thailand-road-accident-case-study",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Geospatial Analytics for Social Good: Thailand Road Accident Case Study",
    "text": "Geospatial Analytics for Social Good: Thailand Road Accident Case Study\nBackground\n\nRoad traffic injuries, WHO.\nRoad traffic deaths and injuries in Thailand"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#the-study-area",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#the-study-area",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "The Study Area",
    "text": "The Study Area\nThe study area is Bangkok Metropolitan Region.\n\n\n\n\n\n\n\nNote\n\n\nThe projected coordinate system of Thailand is WGS 84 / UTM zone 47N and the EPSG code is 32647."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#the-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#the-data",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this exercise, three basic data sets are needed, they are:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#importing-traffic-accident-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#importing-traffic-accident-data",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Importing Traffic Accident Data",
    "text": "Importing Traffic Accident Data\n\nThe taskThe code\n\n\nUsing the steps you learned in previous lessons, import the downloaded accident data into R environment and save the output as an sf tibble data.frame.\n\n\n\n\nrdacc_sf &lt;- read_csv(\"data/thai_road_accident_2019_2022.csv\") %&gt;%\n  filter(!is.na(longitude) & longitude != \"\", \n         !is.na(latitude) & latitude != \"\") %&gt;%\n  st_as_sf(coords = c(\n    \"longitude\", \"latitude\"),\n    crs=4326) %&gt;%\n  st_transform(crs = 32647) \n\nRows: 81735 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): province_th, province_en, agency, route, vehicle_type, presumed_c...\ndbl   (6): acc_code, number_of_vehicles_involved, number_of_fatalities, numb...\ndttm  (2): incident_datetime, report_datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#visualising-the-accident-data",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02-SPPA.html#visualising-the-accident-data",
    "title": "In-class Exercise 2: Spatial Point Patterns Analysis: spatstat methods",
    "section": "Visualising The Accident Data",
    "text": "Visualising The Accident Data\n\nThe taskThe code\n\n\nUsing the steps you learned in previous lessons, import the ACLED data into R environment as an sf tibble data.frame.\n\n\n\n\ntmap_mode(\"plot\")\nacled_sf %&gt;%\n  filter(year == 2023 | \n           event_type == \"Political violence\") %&gt;%\n  tm_shape()+\n  tm_dots()\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/data/rawdata/Punggol_Road.html",
    "href": "In-class_Ex/In-class_Ex03/data/rawdata/Punggol_Road.html",
    "title": "ISSS626",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;  GDM.RoadSectionLine  ENG dataset\n\nGDM.RoadSectionLine\n\nRoad Network lines with information on road names, road category and road code\nSafeguarding Road Reserve  Road Inventory, Road Furniture, Road Elements    mailing address\n\n251 North Bridge Road Singapore 179102\n\n  &lt;city&gt;&lt;/city&gt;\n  &lt;administrativearea&gt;&lt;/administrativearea&gt;\n  &lt;postalcode&gt;&lt;/postalcode&gt;\n  &lt;country&gt;&lt;/country&gt;\n&lt;/contactAddress&gt;\n&lt;name&gt;&lt;/name&gt;\n&lt;organization&gt;&lt;/organization&gt;\n&lt;position&gt;&lt;/position&gt;\n&lt;voice&gt;63328915&lt;/voice&gt;\n&lt;fax&gt;&lt;/fax&gt;\n&lt;email&gt;&lt;/email&gt;\n&lt;role&gt;Point of contact&lt;/role&gt;\n      on need basis The data is for internal use only      0 0     false"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#loading-the-package",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#loading-the-package",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Loading the package",
    "text": "Loading the package\nIn this in-class exercise, sf, spdep, tmap, tidyverse, knitr and GWmodel will be used.\n\nDIYThe code\n\n\nUsing the step you leanred from previous hands-in, install and load the necessary R packages in R environment.\n\n\n\n\npacman::p_load(sf, ggstatsplot, tmap, tidyverse, knitr, GWmodel)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#preparing-the-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#preparing-the-data",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Preparing the Data",
    "text": "Preparing the Data\nFor this in-class exercise, Hunan shapefile and Hunan_2012 data file will be used.\n\nDIYImporting Hunan shapefileImporting Hunan_2012 tableJoining Hunan and Hunan_2012\n\n\nUsing the steps you learned from previous hands-on, complete the following tasks:\n\nimport Hunan shapefile and parse it into a sf polygon feature object.\nimport Hunan_2012.csv file parse it into a tibble data.frame.\njoin Hunan and Hunan_2012 data.frames.\n\n\n\n\n\nhunan_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\n\n\n\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\n\nhunan_sf &lt;- left_join(hunan_sf, hunan2012) %&gt;%\n  select(1:3, 7, 15, 16, 31, 32)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#mapping-gdppc",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Mapping GDPPC",
    "text": "Mapping GDPPC\n\nDIYThe code\n\n\nUsing the steps you learned from Hands-on Exercise 5, prepare a choropleth map showing the geographic distribution of GDPPC of Hunan Province.\n\n\n\n\nbasemap &lt;- tm_shape(hunan_sf) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan_sf, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#converting-to-spatialpolygondataframe",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Converting to SpatialPolygonDataFrame",
    "text": "Converting to SpatialPolygonDataFrame\n\n\n\n\n\n\nNote\n\n\nGWmodel presently is built around the older sp and not sf formats for handling spatial data in R.\n\n\n\n\n\nhunan_sp &lt;- hunan_sf %&gt;%\n  as_Spatial()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nDetermine adaptive bandwidth\n\nCross-validationAIC\n\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\nbw_CV\n\n\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = TRUE, \n             kernel = \"bisquare\", \n             longlat = T)\nbw_AIC"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-1",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nComputing geographically wieghted summary statistics\n\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = TRUE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-2",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-adaptive-bandwidth-2",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with adaptive bandwidth",
    "text": "Geographically Weighted Summary Statistics with adaptive bandwidth\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-geographically-weighted-summary-statistics",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-geographically-weighted-summary-statistics",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Visualising geographically weighted summary statistics",
    "text": "Visualising geographically weighted summary statistics\n\nThe Geographically Weighted MeanThe code\n\n\n\n\n\n\n\n\n\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nDetermine fixed bandwidth\n\nCross-validationAIC\n\n\n\n\nbw_CV &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach = \"CV\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\nbw_CV\n\n\n\n\n\n\nbw_AIC &lt;- bw.gwr(GDPPC ~ 1, \n             data = hunan_sp,\n             approach =\"AIC\",\n             adaptive = FALSE, \n             kernel = \"bisquare\", \n             longlat = T)\nbw_AIC"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-1",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nComputing adaptive bandwidth\n\n\ngwstat &lt;- gwss(data = hunan_sp,\n               vars = \"GDPPC\",\n               bw = bw_AIC,\n               kernel = \"bisquare\",\n               adaptive = FALSE,\n               longlat = T)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-bandwidth",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed bandwidth",
    "text": "Geographically Weighted Summary Statistics with fixed bandwidth\nPreparing the output data\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\ngwstat_df &lt;- as.data.frame(gwstat$SDF)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nhunan_gstat &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-2",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-summary-statistics-with-fixed-2",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Summary Statistics with fixed",
    "text": "Geographically Weighted Summary Statistics with fixed\nVisualising geographically weighted summary statistics\n\nThe Geographically Weighted MeanThe code\n\n\n\n\n\n\n\n\n\n\ntm_shape(hunan_gstat) +\n  tm_fill(\"GDPPC_LM\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Distribution of geographically wieghted mean\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Correlation with Adaptive Bandwidth",
    "text": "Geographically Weighted Correlation with Adaptive Bandwidth\nBusiness question: Is there any relationship between GDP per capita and Gross Industry Output?\n\nConventional statistical solutionThe code\n\n\n\n\n\n\n\nggscatterstats(\n  data = hunan2012, \n  x = Agri, \n  y = GDPPC,\n  xlab = \"Gross Agriculture Output\", ## label for the x-axis\n  ylab = \"GDP per capita\", \n  label.var = County, \n  label.expression = Agri &gt; 10000 & GDPPC &gt; 50000, \n  point.label.args = list(alpha = 0.7, size = 4, color = \"grey50\"),\n  xfill = \"#CC79A7\", \n  yfill = \"#009E73\", \n  title = \"Relationship between GDP PC and Gross Agriculture Output\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth-1",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#geographically-weighted-correlation-with-adaptive-bandwidth-1",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Geographically Weighted Correlation with Adaptive Bandwidth",
    "text": "Geographically Weighted Correlation with Adaptive Bandwidth\nGeospatial analytics solution\n\nDetermine the bandwidthComputing gwCorrelationExtracting the result\n\n\n\n\nbw &lt;- bw.gwr(GDPPC ~ GIO, \n             data = hunan_sp, \n             approach = \"AICc\", \n             adaptive = TRUE)\n\n\n\n\n\n\ngwstats &lt;- gwss(hunan_sp, \n                vars = c(\"GDPPC\", \"GIO\"), \n                bw = bw,\n                kernel = \"bisquare\",\n                adaptive = TRUE, \n                longlat = T)\n\n\n\n\nCode chunk below is used to extract SDF data table from gwss object output from gwss(). It will be converted into data.frame by using as.data.frame().\n\n\ngwstat_df &lt;- as.data.frame(gwstats$SDF) %&gt;%\n  select(c(12,13)) %&gt;%\n  rename(gwCorr = Corr_GDPPC.GIO,\n         gwSpearman = Spearman_rho_GDPPC.GIO)\n\n\nNext, cbind() is used to append the newly derived data.frame onto hunan_sf sf data.frame.\n\n\nhunan_Corr &lt;- cbind(hunan_sf, gwstat_df)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-local-correlation",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#visualising-local-correlation",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "Visualising Local Correlation",
    "text": "Visualising Local Correlation\n\nLocal Correlation CoefficientLocal Spearman CoefficientThe code\n\n\n\n\n\n\n\n\n\n\ntm_shape(hunan_Corr) +\n  tm_fill(\"gwSpearman\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Spearman Rho\",\n            main.title.position = \"center\",\n            main.title.size = 2.0,\n            legend.text.size = 1.2,\n            legend.height = 1.50, \n            legend.width = 1.50,\n            frame = TRUE)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#references",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#references",
    "title": "In-class Exercise 4: Geographically Weighted Summary Statistics - gwModel methods",
    "section": "References",
    "text": "References\nBrunsdon, C. et. al. (2002) “Geographically weighted summary statistics - a framework for localised exploratory data analysis”, Computer, Environment and Urban Systems, Vol 26, pp. 501-525. Available as e-journal, SMU library.\nHarris, P. & Brunsdon, C. (2010) “Exploring spatial variation and spatial relationships in freshwater acidification critical load data set for Great Britain using geographically weighted summary statistics”, Computers & Geosciences, Vol. 36, pp. 54-70. Available as e-journal, SMU library."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#overview",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#overview",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Overview",
    "text": "Overview\n\n\n\nEmerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:\n\nBuilding a space-time cube,\nCalculating Getis-Ord local Gi* statistic for each bin by using an FDR correction,\nEvaluating these hot and cold spot trends by using Mann-Kendall trend test,\nCategorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.\n\n\n\n\n\n\n\nImportant\n\n\nIt is highly recommended to read Emerging Hot Spot Analysis before you continue the exercise."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#getting-started",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#getting-started",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Getting started",
    "text": "Getting started\nInstalling and Loading the R Packages\n\n\nAs usual, p_load() of pacman package will be used to check if the necessary packages have been installed in R, if yes, load the packages on R environment.\nFive R packages are need for this in-class exercise, they are: sf, sfdep, tmap, plotly, and tidyverse.\n\n\npacman::p_load(sf, sfdep, tmap, \n               plotly, tidyverse, \n               Kendall)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#the-data",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#the-data",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "The Data",
    "text": "The Data\n\n\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_GDPPC, an attribute data set in csv format.\n\nBefore getting started, reveal the content of Hunan_GDPPC.csv by using Notepad and MS Excel."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#creating-a-time-series-cube",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#creating-a-time-series-cube",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Creating a Time Series Cube",
    "text": "Creating a Time Series Cube\nBefore getting started, students must read this article to learn the basic concept of spatio-temporal cube and its implementation in sfdep package.\nIn the code chunk below, spacetime() of sfdep ised used to create an spatio-temporal cube.\n\n\nGDPPC_st &lt;- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n\nNext, is_spacetime_cube() of sfdep package will be used to verify if GDPPC_st is indeed an space-time cube object.\n\n\nis_spacetime_cube(GDPPC_st)\n\n\nThe TRUE return confirms that GDPPC_st object is indeed an time-space cube."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#computing-gi",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#computing-gi",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Computing Gi*",
    "text": "Computing Gi*\nNext, we will compute the local Gi* statistics.\nDeriving the spatial weights\n\n\nThe code chunk below will be used to identify neighbors and to derive an inverse distance weights.\n\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(\n    st_contiguity(geometry)),\n    wt = st_inverse_distance(nb, \n                             geometry, \n                             scale = 1,\n                             alpha = 1),\n    .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above\n\n\n\nactivate() of dplyr package is used to activate the geometry context\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen we will activate the data context again and copy over the nb and wt columns to each time-slice using set_nbs() and set_wts()\n\nrow order is very important so do not rearrange the observations after using set_nbs() or set_wts().\n\n\n\n\n\n\nNote that this dataset now has neighbors and weights for each time-slice."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#computing-gi-1",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#computing-gi-1",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Computing Gi*",
    "text": "Computing Gi*\n\n\nWe can use these new columns to manually calculate the local Gi* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use unnest() to unnest gi_star column of the newly created gi_starts data.frame.\n\n\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#mann-kendall-test",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#mann-kendall-test",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Mann-Kendall Test",
    "text": "Mann-Kendall Test\nA monotonic series or function is one that only increases (or decreases) and never changes direction. So long as the function either stays flat or continues to increase, it is monotonic.\nH0: No monotonic trend\nH1: Monotonic trend is present\nInterpretation\n\nReject the null-hypothesis null if the p-value is smaller than the alpha value (i.e. 1-confident level)\nTau ranges between -1 and 1 where:\n\n-1 is a perfectly decreasing series, and\n1 is a perfectly increasing series.\n\n\n\n\n\n\n\n\nImportant\n\n\nYou are encouraged to read Mann-Kendall Test For Monotonic Trend to learn more about the concepts and method of Mann-Kendall test.."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#performing-emerging-hotspot-analysis",
    "href": "In-class_Ex/In-class_Ex06/In-class_Ex06-EHSA.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 6: Emerging Hot Spot Analysis",
    "section": "Performing Emerging Hotspot Analysis",
    "text": "Performing Emerging Hotspot Analysis\n\n\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\n\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st, \n  .var = \"GDPPC\", \n  k = 1, \n  nsim = 99\n)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#getting-started",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "pacman::p_load(olsrr, ggstatsplot, ggpubr, \n               sf, spdep, GWmodel, tmap,\n               tidyverse, gtsummary, performance,\n               see, sfdep)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#importing-the-data",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Importing the data",
    "text": "Importing the data\n\nURA Master Plan 2014 planning subzone boundary\n\ncondo_resale &lt;- read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\n\n\ncondo_resale_sf &lt;- read_rds(\n  \"data/rds/condo_resale_sf.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#correlation-analysis---ggstatsplot-methods",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#correlation-analysis---ggstatsplot-methods",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Correlation Analysis - ggstatsplot methods",
    "text": "Correlation Analysis - ggstatsplot methods\nInstead of using corrplot package, in the code chunk below, ggcorrmat() of ggstatsplot is used.\n\nggcorrmat(condo_resale[, 5:23])"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-a-hedonic-pricing-model-by-using-multiple-linear-regression-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method",
    "text": "Building a Hedonic Pricing Model by using Multiple Linear Regression Method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo_mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + \n                  AGE   + PROX_CBD + PROX_CHILDCARE + \n                  PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n                  PROX_HAWKER_MARKET    + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + \n                  PROX_SUPERMARKET + PROX_BUS_STOP + \n                  NO_Of_UNITS + FAMILY_FRIENDLY + \n                  FREEHOLD + LEASEHOLD_99YR, \n                data=condo_resale_sf)\nsummary(condo_mlr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#model-assessment-olsrr-method",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#model-assessment-olsrr-method",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Model Assessment: olsrr method",
    "text": "Model Assessment: olsrr method\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\n\nGenerating tidy linear regression report\n\nols_regress(condo_mlr)\n\n\nMulticolinearuty\n\nols_vif_tol(condo_mlr)\n\n\n\nVariable selection\n\ncondo_fw_mlr &lt;- ols_step_forward_p(\n  condo_mlr,\n  p_val = 0.05,\n  details = FALSE)\n\n\nplot(condo_fw_mlr)\n\n\n\n\nVisualising model parameters\n\nggcoefstats(condo_mlr,\n            sort = \"ascending\")\n\n\n\nTest for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo_fw_mlr$model)\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\nTest for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo_fw_mlr$model)\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo_fw_mlr$model)\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#testing-for-spatial-autocorrelation",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#testing-for-spatial-autocorrelation",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Testing for Spatial Autocorrelation",
    "text": "Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr_output &lt;- as.data.frame(condo_fw_mlr$model$residuals) %&gt;%\n  rename(`FW_MLR_RES` = `condo_fw_mlr$model$residuals`)\n\nNext, we will join the newly created data frame with condo_resale_sf object.\n\ncondo_resale_sf &lt;- cbind(condo_resale_sf, \n                        mlr_output$FW_MLR_RES) %&gt;%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\")\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\n\nSpatial stationary test\nTo proof that our observation is indeed true, the Moran’s I test will be performed\nHo: The residuals are randomly distributed (also known as spatial stationary) H1: The residuals are spatially non-stationary\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\ncondo_resale_sf &lt;- condo_resale_sf %&gt;%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.25586 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "In-class_Ex/In-class_Ex07/In-class_Ex07-gwr.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "Building Hedonic Pricing Models using GWmodel",
    "text": "Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing by using geographically weighted regression model. Two spatial weights will be used, they are: fixed and adaptive bandwidth schemes.\n\nBuilding Fixed Bandwidth GWR Model\n\nComputing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach agreement.\n\nbw_fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                     PROX_CBD + PROX_CHILDCARE + \n                     PROX_ELDERLYCARE   + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                     NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale_sf, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n\n\nGWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\ngwr_fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + \n                         AGE    + PROX_CBD + PROX_CHILDCARE + \n                         PROX_ELDERLYCARE   +PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH +\n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                         NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale_sf, \n                       bw=bw_fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\nThe output is saved in a list of class “gwrm”. The code below can be used to display the model output.\n\ngwr_fixed\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\nBuilding Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\nComputing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\nbw_adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale_sf, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\ngwr_adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale_sf, \n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\nThe code below can be used to display the model output.\n\ngwr_adaptive\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\nVisualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\nConverting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\ngwr_adaptive_output &lt;- as.data.frame(\n  gwr_adaptive$SDF) %&gt;%\n  select(-c(2:15))\n\n\ngwr_sf_adaptive &lt;- cbind(condo_resale_sf,\n                         gwr_adaptive_output)\n\nNext, glimpse() is used to display the content of condo_resale_sf.adaptive sf data frame.\n\nglimpse(gwr_sf_adaptive)\n\n\nsummary(gwr_adaptive$SDF$yhat)\n\n\n\nVisualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\ntmap_mode(\"plot\")\n\n\n\nVisualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\ntmap_mode(\"plot\")\n\n\nBy URA Plannign Region\n\ntm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(gwr_sf_adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#getting-started",
    "title": "In-class Exercise 9: Geography of Accessibility",
    "section": "Getting Started",
    "text": "Getting Started\n\n\npacman::p_load(SpatialAcc, sf, tidyverse, \n               tmap, ggstatsplot)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#count-number-of-points-within-a-distance",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#count-number-of-points-within-a-distance",
    "title": "In-class Exercise 9: Geography of Accessibility",
    "section": "Count Number of Points within a Distance",
    "text": "Count Number of Points within a Distance\n\nThe taskDIYThe codeBufferingVisualisingCounting points\n\n\n\nTo count number of point features (i.e. CHAS clinics) within 1km of another point features (i.e. eldercare centre).\n\n\n\n\n\n\n\nNote\n\n\nTo complete this section of the in-class exercise, you need to download both the CHAS Clinics and Eldercare Services data sets from data.gov.sg portal. The in-class exercise assumes explicitly the downloaded data sets are saved in rawdata sub-folder of In-class_Ex09 folder. Remember to unzip the file if necessary.\n\n\n\n\n\n\nDownload ELDERCARE shapefile and CHAS kml file from data.gov.sg\nUsing the steps your learned in Hands-on Exercise 1, import ELDERCARE shapefile and CHAS kml file into R\n\n\n\nELDERCARE is in shapefile format, the code chunk below will be used:\n\n\neldercare &lt;- st_read(dsn = \"data/rawdata\",\n                     layer = \"ELDERCARE\") %&gt;%\n  st_transform(crs = 3414)\n\n\nThe code chunk below is used to import kml file.\n\n\nCHAS &lt;- st_read(\"data/rawdata/CHASClinics.kml\") %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\nNext, st_buffer() of sf package is used to create a buffer of 1km around each eldercare features\n\n\nbuffer_1km &lt;- st_buffer(eldercare, \n                        dist = 1000)\n\n\n\n\n\n\nThe code chunk below is used to plot the newly created buffers and the CHAS clinics.\n\n\ntmap_mode(\"view\")\ntm_shape(buffer_1km) +\n  tm_polygons() +\ntm_shape(CHAS) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\nLastly, the code chunk below is used to count the number of CHAS clinics with 1km of each eldercare centre.\n\n\nbuffer_1km$pts_count &lt;- lengths(\n  st_intersects(buffer_1km, CHAS))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#importing-data",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#importing-data",
    "title": "In-class Exercise 9: Geography of Accessibility",
    "section": "Importing Data",
    "text": "Importing Data\n\nGeospatail dataOD Matrix\n\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_NO_SEA_PL\") %&gt;%\n  st_transform(crs = 3414)\n\nhexagons &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"hexagons\") %&gt;%\n  st_transform(crs = 3414)\n\neldercare &lt;- st_read(dsn = \"data/geospatial\",\n                     layer = \"ELDERCARE\") %&gt;%\n  st_transform(csr = 3414)\n\n\n\n\n\n\nODMatrix &lt;- read_csv(\"data/aspatial/OD_Matrix.csv\", \n                     skip = 0)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#data-cleaning-and-updating-attributes",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#data-cleaning-and-updating-attributes",
    "title": "In-class Exercise 9: Geography of Accessibility",
    "section": "Data cleaning and Updating Attributes",
    "text": "Data cleaning and Updating Attributes\n\nSupplyDemandOD Matrix\n\n\n\n\neldercare &lt;- eldercare %&gt;%\n  select(fid, ADDRESSPOS) %&gt;%\n  mutate(capacity = 100)\n\n\n\n\n\n\nhexagons &lt;- hexagons %&gt;%\n  select(fid) %&gt;%\n  mutate(demand = 100)\n\n\n\n\n\n\ndistmat &lt;- ODMatrix %&gt;%\n  select(origin_id, destination_id, total_cost) %&gt;%\n  spread(destination_id, total_cost)%&gt;%\n  select(c(-c('origin_id')))\n\n\ndistmat_km &lt;- as.matrix(distmat/1000)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#computing-handsens-accessibility",
    "href": "In-class_Ex/In-class_Ex09/In-class_Ex09.html#computing-handsens-accessibility",
    "title": "In-class Exercise 9: Geography of Accessibility",
    "section": "Computing Handsen’s Accessibility",
    "text": "Computing Handsen’s Accessibility\n\nThe base codeTidy the outputCombine code chunk\n\n\n\n\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 2, \n                            family = \"Hansen\"))\n\n\n\n\n\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\n\nacc_Hansen &lt;- as_tibble(acc_Hansen)\n\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)\n\n\n\n\n\n\nacc_Hansen &lt;- data.frame(ac(hexagons$demand,\n                            eldercare$capacity,\n                            distmat_km, \n                            #d0 = 50,\n                            power = 0.5, \n                            family = \"Hansen\"))\n\ncolnames(acc_Hansen) &lt;- \"accHansen\"\nacc_Hansen &lt;- as_tibble(acc_Hansen)\nhexagon_Hansen &lt;- bind_cols(hexagons, acc_Hansen)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "In this web site, you will find all course materials here.\nInstructor: Dr. Kam Tin Seong, Associate Professor of Information Systems (Practice)\nDate & Time: Saturday 12:30pm - 3:45pm\nVenue: SOE/SCIS2 Seminar Room 5-2\nPiazza link.\nConsultation booking link.\nVirtual Meeting Room: Zoom\n\n\nFor the next 10 weeks, your learning journey will be very bumpy, especially come to R programming.\n\nLearning R can be difficult at first. It is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham, the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily.\n\n\n\n\n\n\nAll acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them. Emergency\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the WebEx platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the WebEx platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "index.html#words-of-encouragement",
    "href": "index.html#words-of-encouragement",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "For the next 10 weeks, your learning journey will be very bumpy, especially come to R programming.\n\nLearning R can be difficult at first. It is like learning a new language, just like Spanish, French, or Chinese. Hadley Wickham, the chief data scientist at RStudio and the author of some amazing R packages you’ll be using like ggplot2 made this wise observation:\n\nIt’s easy when you start out programming to get really frustrated and think, “Oh it’s me, I’m really stupid,” or, “I’m not made out to program.” But, that is absolutely not the case. Everyone gets frustrated. I still get frustrated occasionally when writing R code. It’s just a natural part of programming. So, it happens to everyone and gets less and less over time. Don’t blame yourself. Just take a break, do something fun, and then come back and try again later.\n\nEven experienced programmers find themselves bashing their heads against seemingly intractable errors.\n\nIf you’re finding yourself taking way too long hitting your head against a wall and not understanding, take a break, talk to classmates, and don’t hesitate to approach me.\nThe students who have a bad time in this course are the ones who don’t work with one another to learn. We are a learning community, and we should help each other to learn.\nIf you understand something and someone is struggling with it, try and help them. If you are struggling, take a breath, and try to pinpoint what you are struggling with.\nOur goal is to be better programmers each day, not to be the perfect programmer. There’s no such thing as a perfect programmer. I’ve been learning new things almost every day.\nI promise you can succeed in this class as long as you are willing to learn, practice, be open minded and don’t give up easily."
  },
  {
    "objectID": "index.html#important-information",
    "href": "index.html#important-information",
    "title": "ISSS626 AY2025-26 August Term",
    "section": "",
    "text": "All acts of academic dishonesty (including, but not limited to, plagiarism, cheating, fabrication, facilitation of acts of academic dishonesty by others, unauthorized possession of exam questions, or tampering with the academic work of other students) are serious offences. All work (whether oral or written) submitted for purposes of assessment must be the student’s own work. Penalties for violation of the policy range from zero marks for the component assessment to expulsion, depending on the nature of the offense. When in doubt, students should consult the instructors of the course. Details on the SMU Code of Academic Integrity may be accessed at http://www.smuscd.org/resources.html\n\n\n\nCourse materials obtained during your course of study at SMU are meant for personal use only, namely, for the purposes of studying and research. You are strictly not permitted to make copies of or print additional copies or distribute such copies of the course materials or any parts thereof, for commercial gain or exchange. For example, offering such materials on the Internet through CourseHero, Carousell and the like, is strictly prohibited.\nThe selling of these materials and/or any copies thereof are strictly prohibited under Singapore copyright laws. Printed materials and electronic materials are both protected by copyright laws. All students are subject to Singapore copyright laws and must strictly adhere to SMU’s procedures and requirements relating to copyright.\nPlease also note that for some materials, the publishers may specifically state that each copy is for the personal use of one individual only and no further reprographic reproduction is allowed, including for personal use. These restrictions are spelt out clearly on these specific sets of resources and students are required to adhere to these rules.\nStudents who infringe any of the aforesaid rules, laws and requirements shall be liable to disciplinary action by SMU. In addition, such students may also leave themselves open to suits by copyright owners who are entitled to take legal action against persons who infringe their copyright.\nWe strongly urge all students to respect the copyright laws and abide by SMU’s procedures and requirements relating to copyright.\n\n\n\nSMU strives to make learning experiences accessible for all. If you anticipate or experience physical or academic barriers due to disability, please let the instructor know immediately. You are also welcome to contact the university’s disability support team if you have questions or concerns about academic accommodations:included@smu.edu.sg\nPlease be aware that the accessible tables in our seminar room should remain available for students who require them. Emergency\n\n\n\nAs part of emergency preparedness, Instructors may conduct lessons online via the WebEx platform during the term, to prepare students for online learning. During an actual emergency, students will be notified to access the WebEx platform for their online lessons. The class schedule will mirror the current face-to-face class timetable unless otherwise stated."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#content",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#content",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Content",
    "text": "Content\n\nAn Overview of Geospatial Data Models\n\nVector and raster data model\nCoordinate systems and map projection\n\nVector Data Wrangling and Analysis Methods\nRaster Data Wrangling and Analysis Methods\n\n\nThis lesson consists of two parts. First, I will talk about Geospatial Data Models. For students who have taken SMT201 GIS for Urban Planning, this is not new at all. However, for students who did not read SMT201, this will be new. Anyway, the focus of this section will be on R. Hence, even for students who have taken SMT201 before, this will be a good revision.\nIn part two of this lesson, I will introduce sp package. It is a relatively new R package specially developed to handle geospatial data R using tidyverse principle."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-models",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-models",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Geospatial Data Models",
    "text": "Geospatial Data Models\nWhy should we worry about?\n\n\nIt is important for us to note that what ever data capture in a database is a model of the real world. When we say model, this means that it is a simplify version of the real world and not the real world themselves."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#basic-spatial-data-models",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#basic-spatial-data-models",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Basic Spatial Data Models",
    "text": "Basic Spatial Data Models\n\nVector - implementation of discrete object conceptual model\n\nPoint, line and polygon representations.\nWidely used in cartography, and network analysis.\n\nRaster – implementation of field conceptual model\n\nArray of cells used to represent objects.\nUseful as background maps and for spatial analysis.\n\n\n\nIn general, there are two types of geospatial data models, namely vector and raster data models.\nVector data model tends to be used to store geospatial data that are discrete in nature. For example bus stop, building footprint, planning area.\nRaster data model, one the other hands, are used to store continuous fenomena such as air polution, elevation and precipitation."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#coordinate-systems-and-map-projections",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#coordinate-systems-and-map-projections",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Coordinate Systems and Map Projections",
    "text": "Coordinate Systems and Map Projections\nWhat is a coordinate system?\n\n\nA coordinate system is an important property of an geospatial data. It provides a location reference to the geospatial data.\n\nThere are two common types of coordinate systems used in mapping, namely: geographic coordinate systems and projected coordinate system.\n\n\n\n\n\n\n\nFurther Reading\n\n\n\nRefer to this article and Chapter 9 Coordinate Systems to learn more about map projection.\n\n\n\n\n\n\n\n\n\nA coordinate system is a reference system used to represent the locations of geographic features, imagery, and observations such as GPS locations within a common geographic framework.\nEach coordinate system is defined by:\n\nIts measurement framework which is either geographic (in which spherical coordinates are measured from the earth’s center) or planimetric (in which the earth’s coordinates are projected onto a two-dimensional planar surface).\nUnit of measurement (typically feet or meters for projected coordinate systems or decimal degrees for latitude–longitude).\nThe definition of the map projection for projected coordinate systems.\nOther measurement system properties such as a spheroid of reference, a datum, and projection parameters like one or more standard parallels, a central meridian, and possible shifts in the x- and y-directions."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#standard-for-geospatial-data-handling-and-analysis",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#standard-for-geospatial-data-handling-and-analysis",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Standard for Geospatial Data Handling and Analysis",
    "text": "Standard for Geospatial Data Handling and Analysis\n\n\n\n\n\n\n\nFurther Reading\n\n\nFor more information, visit this link.\n\n\n\n\nThe OGC OpenGIS Implementation Standard for Geographic Information / ISO 19125 defines:\n\nGeometric objects which can be of type point, line, polygon, multi-point, etc, and are associated to a given Coordinate Reference System;\nMethods on geometric objects return properties like dimension, boundary, area, centroid, etc;\nMethods for testing spatial relations between geometric objects equals, disjoint, intersects, touches, crosses, within, contains, overlaps and relate, which returns TRUE or FALSE;\nMethods that support spatial analysis distance, which returns a distance, and buffer, convex hull, intersection, union, difference, and symmetric difference, which returns new geometric objects.\n\nSource: www.opengeospatial.org/standards/sfa"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#an-introduction-to-simple-features",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#an-introduction-to-simple-features",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "An introduction to simple features",
    "text": "An introduction to simple features\n\nfeature: abstraction of real world phenomena (type or instance); has a geometry and other attributes (properties)\nsimple feature: feature with all geometric attributes described piecewise by straight line or planar interpolation between sets of points (no curves)\nIt is a hierarchical data model that simplifies geographic data by condensing a complex range of geographic forms into a single geometry class."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-object-framework",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#geospatial-data-object-framework",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Geospatial Data Object Framework",
    "text": "Geospatial Data Object Framework\n\nTo begin with, all contributed packages for handling spatial data in R had different representations of the data. This made it difficult to exchange data both within R between packages, and between R and external le formats and applications.\nThe first general package to provide classes and methods for spatial data types that was developed for R is called sp. It was first released on CRAN in 2005.\nIn late October 2016, sf was first released on CRAN to provide standardised support for vector data in R."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#r-packages-that-support-spatial-classes",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#r-packages-that-support-spatial-classes",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "R packages that support spatial classes",
    "text": "R packages that support spatial classes\nIn general, three R packages will be used to handle vector-based geospatial data in spatial classes, they are:\n\nsp provides classes and methods for dealing with spatial data in R.\nrgdal allows R to understand the structure of a geospatial data file by providing functions to read and convert geospatial data into easy-to-work-with R dataframes.\nrgeos implements the methods of the OGC standard."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#introducing-sf-package",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#introducing-sf-package",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "Introducing sf Package",
    "text": "Introducing sf Package\n\nsf package provides a syntax and data-structures which are coherent with the tidyverse.\nA quick introduction can be found here.\nFor more detail, visit this link."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#sf-functions",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#sf-functions",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "sf functions",
    "text": "sf functions\n\nGeospatial data handling\nGeometric confirmation\nGeometric operations\nGeometry creation\nGeometry operations\nGeometric measurement"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Geospatial_Data.html#references",
    "href": "lesson/Lesson01/Lesson01-Geospatial_Data.html#references",
    "title": "Lesson 1: Fundamental of Geospatial Data Models and Modelling",
    "section": "References",
    "text": "References\nAll About sf package\n\n\nReference manual\nTidy spatial data analysis\n\nVignettes:\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\nPlotting Simple Features\nMiscellaneous\nSpherical geometry in sf using s2geometry\n\nOthers:\n\nR spatial follows GDAL and PROJ development"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nA Geographical Information System (GIS) is a toolkit for creating, managing, analysing, visualising, and sharing data of any kind according to where it’s located.\n\n\n\nGeospatial analytics is more than a GIS."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nGeospatial analytics is more than data visualisation\n\n\n\n\n\nSource: Singapore’s first disease map delivers real-time information on infectious diseases"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#demystifying-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Demystifying Geospatial Analytics",
    "text": "Demystifying Geospatial Analytics\n\nGeospatial analytics is more than just mapping."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nAbout 80% of all data maintained by organisations around the world has a location component.\n\n\n\n(Source: BusinessWeek Research Services, 2006)"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nGeospatial information in Smart Nation.\n\n\n\nSee more at this link"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nThe explosion in the availability of open geospatial data from both the public and private sectors at national and international levels."
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#motivation-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Motivation of Geospatial Analytics",
    "text": "Motivation of Geospatial Analytics\n\nThe national geospatial master plan.\n\n\n\nSource: Singapore Geospatial Master Plan"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nUncovering insights not found in statistical graphs and tables.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-1",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo reveal the untapped property of spatial contiguity in geographic knowledge discovery in databases.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-2",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo uncover the complexity of the real world relationship.\n\n\n\nSource: SGSAS - Simple Geo-Spatial Analysis using R Shiny"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#the-role-of-geospatial-analytics-3",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "The role of Geospatial Analytics",
    "text": "The role of Geospatial Analytics\n\nTo model spatial interactions and flows.\n\n\n\nSource: IS415 Bus Rider Flow Project"
  },
  {
    "objectID": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "href": "lesson/Lesson01/Lesson01-Introduction_to_GAA.html#geospatial-analytics-and-social-consciousness",
    "title": "Lesson 1: Introduction to Geospatial Analytics and Applications",
    "section": "Geospatial Analytics and Social Consciousness",
    "text": "Geospatial Analytics and Social Consciousness\nThe true power of geospatial analytics is to provide decision makers and planners with data-driven and process information for better problem solving and more efficient use of resources."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#content",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#content",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\n\n\nIntroducing Spatial Point Patterns\n\nThe basic concepts of spatial point patterns\n1st Order versus 2nd Order\nSpatial Point Patterns in real world\n\n1st Order Spatial Point Patterns Analysis\n\nQuadrat analysis\nKernel density estimation\n\n\n\n\n2nd Order Spatial Point Patterns Analysis\n\nNearest Neighbour Index\nG-function\nF-function\nK-function\nL-function\n\n\n\n\nWelcome to Lesson 4: Spatial Point Pattern Analysis, a family of spatial statistics specially developed to model and to test distribution of spatial point events.\nThe lesson proceeding is divided into three main section. First, I will share with you what are real world spatial point events. This is followed by explaining the concepts and methods of 1st-order and 2nd-order spatial point patterns analysis."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#what-is-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#what-is-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "What is Spatial Point Patterns",
    "text": "What is Spatial Point Patterns\n\nPoints as Events\nMapped pattern\n\nNot a sample\nSelection bias\n\nEvents are mapped, but non-events are not"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#real-world-question",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#real-world-question",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Real World Question",
    "text": "Real World Question\n\nLocation only\n\nare points randomly located or patterned\n\nLocation and value\n\nmarked point pattern\nis combination of location and value random or patterned\n\nWhat is the underlying process?\n\n\nIt is important note that SPPA is exploratory and confirmatory in nature. They are specially developed for describing the spatial point pattern and for confirming the observed patterns statistically. However, they are explanatory nor for prediction."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#points-on-a-plane",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#points-on-a-plane",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Points on a Plane",
    "text": "Points on a Plane\n\nClassic point pattern analysis\n\npoints on an isotropic plane\nno effect of translation and rotation\nclassic examples: tree seedlings, rocks, etc\n\nDistance\n\nstraight line only"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis",
    "text": "Spatial Point Patterns Analysis\n\nPoint pattern analysis (PPA) is the study of the spatial arrangements of points in (usually 2-dimensional) space.\nThe simplest formulation is a set X = {x ∈ D} where D, which can be called the study region, is a subset of Rn, a n-dimensional Euclidean space.\nA fundamental problem of PPA is inferring whether a given arrangement is merely random or the result of some process."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#homogeneous-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#homogeneous-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Homogeneous Spatial Point Patterns",
    "text": "Homogeneous Spatial Point Patterns\n\n\nA homogeneous spatial point pattern assumes that the points are distributed uniformly across the study area. The intensity (expected number of points per unit area) is constant throughout the region.\n\n\nCharacteristics:\n\nThe probability of observing a point is the same across the entire space.\nThe process generating the points does not depend on location.\nThe points are randomly and independently distributed across the space, leading to a uniform density.\n\nModel:\n\nTypically modeled by a Homogeneous Poisson Process, where the intensity λ (the number of points per unit area) is constant."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#heterogeneous-spatial-point-patterns",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#heterogeneous-spatial-point-patterns",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Heterogeneous Spatial Point Patterns",
    "text": "Heterogeneous Spatial Point Patterns\n\n\nA heterogeneous spatial point pattern assumes that the intensity of points varies across the study area. The intensity function is not constant and may depend on spatial covariates, leading to non-uniform distribution.\n\n\nCharacteristics:\n\nThe probability of observing a point varies across the space, often depending on underlying factors like geography, environmental conditions, or other spatial variables.\nThe density of points can be higher in some regions and lower in others, leading to clusters or dispersed patterns.\n\nModel: - Modeled by an Inhomogeneous Poisson Process (IPP), where the intensity function λ(x,y) varies with location. The intensity can be a function of spatial covariates or other influences."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis-techniques",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#spatial-point-patterns-analysis-techniques",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Spatial Point Patterns Analysis Techniques",
    "text": "Spatial Point Patterns Analysis Techniques\n\nFirst-order vs Second-order Analysis of spatial point patterns.\n\n\nReference: 11.4 First and second order effects of Intro to GIS and Spatial Analysis\n\nThe first-order properties describe the way in which the expected value (mean or average) of the spatial point pattern varies across space (i.e., the intensity of the spatial point pattern). Such properties are usually measured with the so-called quadrat analysis, nearest neighbour index and kernel estimation. Second-order properties describe the covariance (or correlation) between values of the spatial point pattern at different regions in space and are usually measured with the G function, K function and L function. Applied to point event data, both properties could be used to explore the spatial variation in the risk of being victimized by a crime, spatial and space-time clustering of criminal activities, and the raised incidence of criminal activities around point sources, such as robberies around ATM machines, subway entrances and exits, etc."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#kernel-density-estimation-silverman-1986",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#kernel-density-estimation-silverman-1986",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Kernel density estimation (Silverman 1986)",
    "text": "Kernel density estimation (Silverman 1986)\n\nA method to compute the intensity of a point distribution.\n\n\n\nThe general formula:\n\n\nGraphically"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#distance-based-nearest-neighbour-index",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#distance-based-nearest-neighbour-index",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Distance-based: Nearest Neighbour Index",
    "text": "Distance-based: Nearest Neighbour Index\nWhat is Nearest Neighbour?\nDirect distance from a point to its nearest neighbour."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#nearest-neighbour-index",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#nearest-neighbour-index",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Nearest Neighbour Index",
    "text": "Nearest Neighbour Index\nThe Nearest Neighbour Index is expressed as the ratio of the Observed Mean Distance to the Expected Mean Distance."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#g-function",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#g-function",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "G function",
    "text": "G function\n\n\nThe formula"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#f-function",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#f-function",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "F function",
    "text": "F function\n\nSelect a sample of point locations anywhere in the study region at random\n\nDetermine minimum distance from each point to any event in the study area.\n\nThree steps:\n\nRandomly select m points (p1, p2, ….., pn),\nCalculate dmin(pi,s) as the minimum distance from location pi to any event in the point patterns, and\nCalculate F(d)."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#ripleys-k-function-ripley-1981",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#ripleys-k-function-ripley-1981",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "Ripley’s K function (Ripley, 1981)",
    "text": "Ripley’s K function (Ripley, 1981)\n\nLimitation of nearest neighbor distance method is that it uses only nearest distance\nConsiders only the shortest scales of variation.\nK function uses more points.\n\nProvides an estimate of spatial dependence over a wider range of scales.\nBased on all the distances between events in the study area.\nAssumes isotropy over the region."
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#the-l-function-besag-1977",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#the-l-function-besag-1977",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "The L function (Besag 1977)",
    "text": "The L function (Besag 1977)\n\n\nIn practice, K function will be normalised to obtained a benchmark of zero.\nThe formula:"
  },
  {
    "objectID": "lesson/Lesson02/Lesson02-SPPA.html#references",
    "href": "lesson/Lesson02/Lesson02-SPPA.html#references",
    "title": "Lesson 2: Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nChapter 11 Point Pattern Analysis of Intro to GIS and Spatial Analysis. Section 11.2, 11.3, 11.3.1 and 11.4\nGIS&T Body of Knowledge AM-07-Point Pattern Analysis\nGIS&T Body of Knowledge AM-08-Kernels and Density Estimation\nAnalyzing Patterns in Business Point Data, Directions Magazine March 17, 2005.\nO’Sullivan, D., and Unwin, D. (2010) Geographic Information Analysis, Second Edition. John Wiley & Sons Inc., New Jersey, Canada. Chapter 5-6.\nBaddeley A., Rubak E. and Turner R. (2015) Spatial Point Patterns: Methodology and Applications with R, Chapman and Hall/CRC."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-is-geographically-referenced-attribute",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "What is geographically referenced attribute?",
    "text": "What is geographically referenced attribute?\n\n\nRows: 323\nColumns: 12\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;fct&gt; MSSZ01, OTSZ01, SRSZ03, BMSZ08, BMSZ03, BMSZ07, BMSZ0…\n$ PLN_AREA_N       &lt;fct&gt; MARINA SOUTH, OUTRAM, SINGAPORE RIVER, BUKIT MERAH, B…\n$ PLN_AREA_C       &lt;fct&gt; MS, OT, SR, BM, BM, BM, BM, SR, QT, QT, QT, BM, ME, R…\n$ REGION_N         &lt;fct&gt; CENTRAL REGION, CENTRAL REGION, CENTRAL REGION, CENTR…\n$ REGION_C         &lt;fct&gt; CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, CR, C…\n$ YOUNG            &lt;dbl&gt; NA, 1100, 0, 2620, 2840, 2910, 2850, 0, 1120, 30, NA,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; NA, 3420, 50, 7500, 6260, 7560, 8340, 50, 2750, 210, …\n$ AGED             &lt;dbl&gt; NA, 2110, 20, 3260, 1630, 3310, 3590, 10, 560, 50, NA…\n$ TOTAL            &lt;dbl&gt; NA, 6630, 70, 13380, 10730, 13780, 14780, 60, 4430, 2…\n$ DEPENDENCY       &lt;dbl&gt; NA, 0.9385965, 0.4000000, 0.7840000, 0.7140575, 0.822…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n\n\n\nA kind of data that is very similar to an ordinary data. The only difference is that each observation is associated with some form of geography such as numbers of aged population by planning subzone."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-are-spatial-weights-wij",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#what-are-spatial-weights-wij",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "What are Spatial Weights (wij)",
    "text": "What are Spatial Weights (wij)\n\nA way to define spatial neighbourhood.\n\n\n\nBefore we can perform statistics test of spatial randomness, we need to understand how spatial relationship among geographical areas can be defined mathematically."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#applications-of-spatial-weights",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#applications-of-spatial-weights",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "Applications of Spatial Weights",
    "text": "Applications of Spatial Weights\nFormally, for observation i, the spatial lag of yi, referred to as [Wy]i (the variable Wy observed for location i) is:\n\nwhere the weights wij consist of the elements of the i-th row of the matrix W, matched up with the corresponding elements of the vector y.\n\nWith a neighbor structure defined by the non-zero elements of the spatial weights matrix W, a spatially lagged variable is a weighted sum or a weighted average of the neighboring values for that variable. In most commonly used notation, the spatial lag of y is then expressed as Wy."
  },
  {
    "objectID": "lesson/Lesson04/Lesson04-Spatial_Weights.html#references",
    "href": "lesson/Lesson04/Lesson04-Spatial_Weights.html#references",
    "title": "Lesson 4: Spatial Weights and Applications",
    "section": "References",
    "text": "References\n\nChapter 2. Codifying the neighbourhood structure of Handbook of Spatial Analysis: Theory and Application with R.\nFrançois Bavaud (2010) “Models for Spatial Weights: A Systematic Look” Geographical Analysis, Vol. 30, No.2, pp 153-171.\nTony H. Grubesic and Andrea L. Rosso (2014) “The Use of Spatially Lagged Explanatory Variables for Modeling Neighborhood Amenities and Mobility in Older Adults”, Cityscape, Vol. 16, No. 2, pp. 205-214."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#content",
    "href": "lesson/Lesson06/Lesson06.html#content",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geographic Segmentation\nSpatialising classic clustering methods\nSpatially Constrained Clustering - Hierarchical methods\n\nskater\nREDCAP\nclustGeo\n\nSpatially Constrained Clustering - Partitioning methods\n\nAutomatic zoning procedure (AZP)\nmax-p heuristic"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#regionalisation-and-clustering",
    "href": "lesson/Lesson06/Lesson06.html#regionalisation-and-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Regionalisation and Clustering",
    "text": "Regionalisation and Clustering\n\n\n\nRegionalisation is a process of to group a large number of geographical units such as provinces, districts or counties spatial objects into a smaller number of subsets of objects also known as regions, which are internally homogeneous and occupy contiguous regions in space.\nThe process taking into consideration multivariates. Figure on the right shows regions delineated by using six ICT measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home of Shan State, Myanmar."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "href": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering-versus-non-spatially-constrained-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering versus Non-spatially Constrained Clustering\n\n\nA comparison of conventional non-spatially constrained clustering and spatially constrained clustering. Stars represent the centroids of sampled grid cells and polygons are Thiessen polygons that contain the centroids. Grey shading contrasts between polygons stand for the Simpson dissimilarity index (βsim) between them. Non-spatially constrained clustering produces two clusters, one of which contains polygons (C, D, and E) that are spatially disjoint. In contrast, the two clusters produced by the spatially constrained clustering form two spatially contiguous regions."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering",
    "href": "lesson/Lesson06/Lesson06.html#spatially-constrained-clustering",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "Spatially Constrained Clustering",
    "text": "Spatially Constrained Clustering\n\nSKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\nREDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning algorithm\nClustGeo algorithm"
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "href": "lesson/Lesson06/Lesson06.html#skater-spatial-kluster-analysis-by-tree-edge-removal-algorithm",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm",
    "text": "SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) algorithm\n\n\nThe SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) builds off of a connectivity graph to represent spatial relationships between neighbouring areas, where each area is represented by a node and edges represent connections between areas. Edge costs are calculated by evaluating the dissimilarity between neighbouring areas. The connectivity graph is reduced by pruning edges with higher dissimilarity until we are left with n nodes and n−1 edges. At this point any further pruning would create subgraphs and these subgraphs become cluster candidates."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#redcap-algorithm",
    "href": "lesson/Lesson06/Lesson06.html#redcap-algorithm",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "REDCAP algorithm",
    "text": "REDCAP algorithm\n\nRegionalization with dynamically constrained agglomerative clustering and partitioning, in short REDCAP is specially developed by D. Guo (2008) to the limitation of SKATER discussed in previous slide.\nLike SKATER, REDCAP starts from building a spanning tree with 4 different ways (single-linkage, average-linkage, ward-linkage and the complete-linkage). The single-linkage way leads to build a minimum spanning tree. Then,REDCAP provides 2 different ways (first-order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is exactly the same with SKATER."
  },
  {
    "objectID": "lesson/Lesson06/Lesson06.html#clustgeo-package",
    "href": "lesson/Lesson06/Lesson06.html#clustgeo-package",
    "title": "Lesson 6: Geographic Segmentation with Spatial Clustering",
    "section": "ClustGeo Package",
    "text": "ClustGeo Package\nThe R package ClustGeo implements a Ward-like hierarchical clustering algorithm including spatial/geographical constraints.\n\nTwo dissimilarity matrices D0 and D1 are inputted, along with a mixing parameter alpha in [0,1]. The dissimilarities can be non-Euclidean and the weights of the observations can be non-uniform.\nThe first matrix gives the dissimilarities in the “feature space”” and the second matrix gives the dissimilarities in the “constraint space”.\nThe criterion minimized at each stage is a convex combination of the homogeneity criterion calculated with D0 and the homogeneity criterion calculated with D1. The idea is to determine a value of alpha which increases the spatial contiguity without deteriorating too much the quality of the solution based on the variables of interest i.e. those of the feature space."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#content",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#content",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Content",
    "text": "Content\n\nWhat is Predictive Modelling?\nWhat is Geospatial Predictive Modelling\nIntroducing Recursive Partitioning\nAdvanced Recursive Partitioning: Random Forest\nIntroducing Geographically Weighted Random Forest"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#what-is-predictive-modelling",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#what-is-predictive-modelling",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "What is Predictive Modelling?",
    "text": "What is Predictive Modelling?\n\n\n\nPredictive modelling uses statistical learning or machine learning techniques to predict outcomes.\n\nBy and large, the event one wants to predict is in the future. However, a set of known outcome and predictors (also known as variables) will be used to calibrate the predictive models."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#what-is-geospatial-predictive-modelling",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#what-is-geospatial-predictive-modelling",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "What is Geospatial Predictive Modelling",
    "text": "What is Geospatial Predictive Modelling\n\nGeospatial predictive modelling is conceptually rooted in the principle that the occurrences of events being modeled are limited in distribution.\n\nWhen geographically referenced data are used, occurrences of events are neither uniform nor random in distribution over space. There are geospatial factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur.\nGeospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#predictive-modelling-process",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#predictive-modelling-process",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Predictive Modelling Process",
    "text": "Predictive Modelling Process"
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#introducing-recursive-partitioning",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#introducing-recursive-partitioning",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Introducing recursive partitioning",
    "text": "Introducing recursive partitioning\n\nA predictive methodology involving a dependent variable y and one and more predictors.\nThe dependent variable can be either a continuous or categorical scales.\nRules partition data into mutually exclusive groups.\nNo need to worry about transformations such as logs.\nNo prior distribution requirement."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#advanced-recursive-partitioning-random-forest",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#advanced-recursive-partitioning-random-forest",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Advanced Recursive Partitioning: Random Forest",
    "text": "Advanced Recursive Partitioning: Random Forest\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble.\n\nEach individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction as shown the figure."
  },
  {
    "objectID": "lesson/Lesson08/Lesson08-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "href": "lesson/Lesson08/Lesson08-GWRF.html#introducing-geographically-weighted-random-forest-gwrf",
    "title": "Lesson 8: Geographically Weighted Predictive Modelling",
    "section": "Introducing Geographically Weighted Random Forest (gwRF)",
    "text": "Introducing Geographically Weighted Random Forest (gwRF)\n\nGeographically Weighted Random Forest (GRF) is a spatial analysis method using a local version of the famous Machine Learning algorithm.\n\nThis technique adopts the idea of the Geographically Weighted Regression.\nThe main difference between a tradition (linear) GWR and GRF is that we can model non-stationarity coupled with a flexible non-linear model which is very hard to overfit due to its bootstrapping nature, thus relaxing the assumptions of traditional Gaussian statistics."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#content",
    "href": "lesson/Lesson10/Lesson10-SIM.html#content",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Content",
    "text": "Content\n\nCharacteristics of Spatial Interaction Data\nSpatial Interaction Models\n\nUnconstrained\nOrigin constrined\nDestination constrained\nDoubly constrained\n\nWhat is Spatial Econometrics?\nWhat is Spatial Econometric Interaction Models?\nIntroducing spflow package\nSpatial Econometric Modelling of O-D Flows: spflow application"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-spatial-interaction-models-are",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-spatial-interaction-models-are",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What Spatial Interaction Models are?",
    "text": "What Spatial Interaction Models are?\n\n\nSpatial interaction or “gravity models” estimate the flow of people, material, or information between locations in geographical space.\n\n\n\n\n\n\n\n\nNote\n\n\nSpatial interaction models seek to explain existing spatial flows. As such it is possible to measure flows and predict the consequences of changes in the conditions generating them. When such attributes are known, it is possible to better allocate transport resources such as conveyances, infrastructure, and terminals."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#constructing-an-od-matrix",
    "href": "lesson/Lesson10/Lesson10-SIM.html#constructing-an-od-matrix",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Constructing an O/D Matrix",
    "text": "Constructing an O/D Matrix\n\nThe construction of an origin / destination matrix requires directional flow information between a series of locations.\nFigure below represents movements (O/D pairs) between five locations (A, B, C, D and E). From this graph, an O/D matrix can be built where each O/D pair becomes a cell. A value of 0 is assigned for each O/D pair that does not have an observed flow."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#three-basic-types-of-interaction-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#three-basic-types-of-interaction-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Three Basic Types of Interaction Models",
    "text": "Three Basic Types of Interaction Models\n\nThe general formulation of the spatial interaction model is stated as Tij, which is the interaction between location i (origin) and location j (destination). Vi are the attributes of the location of origin i, Wj are the attributes of the location of destination j, and Sij are the attributes of separation between the location of origin i and the location of destination j.\nFrom this general formulation, three basic types of interaction models can be derived:"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#gravity-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#gravity-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Gravity Models",
    "text": "Gravity Models\n\n\n\nThe general formula (also known as unconstrained):\n\n\nTij is the transition/trip or flow, 𝑇, between origin 𝑖 (always the rows in a matrix) and destination 𝑗 (always the columns in a matrix). If you are not overly familiar with matrix notation, the 𝑖 and 𝑗 are just generic indexes to allow us to refer to any cell in the matrix.\n𝑉 is a vector (a 1 dimensional matrix – or, if you like, a single line of numbers) of origin attributes which relate to the emissivity of all origins in the dataset, 𝑖 – this could be any of the origin-related variables.\n\n\n\n𝑊 is a vector of destination attributes relating to the attractiveness of all destinations in the dataset, 𝑗 – similarly, this could be any of the destination related variables.\n𝑑 is a matrix of costs (frequently distances – hence, d) relating to the flows between 𝑖 and 𝑗.\n𝑘, 𝜇, 𝛼 and 𝛽 are all model parameters to be estimated. 𝛽 is assumed to be negative, as with an increase in cost/distance we would expect interaction to decrease."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-models",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-models",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Interaction Models",
    "text": "Spatial Econometric Interaction Models\nLimitation of Spatial Interaction Models\n\nThe gravity model assumes independence among observations, and this assumption seems heroic for many fundamentally spatial problems."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-model-for-origin-destination-flows",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-model-for-origin-destination-flows",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Model for Origin-Destination Flows",
    "text": "Spatial Econometric Model for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-model",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-econometric-interaction-model",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Econometric Interaction Model",
    "text": "Spatial Econometric Interaction Model\nThe general formula of Spatial Econometric Interaction Model is defined as follow:\n\nwhere by 𝐖𝓭, 𝐖𝒐 and 𝐖𝓌 are spatial weights of destination, origin and origin-destination."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Econometrics",
    "text": "What is Econometrics\n\nEconometrics is an application of statistical methods to economic data in order to give empirical content to economic relationships. More precisely, it is “the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference.\nA basic tool for econometrics is the multiple linear regression model.\nEconometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods.\nEconometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency.\nApplied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Spatial Econometrics?",
    "text": "What is Spatial Econometrics?\n\nA branch of economics that deals with the study of economic phenomena that exhibit spatial dependence.\nThis branch of economics has its roots in classical economics, which focused on the study of how economic activity was related to the location of factors of production.\nClassical economists developed theories of how businesses locate themselves in relation to their markets and to each other. These theories formed the basis for the development of modern spatial economics."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics-1",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-is-spatial-econometrics-1",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What is Spatial Econometrics?",
    "text": "What is Spatial Econometrics?\n\nIn a broader sense it is inclusive of the models and theoretical instruments of spatial statistics and spatial data analysis to analyze various economic effects such as externalities, interactions, spatial concentration and many others.\nDiscrete spatial data can take the form of points, lines and polygons. Point data refer to the position of the single economic agent observed at an individual level. Lines in space take the form of interactions between two spatial locations such as flows of goods, individuals and information. Finally data observed within polygons can take the form of predefined irregular portions of space, usually administrative partitions such as countries, regions or counties within one country."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#what-are-the-examples-of-applications-using-spatial-econometrics",
    "href": "lesson/Lesson10/Lesson10-SIM.html#what-are-the-examples-of-applications-using-spatial-econometrics",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "What Are The Examples of Applications Using Spatial Econometrics ?",
    "text": "What Are The Examples of Applications Using Spatial Econometrics ?\nThere are many applications for spatial econometrics . Here are a few examples :\n\nEvaluating the impact of a new road or railway on property values.\nEstimating the effect of environmental regulations on firm location decisions.\nAnalyzing the determinants of crime rates across neighborhoods\nStudying the relationship between house prices and income levels in different regions.\nInvestigating the spread of infectious diseases through a population.\nModeling the relationship between land values and location-specific services.\nAnalyzing the relationship between proximity to facilities and job opportunities ."
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-1",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-1",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-2",
    "href": "lesson/Lesson10/Lesson10-SIM.html#spatial-model-specification-for-origin-destination-flows-2",
    "title": "Lesson 10: Spatial Interaction Models",
    "section": "Spatial Model Specification for Origin-Destination Flows",
    "text": "Spatial Model Specification for Origin-Destination Flows"
  },
  {
    "objectID": "outline/Lesson01_outline.html",
    "href": "outline/Lesson01_outline.html",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of three parts. First, it provides an overview of geospatial analytics. Second, it explains the popular geospatial models used to store geographical data. The methods used to import, integrate, wrangle, process geospatial data will be discussed too. Lastly, the basic principles and concepts of thematic mapping and geovisualisation will be introduced.\nThe hands-on exercises will allow you to gaion hands-on experience on using:\n\nsf package to import and wrangle vector-based data,\nterra package to import and wrangle raster-based data, and\ntmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson01_outline.html#overview",
    "href": "outline/Lesson01_outline.html#overview",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "",
    "text": "This lesson consists of three parts. First, it provides an overview of geospatial analytics. Second, it explains the popular geospatial models used to store geographical data. The methods used to import, integrate, wrangle, process geospatial data will be discussed too. Lastly, the basic principles and concepts of thematic mapping and geovisualisation will be introduced.\nThe hands-on exercises will allow you to gaion hands-on experience on using:\n\nsf package to import and wrangle vector-based data,\nterra package to import and wrangle raster-based data, and\ntmap package to build cartographic quality thematic maps."
  },
  {
    "objectID": "outline/Lesson01_outline.html#content",
    "href": "outline/Lesson01_outline.html#content",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Content",
    "text": "Content\n\nIntroduction to Geospatial Analytics\n\nDemystifying Geospatial Analytics\nMotivation of Geospatial Analytics\nA Tour Through the Geospatial Analytics Zoo\nGeospatial Analytics and Social Consciousness\n\nFundamentals of Geospatial Data Models\n\nVector and raster data model\nCoordinate systems and map projection\nHandling and wrangling vector data in R: sf methods ````- Handling and wrangling raster data in R: terra methods\n\nFundamentals of Geospatial Data Visualisation and tmap Methods\n\nClassification of maps\nPrinciples of map design\nThematic mapping techniques\ntmap methods"
  },
  {
    "objectID": "outline/Lesson01_outline.html#lesson-slides",
    "href": "outline/Lesson01_outline.html#lesson-slides",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Lesson Slides",
    "text": "Lesson Slides\n\nLesson 1: Introduction to Geospatial Analytics slides.\nLesson 1: Fundamentals of Geospatial Data Models and Modelling slides.\nLesson 1: Fundamentals of Geospatial Data Visualisation and tmap Methods slides."
  },
  {
    "objectID": "outline/Lesson01_outline.html#self-reading-before-lesson",
    "href": "outline/Lesson01_outline.html#self-reading-before-lesson",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Self-reading Before Lesson",
    "text": "Self-reading Before Lesson\n\n“Spatial Data, Spatial Analysis, Spatial Data Science” by Prof. Luc Anselin. (This is a long lecture 1hr 15minutes but don’t turn away just because it is lengthy.)\nXie, Yiqun et. al. (2017) “Transdisciplinary Foundations of Geospatial Data Science” ISPRS International Journal of Geo-information, 2017, Vol.6 (12), p.395."
  },
  {
    "objectID": "outline/Lesson01_outline.html#hands-on-exercise",
    "href": "outline/Lesson01_outline.html#hands-on-exercise",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "Hands-on Exercise",
    "text": "Hands-on Exercise\n\nHands-on Exercise 1: Geospatial Data Wrangling with R\nHands-on Exercise 1: Choropleth Mapping with R"
  },
  {
    "objectID": "outline/Lesson01_outline.html#all-about-r",
    "href": "outline/Lesson01_outline.html#all-about-r",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "All About R",
    "text": "All About R\n\nR packages for Data Science\n\nsf package.\n\nSimple Features for R\nReading, Writing and Converting Simple Features\nManipulating Simple Feature Geometries\nManipulating Simple Features\n\ntidyverse: a family of modern R packages specially designed to meet the tasks of Data Science in R.\n\nreadr: a fast and effective library to parse csv, txt, and tsv files as tibble data.frame in R. To get started, refer to Chapter 11 Data import of R for Data Science book.\n\ntidyr: an R package for tidying data. To get started, refer to Chapter 5 Data tidying of R for Data Science book.\n\ndplyr: a grammar of data manipulation. To get started, read articles under Getting Started and Articles tabs.\nggplot2: a grammar of graphics. To get started, read Chapter 1: Data Visualization, Chapter 10 Exploratory Data Analysis and Chapter 11 Communication of R for Data Science (2ed) book.\npipes: a powerful tool for clearly expressing a sequence of multiple operations. To get started, read Chapter 5 Workflow: pipes of R for Data Science (2ed) book.\n\n\n\n\nR Package for GeoVisualisation and Thematic Mapping\n\nTennekes, M. (2018) “tmap: Thematic Maps in R”, Journal of Statistical Software, Vol 84:6, 1-39.\ntmap: thematic maps in R package especially:\n\ntmap: get started!,\ntmap: version changes, and\nChapter 8 Making maps with R of Geocomputation with R."
  },
  {
    "objectID": "outline/Lesson01_outline.html#references",
    "href": "outline/Lesson01_outline.html#references",
    "title": "Lesson 1: Introduction to Geospatial Analytics",
    "section": "References",
    "text": "References\n\nGeospatial Analytics\n\nPaez, A., and Scott, D.M. (2004) “Spatial statistics for urban analysis: A review of techniques with examples”, GeoJournal, 61: 53-67. Available in SMU eLibrary.\n“Geospatial Analytics Will Eat The World, And You Won’t Even Know It”.\n\n\n\nGeoVisualisation and Thematic Mapping\n\nProportional Symbols\nChoropleth Maps\nThe Basics of Data Classification\nChoropleth Mapping with Exploratory Data Analysis\nThe Concept of Map Symbols\nChoropleth map\nChoropleth Maps – A Guide to Data Classification\nBivariate Choropleth\nValue-by-alpha maps\nWhat to consider when creating choropleth maps"
  },
  {
    "objectID": "outline/Lesson03_outline.html",
    "href": "outline/Lesson03_outline.html",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "",
    "text": "In this lesson, you will learn two advanced spatial point patterns analysis methods, they are: spatiotemporal KDE and the Network Constrained KDE. Using real-world use cases, you will also gain hands-on experience on using spNetwork to analyse spatial point patterns and temporal spatial point event along networks."
  },
  {
    "objectID": "outline/Lesson03_outline.html#content",
    "href": "outline/Lesson03_outline.html#content",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "Content",
    "text": "Content\n\nSpatiotemporal Kernel Density Estimation\nNetwork Constrained Kernel Density Estimation (NCKDE)\n\nBasic concepts of network constrained spatial point patterns\nNetwork Constrained KDE methods\nThe Three versions of Network Constrained KDE\n\nTemporal Network Kernel Density Estimation (TNKED)\n\nTemporal dimension\nSpatial dimension\nSpatiotemporal point patterns\nThe Temporal Network Kernel Density Estimation method\n\n\n\nLesson Slides and Hands-on Notes\n\nLesson 3 slides\nHands-on Exercise 3: Network Constrained Spatial Point Patterns Analysis."
  },
  {
    "objectID": "outline/Lesson03_outline.html#references",
    "href": "outline/Lesson03_outline.html#references",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "References",
    "text": "References\n\nATSUYUKI OKABE, TOSHIAKI SATOH & KOKICHI SUGIHARA (2009) “A kernel density estimation method for networks, its computational method and a GIS-based tool”, International Journal of Geographical Information Science, Vol. 23, No. 1, January 2009, pp. 7–32.\nIkuho Yamada & Jean-Claude Thill (2007) “Local Indicators of Network-Constrained Clusters in Spatial Point Patterns”, Geographical Analysis, Vol. 39, pp 268–292.\nJérémy Gelb & Philippe Apparicio (2023) “Temporal Network Kernel Density Estimation”, Geographical Analysis. (Online open access version)"
  },
  {
    "objectID": "outline/Lesson03_outline.html#all-about-r",
    "href": "outline/Lesson03_outline.html#all-about-r",
    "title": "Lesson 3: Advanced Spatial Point Patterns Analysis",
    "section": "All About R",
    "text": "All About R\n\nspNetwork: An R package to perform spatial analysis on networks.\n\nDetails about NKDE\nNetwork k Functions\nNetwork Kernel Density Estimate\nTemporal Network Kernel Density Estimate"
  },
  {
    "objectID": "outline/Lesson05_outline.html",
    "href": "outline/Lesson05_outline.html",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "In this lesson, you will learn a collection of geospatial statistical methods specially designed for measuring global and local spatial association.\nThese spatial statistics are well suited for:"
  },
  {
    "objectID": "outline/Lesson05_outline.html#content",
    "href": "outline/Lesson05_outline.html#content",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Content",
    "text": "Content\n\nWhat is Spatial Autocorrelation\n\nMeasures of Global Spatial Autocorrelation\nMeasures of Global High/Low Clustering\n\nIntroducing Localised Geospatial Analysis\n\nLocal Indicators of Spatial Association (LISA)\n\nCluster and Outlier Analysis\n\nLocal Moran and Local Geary\nMoran scatterplot\nLISA Cluster Map\n\nHot Spot and Cold Spot Areas Analysis\n\nGetis and Ord’s G-statistics\n\nEmerging Hot Spot Analysis (EHSA)"
  },
  {
    "objectID": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson05_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 5 slides.\nHands-on Exercise 5: Global Measures of Spatial Autocorrelation.\nHands-on Exercise 5: Local Measures of Spatial Autocorrelation"
  },
  {
    "objectID": "outline/Lesson05_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson05_outline.html#self-reading-before-meet-up",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nMoran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena”. Biometrika. 37 (1): 17–23.\nGeary, R.C. (1954) “The Contiguity Ratio and Statistical Mapping”. The Incorporated Statistician, Vol. 5, No. 3, pp. 115-127.\nGetis, A., & Ord, K. (1992). “The Analysis of Spatial Association by Use of Distance Statistics”. Geographical Analysis, 24, 189–206.\nAnselin, L. (1995). “Local indicators of spatial association – LISA”. Geographical Analysis, 27(4): 93-115.\nGetis, A. and Ord, J.K. (1992) “The analysis of spatial association by use of distance statistics”. Geographical Analysis, 24(3): 189-206.\nOrd, J.K. and Getis, A. (2010) “Local spatial autocorrelation statistics: Distributional issues and an application”. Geographical Analysis, 27(4): 286-306.\n\nThese six papers are classics of Global and Local Spatial Autocorrelation. Be warned: All classic papers assume that the readers are academic researchers."
  },
  {
    "objectID": "outline/Lesson05_outline.html#references",
    "href": "outline/Lesson05_outline.html#references",
    "title": "Lesson 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "References",
    "text": "References\n\nD. A. Griffith (2009) “Spatial autocorrelation”.\nGetis, A., 2010 “B.3 Spatial Autocorrelation” in Fischer, M.M., and Getis, A. 2010 Handbook of Applied Spatial Analysis: Software Tools, Methods and Applications, Springer.\nAnselin, L. (1996) “The Moran scatterplot as an ESDA tool to assess local instability in spatial association”\nGriffith, Daniel (2009) “Modeling spatial autocorrelation in spatial interaction data: empirical evidence from 2002 Germany journey-to-work flows”. Journal of Geographical Systems, Vol.11(2), pp.117-140.\nCelebioglu, F., and Dall’erba, S. (2010) “Spatial disparities across the regions of Turkey: An exploratory spatial data analysis”. The Annals of Regional Science, 45:379–400.\nMack, Z.W.V. and Kam T.S. (2018) “Is There Space for Violence?: A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict” Proceedings of 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities (ACM SIGSPATIAL’18). Seattle, Washington, USA, 10 pages.\nTAN, Yong Ying and KAM, Tin Seong (2019). “Exploring and Visualizing Household Electricity Consumption Patterns in Singapore: A Geospatial Analytics Approach”, Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31–April 3, 2019, Proceedings. Pp 785-796.\nSingh A., Pathak P.K., Chauhan R.K., and Pan, W. (2011) “Infant and Child Mortality in India in the Last Two Decades: A Geospatial Analysis”. PLoS ONE 6(11), 1:19."
  },
  {
    "objectID": "outline/Lesson07_outline.html",
    "href": "outline/Lesson07_outline.html",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "",
    "text": "In this lesson, you will learn the basic concepts and methods of geographically weighted regression."
  },
  {
    "objectID": "outline/Lesson07_outline.html#content",
    "href": "outline/Lesson07_outline.html#content",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Content",
    "text": "Content\n\nBasic concepts and principles of linear regression\n\nSimple linear regression\nMultiple linear regression\n\nThe spatial stationarity assumption of multiple linear regression.\nIntroducing Geographically Weighted Regression\n\nWeighting functions (kernel)\nWeighting schemes\nBandwidth\nInterpreting and Visualising"
  },
  {
    "objectID": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson07_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 7 slides.\nHands-on Exercise 7."
  },
  {
    "objectID": "outline/Lesson07_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson07_outline.html#self-reading-before-meet-up",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nTo read before class:\n\nBrunsdon, C., Fotheringham, A.S., and Charlton, M. (2002) “Geographically weighted regression: A method for exploring spatial nonstationarity”. Geographical Analysis, 28: 281-289.\nBrunsdon, C., Fotheringham, A.S. and Charlton, M., (1999) “Some Notes on Parametric Significance Tests for Geographically Weighted Regression”. Journal of Regional Science, 39(3), 497-524."
  },
  {
    "objectID": "outline/Lesson07_outline.html#references",
    "href": "outline/Lesson07_outline.html#references",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "References",
    "text": "References\n\nMennis, Jeremy (2006) “Mapping the Results of Geographically Weighted Regression”, The Cartographic Journal, Vol.43 (2), p.171-179.\nStephen A. Matthews ; Tse-Chuan Yang (2012) “Mapping the results of local statistics: Using geographically weighted regression”, Demographic Research, Vol.26, p.151-166."
  },
  {
    "objectID": "outline/Lesson07_outline.html#all-about-r",
    "href": "outline/Lesson07_outline.html#all-about-r",
    "title": "Lesson 7: Geographically Weighted Regression",
    "section": "All About R",
    "text": "All About R\n\nGWmodel package, especially\n\nGollini, I et. al. (2015) “GWmodel: An R Package for Exploring Spatial Heterogeneity Using Geographically Weighted Models”, Journal of Statistical Software, Volume 63, Issue 17 and\nBinbin Lu, Paul Harris, Martin Charlton & Chris Brunsdon (2014) “The GWmodel R package: further topics for exploring spatial heterogeneity using geographically weighted models”, Geo-spatial Information Science, 17:2, 85-101, DOI: 10.1080/10095020.2014.917453.\n\nlctools package especially gw() and gwr() related functions.\nspgwr implements of geographically weighted regression methods for exploring possible non-stationarity.\ngwrr: its geographically weighted regression (GWR) models and has tools to diagnose and remediate collinearity in the GWR models. Also fits geographically weighted ridge regression (GWRR) and geographically weighted lasso (GWL) models."
  },
  {
    "objectID": "outline/Lesson09_outline.html",
    "href": "outline/Lesson09_outline.html",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geographic Accessibility\nGravitational Law and Distance Decay Function\nIntroducing Potential Models\n\nThe classic model\n\nHansen Potential Accessibility Model\nTwo-step Floating Catchment Area (2SFCA) Method\nKernel Density Two-Step Floating Catchment Area (KD2SFCA) Method\nInterpreting and Visualising Modelling Results"
  },
  {
    "objectID": "outline/Lesson09_outline.html#content",
    "href": "outline/Lesson09_outline.html#content",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "",
    "text": "Basic Concepts of Geographic Accessibility\nGravitational Law and Distance Decay Function\nIntroducing Potential Models\n\nThe classic model\n\nHansen Potential Accessibility Model\nTwo-step Floating Catchment Area (2SFCA) Method\nKernel Density Two-Step Floating Catchment Area (KD2SFCA) Method\nInterpreting and Visualising Modelling Results"
  },
  {
    "objectID": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "href": "outline/Lesson09_outline.html#lesson-slides-and-hands-on-notes",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Lesson Slides and Hands-on Notes",
    "text": "Lesson Slides and Hands-on Notes\n\nLesson 9 slides.\nHands-on Exercise 9 handout."
  },
  {
    "objectID": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "href": "outline/Lesson09_outline.html#self-reading-before-meet-up",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Self-reading Before Meet-up",
    "text": "Self-reading Before Meet-up\nRead before lesson:\n\nLong, J. (2017). Modelling Accessibility. The Geographic Information Science & Technology Body of Knowledge (3rd Quarter 2017 Edition), John P. Wilson (ed.).\nHansen, W. G. (1959): “How Accessibility Shapes Land Use”. Journal of the American Institute of Planners, 25, 2, p. 73-76.\nLuo, W.; Wang, F. (2003) “Measures of spatial accessibility to health care in a GIS environment: synthesis and a case study in the Chicago region”. Environment and Planning B: Planning and Design. 30 (6): 865–884. doi:10.1068/b29120.\nLuo, W.; Qi, Y. (2009). “An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians”. Health & Place. 15 (4): 1100–1107."
  },
  {
    "objectID": "outline/Lesson09_outline.html#reference",
    "href": "outline/Lesson09_outline.html#reference",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "Reference",
    "text": "Reference\n\nSection on “Transportation and Accessibility” in The Geography of Transport Systems.\nRich, D.C. (1980) Potential Models in Human Geography.\nOrpana, T./Lampinen, J. (2003) “Building spatial choice models from aggregate data”. Journal of Regional Science,43, 2, p. 319-347.\nTwo-step floating catchment area method.\nCheng, Gang et. al. (2016) “Spatial difference analysis for accessibility to high level hospitals based on travel time in Shenzhen, China” Habitat International, Vol.53, p.485-494.\nPolzin, Pierre ; Borges, José ; Coelho, António (2014) “An Extended Kernel Density Two-Step Floating Catchment Area Method to Analyze Access to Health Care” Environment and planning. B, Planning & design, Vol.41 (4), p.717-735."
  },
  {
    "objectID": "outline/Lesson09_outline.html#all-about-r",
    "href": "outline/Lesson09_outline.html#all-about-r",
    "title": "Lesson 9: Modelling Geographic of Accessibility",
    "section": "All About R:",
    "text": "All About R:\n\naccessibility\nSpatialAcc package.\nPotential package."
  },
  {
    "objectID": "outline/workshop.html",
    "href": "outline/workshop.html",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/workshop.html#content",
    "href": "outline/workshop.html#content",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "",
    "text": "Motivation of developing web-enabled applications\n\nBasic principles of web applications\n\nIntroducing R Shiny\n\nGetting to know Shiny\nArchitecture of Shiny\nBuilding block of Shiny app\n\nAdvanced R Shiny\n\nReactive feature of Shiny\nBuild interactive Shiny application by using plotly R\nBuild static, interactive and reactive geovisualisation application by using tmap\n\nManaging R Shiny Project\n\nThe basic development cycle of creating apps\nDebug errors in the codes\nBuild complex Shiny application using module\nImprove the productivity of Shiny applications development\n\nDeploying Shiny Application\n\nDeploy to the cloud: shinyapps.io\nDeploy on-premises (open source): Shiny Server\nDeploy on-premises (commercial): RStudio Connect"
  },
  {
    "objectID": "outline/workshop.html#workshop-slides-and-hands-on-notes",
    "href": "outline/workshop.html#workshop-slides-and-hands-on-notes",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Workshop Slides and Hands-on Notes",
    "text": "Workshop Slides and Hands-on Notes\n\nIntroducing R Shiny slides in html and pdf formats\nAdvanced R Shiny I slides in html and pdf formats\nManaging R Shiny Project slides in html and pdf formats"
  },
  {
    "objectID": "outline/workshop.html#must-do",
    "href": "outline/workshop.html#must-do",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Must Do!",
    "text": "Must Do!\n\nView the three parts series of Shiny Tutorial at this link."
  },
  {
    "objectID": "outline/workshop.html#readings",
    "href": "outline/workshop.html#readings",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Readings",
    "text": "Readings\n\nCore Readings\n\nShiny reference guide at CRAN.\nHadley Wickham (2021) Mastering Shiny, O’Reilly Media. This is a highly recommended book. You can find the online version with this link.\nPaula Moraga (2020) Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny, Chapman & Hall/CRC. Chapter 13, 14 and 15.\n\n\n\nAdditional references\n\nColin Fay, Sébastien Rochette, Vincent Guyader, Cervan Girard (2020), Engineering Production-Grade Shiny Apps, Chapman & Hall. You can find the online version with this link.\nOutstanding User Interfaces with Shiny.\nHow to Build a Shiny Application from Scratch.\n\n\n\nDeploying Shiny Application on shinyapps.io\n\nGetting started with shinyapps.io\nshinyapps.io user guide"
  },
  {
    "objectID": "outline/workshop.html#gallery",
    "href": "outline/workshop.html#gallery",
    "title": "Workshop on Building Web-enabled Geospatial Analytics Applications with R Shiny",
    "section": "Gallery",
    "text": "Gallery\n\nWinners of the 1st Shiny Contest\nWinners of the 2nd Annual Shiny Contest\nWinners of the 3rd annual Shiny Contest\nShiny Gallery\nFifteen New Zealand government Shiny web apps\nShinyApps Gallery\ndreamRs shiny gallery\nTools for Teaching Quantitative Thinking"
  },
  {
    "objectID": "Take-home_Ex01.html",
    "href": "Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively."
  },
  {
    "objectID": "Take-home_Ex01.html#setting-the-scene",
    "href": "Take-home_Ex01.html#setting-the-scene",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "",
    "text": "According to World Health Organisation (WHO), road traffic accidents cause the death of approximately 1.19 million people each year leave between 20 and 50 million people with non-fatal injuries. More than half of all road traffic deaths occur among vulnerable road users, such as pedestrians, cyclists and motorcyclists.\nRoad traffic injuries are the leading cause of death for children and young adults aged 5–29. Yet two thirds of road traffic fatalities occur among people of working age (18–59 years). Nine in 10 fatalities on the roads occur in low- and middle-income countries, even though these countries have around 60% of the world’s vehicles.\nIn addition to the human suffering caused by road traffic injuries, they also incur a heavy economic burden on victims and their families, both through treatment costs for the injured and through loss of productivity of those killed or disabled. More broadly, road traffic injuries have a serious impact on national economies, costing countries 3% of their annual gross domestic product.\nThailand’s roads are the deadliest in Southeast Asia and among the worst in the world, according to the World Health Organisation. About 20,000 people die in road accidents each year, or about 56 deaths a day (WHO).\nBetween 2014 and 2021, Thailand experienced a notable increase in accident frequencies. Specifically, 19% of all accidents in Thailand occurred on the national highways, which constituted the primary public thoroughfares connecting various regions, provinces, districts, and significant locations within a comprehensive network. Within the broader context of accidents across the country, there existed a considerable 66% likelihood of encountering accident-prone zones, often termed ‘black spots,’ distributed as follows: 66% on straight road segments, 13% at curves, 6% at median points of cross-shaped intersections, 5% at T-shaped intersections and Y-shaped intersections, 3% at cross-shaped intersections, 2% on bridges, and 2% on steep slopes, respectively."
  },
  {
    "objectID": "Take-home_Ex01.html#objectives",
    "href": "Take-home_Ex01.html#objectives",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Objectives",
    "text": "Objectives\nBy and large, road traffic accidents can be attributed by two major factors, namely: behavioural and environmental factors. Behavioural factors in driving are considered to be major causes of traffic accidents either in direct or indirect manner (Lewin, 1982). These factors can be further grouped into two as, driver behavior (also called driver/driving style) and driver performance, in other words, driver/driving skills (Elander, West, & French, 1993). Environmental factors, on the other hand, includes but not limited to weather condition such as poor visibility during heavy rain or foggy and road conditions such as sharp bend road, slippery slope road, and blind spot.\nPrevious studies have demonstrated the significant potential of Spatial Point Patterns Analysis (SPPA) in exploring and identifying factors influencing road traffic accidents. However, these studies often focus solely on either behavioral or environmental factors, with limited consideration of temporal factors such as season, day of the week, or time of day.\nIn view of this, you are tasked to discover factors affecting road traffic accidents in the Bangkok Metropolitan Region BMR by employing both spatial spatio-temporal point patterns analysis methods.\nThe specific objectives of this take-home exercise are as follows:\n\nTo visualize the spatio-temporal dynamics of road traffic accidents in BMR using appropriate statistical graphics and geovisualization methods.\nTo conduct detailed spatial analysis of road traffic accidents using appropriate Network Spatial Point Patterns Analysis methods.\nTo conduct detailed spatio-temporal analysis of road traffic accidents using appropriate Temporal Network Spatial Point Patterns Analysis methods."
  },
  {
    "objectID": "Take-home_Ex01.html#the-data",
    "href": "Take-home_Ex01.html#the-data",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this exercise, three basic data sets must be used, they are:\n\nThailand Road Accident [2019-2022] on Kaggle\nThailand Roads (OpenStreetMap Export) on HDX.\nThailand - Subnational Administrative Boundaries on HDX.\n\nStudents are free to include other data sets if they help in the study."
  },
  {
    "objectID": "Take-home_Ex01.html#grading-criteria",
    "href": "Take-home_Ex01.html#grading-criteria",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability to employ appropriate R functions from various R packages specifically designed for modern data science such as readr, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the entire geospatial data wrangling processes, including. This is not limited to data import, data extraction, data cleaning and data transformation. Besides assessing your ability to use the R functions, this criterion also includes your ability to clean and derive appropriate variables to meet the analysis need.\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them).\n\n\n\nGeospatial Analysis (25 marks): In this exercise, you are expected to utilize the geospatial analytics methods introduced in class, along with the R packages provided during the hands-on exercises, to perform your analysis. You will be assessed on your ability to apply these methods correctly and to provide accurate interpretations and discussions of the analysis results.\nGeovisualisation and Geocommunication (25 marks): In this section, your ability to effectively communicate complex geospatial analysis results through user-friendly visual representations will be assessed. Since this course is focused on geospatial analysis, it is crucial that you demonstrate proficiency in using appropriate geovisualization techniques to clearly convey the findings of your analysis.\nReproducibility (20 marks): This is a key learning outcome of this course. You will be assessed on your ability to thoroughly document the analysis procedures using code chunks within Quarto. It is important to note that simply providing the code chunks is insufficient; you must also include explanations of the purpose behind each step and the R function(s) used.\nBonus (10 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data. The methods used must be geospatial in nature."
  },
  {
    "objectID": "Take-home_Ex01.html#submission-instructions",
    "href": "Take-home_Ex01.html#submission-instructions",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nThe R project of the take-home exercise must be pushed onto your Github repository.\nYou are required to provide the links to Netlify service of the take-home exercise write-up and github repository on eLearn."
  },
  {
    "objectID": "Take-home_Ex01.html#due-date",
    "href": "Take-home_Ex01.html#due-date",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Due Date",
    "text": "Due Date\n22nd September 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex01.html#references",
    "href": "Take-home_Ex01.html#references",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "References",
    "text": "References\n\nWHO (2023) Road traffic injuries\nRoad traffic deaths and injuries in Thailand\nLewin, I. (1982). Driver training: A perceptual-motor skill approach. Ergonomics, 25(10), 917–924.\nElander, J., West, R., & French, D. (1993). Behavioral correlates of individual differences in road-traffic crash risk: An examination of methods and findings. Psychological Bulletin, 113(2), 279."
  },
  {
    "objectID": "Take-home_Ex01.html#survival-tips",
    "href": "Take-home_Ex01.html#survival-tips",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Survival Tips",
    "text": "Survival Tips"
  },
  {
    "objectID": "Take-home_Ex01.html#learning-from-seniors",
    "href": "Take-home_Ex01.html#learning-from-seniors",
    "title": "Take-home Exercise 1: Geospatial Analytics for Public Good",
    "section": "Learning from seniors",
    "text": "Learning from seniors\n\nCHAI ZHIXUAN  This is one of the two submission that includes steps on how to download the Passenger O-D data by using LTA DataMall API and opensource Postmen. Refer to sub-section 3.1.1 Aspatial data. Although it is incomplete (Step 3 :)) but still one of the best.\n\nKRISTINE JOY PAAS  Have done well in all five grading criteria especially the reproducibility, geovisualisation and geocommunication criteria. Geospatial Analytics criterion can be improved by including a paragraph describing the purpose, concepts and methods of the geospatial analytics used.\nKYLIE TAN JING YI  Section 5: Spatial Association Analysis of this submission provides a comprehensive discussion of the methods used and analysis results.\nMUHAMAD AMEER NOOR  Have done well in all five grading criteria including a short write-up of the geospatial analytics methods used.\nNEO YI XIN This submission put function programming of R into good used. For example, subsection Processing the aspatial OD data for processing data with same structure repetitively, Task 1: Geovisulisation and Analysis to ensure that a same classification scale are used. Further more Sub-section Computing Distance-Based Spatial Weights Matrix serves as a good example on how to discussion geospatial analytics methods used. There are at least three other students did show the spatial weights map but they are way too messy."
  },
  {
    "objectID": "Take-home_Ex03a.html",
    "href": "Take-home_Ex03a.html",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "",
    "text": "Important\n\n\n\nThis handout provides the context, the task, the expectation and the grading criteria of Take-home Exercise 3. Students must review and understand them before getting started with the take-home exercise."
  },
  {
    "objectID": "Take-home_Ex03a.html#setting-the-scene",
    "href": "Take-home_Ex03a.html#setting-the-scene",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nAccording to Wikipedia, financial inclusion is the availability and equality of opportunities to access financial services. It refers to processes by which individuals and businesses can access appropriate, affordable, and timely financial products and services - which include banking, loan, equity, and insurance products. It provides paths to enhance inclusiveness in economic growth by enabling the unbanked population to access the means for savings, investment, and insurance towards improving household income and reducing income inequality."
  },
  {
    "objectID": "Take-home_Ex03a.html#the-task",
    "href": "Take-home_Ex03a.html#the-task",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "The Task",
    "text": "The Task\nIn this take-home exercise, you are required to build an explanatory model to determine factors affecting financial inclusion by using geographical weighted regression methods."
  },
  {
    "objectID": "Take-home_Ex03a.html#the-data",
    "href": "Take-home_Ex03a.html#the-data",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "The Data",
    "text": "The Data\nFor the purpose of this take-home exercise, either FinScope Uganda 2023 or FinScope Tanzania 2023 should be used. The study should be conducted at the district level. The district level boundary GIS data can be downloaded from geoBoundaries portal."
  },
  {
    "objectID": "Take-home_Ex03a.html#grading-criteria",
    "href": "Take-home_Ex03a.html#grading-criteria",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Grading Criteria",
    "text": "Grading Criteria\nThis exercise will be graded by using the following criteria:\n\nGeospatial Data Wrangling (20 marks): This is an important aspect of geospatial analytics. You will be assessed on your ability:\n\nto employ appropriate R functions from various R packages specifically designed for modern data science such as readxl, tidyverse (tidyr, dplyr, ggplot2), sf just to mention a few of them, to perform the import and extract the data.\nto clean and derive appropriate variables for meeting the analysis need.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll data are like vast grassland full of land mines. Your job is to clear those mines and not to step on them.\n\n\n\nGeospatial Analysis (30 marks): In this exercise, you are expected to use the appropriate non-spatial regression method and geographically weighted regression methods learned in Lesson 7 to perform the analysis. You will be assessed on your ability:\n\nto describe the methods used correctly including model diagnostics, and\nto provide accurate interpretation of the analysis results.\n\nGeovisualisation and geocommunication (20 marks): In this section, you will be assessed on your ability to communicate the results in business friendly visual representations. This course is geospatial centric, hence, it is important for you to demonstrate your competency in using appropriate geovisualisation techniques to reveal and communicate the findings of your analysis.\nReproducibility (15 marks): This is an important learning outcome of this exercise. You will be assessed on your ability to provide a comprehensive documentation of the analysis procedures in the form of code chunks of Quarto. It is important to note that it is not enough by merely providing the code chunk without any explanation on the purpose and R function(s) used.\nBonus (15 marks): Demonstrate your ability to employ methods beyond what you had learned in class to gain insights from the data."
  },
  {
    "objectID": "Take-home_Ex03a.html#submission-instructions",
    "href": "Take-home_Ex03a.html#submission-instructions",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Submission Instructions",
    "text": "Submission Instructions\n\nThe write-up of the take-home exercise must be in Quarto html document format. You are required to publish the write-up on Netlify.\nZip the take-home exercise folder and upload it onto eLearn. If the size of the zip file is beyond the capacity of eLearn, you can upload it on SMU OneDrive and provide the download link on eLearn.."
  },
  {
    "objectID": "Take-home_Ex03a.html#due-date",
    "href": "Take-home_Ex03a.html#due-date",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Due Date",
    "text": "Due Date\n10th November 2024 (Sunday), 11.59pm (midnight)."
  },
  {
    "objectID": "Take-home_Ex03a.html#learning-from-senior",
    "href": "Take-home_Ex03a.html#learning-from-senior",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Learning from senior",
    "text": "Learning from senior\nYou are advised to review these sample submissions prepared by your seniors."
  },
  {
    "objectID": "Take-home_Ex03a.html#q-a",
    "href": "Take-home_Ex03a.html#q-a",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Q & A",
    "text": "Q & A\nPlease submit your questions or queries related to this take-home exercise on Piazza."
  },
  {
    "objectID": "Take-home_Ex03a.html#peer-learning",
    "href": "Take-home_Ex03a.html#peer-learning",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Peer Learning",
    "text": "Peer Learning"
  },
  {
    "objectID": "Take-home_Ex03a.html#reference",
    "href": "Take-home_Ex03a.html#reference",
    "title": "Take-home Exercise 3a: Modelling Geography of Financial Inclusion with Geographically Weighted Methods",
    "section": "Reference",
    "text": "Reference\n\nFinScope Tanzania 2023\nFinScope Uganda 2023\n\n\nResearch articles\n\nKaliba, Aloyce R ; Bishagazi, Kaihula P ; Gongwe, Anne G (2023) “Financial Inclusion in Tanzania Determinants, Barriers, and Impact”, The Journal of developing areas, Vol.57 (2), pp.65-87. SMU library e-journal.\nJana S. Hamdan, Katharina Lehmann-Uschner & Lukas Menkhoff (2022) Mobile Money, Financial Inclusion, and Unmet Opportunities: Evidence from Uganda, The Journal of Development Studies, 58:4, 671-691. SMU library e-journal.\nNguyen, Nhan Thien, et. al. (2021) “The convergence of financial inclusion across provinces in Vietnam: A novel approach” PloS one, Vol.16 (8). SMU library e-journal."
  }
]